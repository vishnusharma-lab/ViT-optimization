{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms (resize all images to 224x224)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "# Path to your ImageNet data\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "# Load ImageNet dataset and filter only the first 200 classes\n",
    "filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# Use only the first 200 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(filtered_dataset))\n",
    "test_size = len(filtered_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the number of samples in each set\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "        self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "        # Ensure that the number of heads divides the embedding dimension\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPBlock(nn.Module):  \n",
    "    def __init__(self, embed_dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "        #  Fix: Correct positional embedding key\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm import create_model\n",
    "\n",
    "# Load pretrained ViT-Base\n",
    "pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "pretrained_state_dict = pretrained_vit.state_dict()\n",
    "\n",
    "# Initialize our super network\n",
    "super_vit = DynamicViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=1000)\n",
    "\n",
    "## check this whether it is 1000 or 200 and finetune \n",
    "\n",
    "# Filter matching weights\n",
    "model_state_dict = super_vit.state_dict()\n",
    "filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "# Load pretrained weights\n",
    "super_vit.load_state_dict(filtered_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperband and random search algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## also we can vary patch size and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load a sampled model\n",
    "# def create_and_load_sampled_model(img_size=224, patch_size=16, num_classes=200, save_path='/home/pratibha/nas_vision/weights-hyperband'):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = sample_subnetwork()\n",
    "    \n",
    "#     sampled_vit = DynamicViT(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim, \n",
    "#                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=num_classes)\n",
    "    \n",
    "#     pretrained_vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     model_state_dict = sampled_vit.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "#     pretrained_loaded = len(filtered_dict) > 0\n",
    "#     if pretrained_loaded:\n",
    "#         print(f\"Pretrained weights loaded for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "#     else:\n",
    "#         print(f\"No pretrained weights matched for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "\n",
    "#     sampled_vit.load_state_dict(filtered_dict, strict=False)\n",
    "\n",
    "#     # Create a directory for each architecture\n",
    "#     architecture_folder = os.path.join(save_path, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "#     os.makedirs(architecture_folder, exist_ok=True)\n",
    "\n",
    "#     checkpoint_filename = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "    \n",
    "#     print(f\"Number of parameters for this architecture: {count_parameters(sampled_vit):,}\")\n",
    "    \n",
    "#     # Save the model after sampling\n",
    "#     torch.save(sampled_vit.state_dict(), checkpoint_filename)\n",
    "\n",
    "#     return sampled_vit, checkpoint_filename, architecture_folder\n",
    "\n",
    "# Main loop to sample, fine-tune, and test models\n",
    "# def train_all_configurations():\n",
    "#     architectures = []\n",
    "#     checkpoint_base_path = '/home/pratibha/nas_vision/weights-hyperband'  # Base path for checkpoints\n",
    "#     epochs = 3                                                                                        # Initial number of epochs\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect the device (GPU if available)\n",
    "\n",
    "#     # To keep track of architecture combinations\n",
    "#     seen_architectures = set()\n",
    "\n",
    "#     for i in range(25):                                                                                 # Initial 25 random architectures\n",
    "#         print(f\"Sampling subnetwork {i+1}...\")\n",
    "#         sampled_model, checkpoint_filename, architecture_folder = create_and_load_sampled_model(save_path=checkpoint_base_path)\n",
    "        \n",
    "#         # Ensure no duplicate architecture by checking the combination\n",
    "#         architecture_key = (sampled_model.depth, sampled_model.num_heads, sampled_model.mlp_ratio, sampled_model.embed_dim)\n",
    "        \n",
    "#         if architecture_key not in seen_architectures:\n",
    "#             seen_architectures.add(architecture_key)\n",
    "#             sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "#             architectures.append((sampled_model, checkpoint_filename, architecture_folder))\n",
    "#         else:\n",
    "#             print(f\"Duplicate architecture {architecture_key} detected, skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop to sample, fine-tune, and test models\n",
    "# def train_all_configurations():\n",
    "#     architectures = []\n",
    "#     checkpoint_base_path = '/home/pratibha/nas_vision/weights-hyperband'  # Base path for checkpoints\n",
    "#     epochs = 3                                                                                        # Initial number of epochs\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect the device (GPU if available)\n",
    "\n",
    "#     for i in range(25):                                                                                 # Initial 25 random architectures\n",
    "#         print(f\"Sampling subnetwork {i+1}...\")\n",
    "#         sampled_model, checkpoint_filename, architecture_folder = create_and_load_sampled_model(save_path=checkpoint_base_path)\n",
    "#         sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "#         architectures.append((sampled_model, checkpoint_filename, architecture_folder))\n",
    "\n",
    "#     while len(architectures) > 1:  # Continue until one architecture remains\n",
    "#         print(f\"\\nTraining and ranking architectures. {len(architectures)} architectures remaining.\")\n",
    "\n",
    "#         # Fine-tune and evaluate\n",
    "#         test_losses = []\n",
    "#         accuracy_scores = []\n",
    "#         latencies = []\n",
    "#         memory_usages = []\n",
    "#         for sampled_model, checkpoint_filename, architecture_folder in architectures:\n",
    "#             # Load the weights from the last stage\n",
    "#             sampled_model.load_state_dict(torch.load(checkpoint_filename))\n",
    "#             sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "#             finetune_model(sampled_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=checkpoint_filename)\n",
    "#             test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(sampled_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "#             test_losses.append(test_loss)\n",
    "#             accuracy_scores.append(test_accuracy)\n",
    "#             latencies.append(latency)\n",
    "#             memory_usages.append(count_parameters(sampled_model) * 4 / (1024**2))  # Memory in MB\n",
    "\n",
    "#         # Rank architectures based on weighted score\n",
    "#         architectures = rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages)\n",
    "#         architectures = architectures[:len(architectures) // 2]  # Keep top half\n",
    "#         print(f\"Top Half Architectures after Stage: {len(architectures)} architectures\")\n",
    "\n",
    "#         # Double the epochs for the next stage\n",
    "#         epochs *= 2\n",
    "\n",
    "#     # Final test on the best architecture\n",
    "#     print(f\"\\nTraining the best architecture for final evaluation.\")\n",
    "#     best_model, best_checkpoint_filename, best_architecture_folder = architectures[0]\n",
    "#     best_model = best_model.to(device)  # Move the best model to the selected device\n",
    "#     finetune_model(best_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=best_checkpoint_filename)\n",
    "#     test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(best_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "#     print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "#     print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Final Latency: {latency:.6f} seconds per image\")\n",
    "\n",
    "# Main loop to sample, fine-tune, and test models\n",
    "# def train_all_configurations():\n",
    "#     architectures = []\n",
    "#     checkpoint_base_path = '/home/pratibha/nas_vision/weights-hyperband'  # Base path for checkpoints\n",
    "#     epochs = 3                                                                                        # Initial number of epochs\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect the device (GPU if available)\n",
    "\n",
    "#     for i in range(25):                                                                                 # Initial 25 random architectures\n",
    "#         print(f\"Sampling subnetwork {i+1}...\")\n",
    "#         sampled_model, checkpoint_filename, architecture_folder = create_and_load_sampled_model(save_path=checkpoint_base_path)\n",
    "#         sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "#         architectures.append((sampled_model, checkpoint_filename, architecture_folder))\n",
    "\n",
    "#     while len(architectures) > 1:  # Continue until one architecture remains\n",
    "#         print(f\"\\nTraining and ranking architectures. {len(architectures)} architectures remaining.\")\n",
    "\n",
    "#         # Fine-tune and evaluate\n",
    "#         test_losses = []\n",
    "#         accuracy_scores = []\n",
    "#         latencies = []\n",
    "#         memory_usages = []\n",
    "#         for sampled_model, checkpoint_filename, architecture_folder in architectures:\n",
    "#             # Load the weights from the last stage\n",
    "#             sampled_model.load_state_dict(torch.load(checkpoint_filename))\n",
    "#             sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "#             finetune_model(sampled_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=checkpoint_filename)\n",
    "#             test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(sampled_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "#             test_losses.append(test_loss)\n",
    "#             accuracy_scores.append(test_accuracy)\n",
    "#             latencies.append(latency)\n",
    "#             memory_usages.append(count_parameters(sampled_model) * 4 / (1024**2))  # Memory in MB\n",
    "\n",
    "#         # Rank architectures based on weighted score\n",
    "#         architectures = rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages)\n",
    "#         print(f\"Architectures considered for the next stage: {[f'{arch[0].depth}-{arch[0].num_heads}-{arch[0].mlp_ratio}-{arch[0].embed_dim}' for arch in architectures]}\")\n",
    "\n",
    "#         architectures = architectures[:len(architectures) // 2]  # Keep top half\n",
    "#         print(f\"Top Half Architectures after Stage: {len(architectures)} architectures\")\n",
    "\n",
    "#         # Double the epochs for the next stage\n",
    "#         epochs *= 2\n",
    "\n",
    "#     # Final test on the best architecture\n",
    "#     print(f\"\\nTraining the best architecture for final evaluation.\")\n",
    "#     best_model, best_checkpoint_filename, best_architecture_folder = architectures[0]\n",
    "#     best_model = best_model.to(device)  # Move the best model to the selected device\n",
    "#     finetune_model(best_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=best_checkpoint_filename)\n",
    "#     test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(best_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "#     print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "#     print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "#     print(f\"Final Latency: {latency:.6f} seconds per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on imgnet 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling subnetwork 1...\n",
      "Pretrained weights loaded for model 4-8-2.0-768.\n",
      "Number of parameters for this architecture: 19,806,152\n",
      "Sampling subnetwork 2...\n",
      "Pretrained weights loaded for model 12-12-2.0-768.\n",
      "Number of parameters for this architecture: 57,622,472\n",
      "Sampling subnetwork 3...\n",
      "Pretrained weights loaded for model 8-12-6.0-768.\n",
      "Number of parameters for this architecture: 76,487,624\n",
      "Sampling subnetwork 4...\n",
      "Pretrained weights loaded for model 10-4-2.0-768.\n",
      "Number of parameters for this architecture: 48,168,392\n",
      "Sampling subnetwork 5...\n",
      "Pretrained weights loaded for model 10-4-6.0-768.\n",
      "Number of parameters for this architecture: 95,385,032\n",
      "Sampling subnetwork 6...\n",
      "Pretrained weights loaded for model 12-16-6.0-768.\n",
      "Number of parameters for this architecture: 114,282,440\n",
      "Sampling subnetwork 7...\n",
      "Pretrained weights loaded for model 4-8-6.0-768.\n",
      "Number of parameters for this architecture: 38,692,808\n",
      "Sampling subnetwork 8...\n",
      "Pretrained weights loaded for model 12-4-2.0-768.\n",
      "Number of parameters for this architecture: 57,622,472\n",
      "Sampling subnetwork 9...\n",
      "Pretrained weights loaded for model 6-8-2.0-768.\n",
      "Number of parameters for this architecture: 29,260,232\n",
      "Sampling subnetwork 10...\n",
      "Pretrained weights loaded for model 6-4-4.0-768.\n",
      "Number of parameters for this architecture: 43,425,224\n",
      "Sampling subnetwork 11...\n",
      "Pretrained weights loaded for model 6-16-4.0-768.\n",
      "Number of parameters for this architecture: 43,425,224\n",
      "Sampling subnetwork 12...\n",
      "Pretrained weights loaded for model 8-12-6.0-768.\n",
      "Number of parameters for this architecture: 76,487,624\n",
      "Duplicate architecture (8, 12, 6.0, 768) detected, skipping.\n",
      "Sampling subnetwork 13...\n",
      "Pretrained weights loaded for model 12-16-4.0-768.\n",
      "Number of parameters for this architecture: 85,952,456\n",
      "Sampling subnetwork 14...\n",
      "Pretrained weights loaded for model 4-12-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Sampling subnetwork 15...\n",
      "Pretrained weights loaded for model 8-4-4.0-768.\n",
      "Number of parameters for this architecture: 57,600,968\n",
      "Sampling subnetwork 16...\n",
      "Pretrained weights loaded for model 8-8-4.0-768.\n",
      "Number of parameters for this architecture: 57,600,968\n",
      "Sampling subnetwork 17...\n",
      "Pretrained weights loaded for model 6-4-6.0-768.\n",
      "Number of parameters for this architecture: 57,590,216\n",
      "Sampling subnetwork 18...\n",
      "Pretrained weights loaded for model 6-12-2.0-768.\n",
      "Number of parameters for this architecture: 29,260,232\n",
      "Sampling subnetwork 19...\n",
      "Pretrained weights loaded for model 10-12-2.0-768.\n",
      "Number of parameters for this architecture: 48,168,392\n",
      "Sampling subnetwork 20...\n",
      "Pretrained weights loaded for model 10-4-6.0-768.\n",
      "Number of parameters for this architecture: 95,385,032\n",
      "Duplicate architecture (10, 4, 6.0, 768) detected, skipping.\n",
      "Sampling subnetwork 21...\n",
      "Pretrained weights loaded for model 6-4-4.0-768.\n",
      "Number of parameters for this architecture: 43,425,224\n",
      "Duplicate architecture (6, 4, 4.0, 768) detected, skipping.\n",
      "Sampling subnetwork 22...\n",
      "Pretrained weights loaded for model 4-8-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Sampling subnetwork 23...\n",
      "Pretrained weights loaded for model 4-16-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Sampling subnetwork 24...\n",
      "Pretrained weights loaded for model 4-4-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Sampling subnetwork 25...\n",
      "Pretrained weights loaded for model 4-12-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Duplicate architecture (4, 12, 4.0, 768) detected, skipping.\n",
      "Sampling subnetwork 26...\n",
      "Pretrained weights loaded for model 6-4-4.0-768.\n",
      "Number of parameters for this architecture: 43,425,224\n",
      "Duplicate architecture (6, 4, 4.0, 768) detected, skipping.\n",
      "Sampling subnetwork 27...\n",
      "Pretrained weights loaded for model 10-12-4.0-768.\n",
      "Number of parameters for this architecture: 71,776,712\n",
      "Sampling subnetwork 28...\n",
      "Pretrained weights loaded for model 4-12-2.0-768.\n",
      "Number of parameters for this architecture: 19,806,152\n",
      "Sampling subnetwork 29...\n",
      "Pretrained weights loaded for model 8-16-6.0-768.\n",
      "Number of parameters for this architecture: 76,487,624\n",
      "Sampling subnetwork 30...\n",
      "Pretrained weights loaded for model 8-4-2.0-768.\n",
      "Number of parameters for this architecture: 38,714,312\n",
      "Sampling subnetwork 31...\n",
      "Pretrained weights loaded for model 4-8-6.0-768.\n",
      "Number of parameters for this architecture: 38,692,808\n",
      "Duplicate architecture (4, 8, 6.0, 768) detected, skipping.\n",
      "Sampling subnetwork 32...\n",
      "Pretrained weights loaded for model 6-8-2.0-768.\n",
      "Number of parameters for this architecture: 29,260,232\n",
      "Duplicate architecture (6, 8, 2.0, 768) detected, skipping.\n",
      "Sampling subnetwork 33...\n",
      "Pretrained weights loaded for model 4-12-4.0-768.\n",
      "Number of parameters for this architecture: 29,249,480\n",
      "Duplicate architecture (4, 12, 4.0, 768) detected, skipping.\n",
      "Sampling subnetwork 34...\n",
      "Pretrained weights loaded for model 10-8-2.0-768.\n",
      "Number of parameters for this architecture: 48,168,392\n",
      "Sampling subnetwork 35...\n",
      "Pretrained weights loaded for model 6-12-2.0-768.\n",
      "Number of parameters for this architecture: 29,260,232\n",
      "Duplicate architecture (6, 12, 2.0, 768) detected, skipping.\n",
      "\n",
      "Training and ranking architectures. 26 architectures remaining.\n",
      "Epoch 1/3, Train Loss: 4.0428, Test Loss: 3.3381, Test Accuracy: 23.28%\n",
      "Precision: 0.2594, Recall: 0.2278, F1 Score: 0.2079, Latency: 0.003343 seconds per image\n",
      "Epoch 1 Execution Time: 496.8991 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.9989, Test Loss: 2.8685, Test Accuracy: 31.69%\n",
      "Precision: 0.3427, Recall: 0.3119, F1 Score: 0.2998, Latency: 0.003295 seconds per image\n",
      "Epoch 2 Execution Time: 491.4574 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.5391, Test Loss: 2.7188, Test Accuracy: 35.16%\n",
      "Precision: 0.3813, Recall: 0.3467, F1 Score: 0.3380, Latency: 0.003318 seconds per image\n",
      "Epoch 3 Execution Time: 488.3894 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0345, Test Loss: 3.1965, Test Accuracy: 25.40%\n",
      "Precision: 0.2816, Recall: 0.2491, F1 Score: 0.2229, Latency: 0.004122 seconds per image\n",
      "Epoch 1 Execution Time: 753.6356 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.8084, Test Loss: 2.6611, Test Accuracy: 35.60%\n",
      "Precision: 0.3851, Recall: 0.3506, F1 Score: 0.3404, Latency: 0.004064 seconds per image\n",
      "Epoch 2 Execution Time: 752.7935 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.2518, Test Loss: 2.3827, Test Accuracy: 41.20%\n",
      "Precision: 0.4374, Recall: 0.4066, F1 Score: 0.4005, Latency: 0.004101 seconds per image\n",
      "Epoch 3 Execution Time: 754.2063 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0211, Test Loss: 3.2635, Test Accuracy: 24.11%\n",
      "Precision: 0.2627, Recall: 0.2368, F1 Score: 0.2152, Latency: 0.004256 seconds per image\n",
      "Epoch 1 Execution Time: 780.4168 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.8866, Test Loss: 2.7202, Test Accuracy: 34.87%\n",
      "Precision: 0.3694, Recall: 0.3429, F1 Score: 0.3271, Latency: 0.004162 seconds per image\n",
      "Epoch 2 Execution Time: 774.4732 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.3726, Test Loss: 2.5067, Test Accuracy: 38.87%\n",
      "Precision: 0.4207, Recall: 0.3828, F1 Score: 0.3767, Latency: 0.004158 seconds per image\n",
      "Epoch 3 Execution Time: 775.8376 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0583, Test Loss: 3.3015, Test Accuracy: 23.82%\n",
      "Precision: 0.2610, Recall: 0.2352, F1 Score: 0.2129, Latency: 0.003850 seconds per image\n",
      "Epoch 1 Execution Time: 653.8904 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.9178, Test Loss: 2.7487, Test Accuracy: 33.87%\n",
      "Precision: 0.3626, Recall: 0.3324, F1 Score: 0.3173, Latency: 0.003738 seconds per image\n",
      "Epoch 2 Execution Time: 651.0920 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.3828, Test Loss: 2.4864, Test Accuracy: 39.42%\n",
      "Precision: 0.4170, Recall: 0.3888, F1 Score: 0.3800, Latency: 0.003722 seconds per image\n",
      "Epoch 3 Execution Time: 650.7924 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.2068, Test Loss: 3.4683, Test Accuracy: 20.85%\n",
      "Precision: 0.2279, Recall: 0.2037, F1 Score: 0.1830, Latency: 0.004406 seconds per image\n",
      "Epoch 1 Execution Time: 844.1783 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 3.1038, Test Loss: 2.8985, Test Accuracy: 31.10%\n",
      "Precision: 0.3403, Recall: 0.3065, F1 Score: 0.2915, Latency: 0.004460 seconds per image\n",
      "Epoch 2 Execution Time: 847.0695 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.5584, Test Loss: 2.5915, Test Accuracy: 37.03%\n",
      "Precision: 0.3899, Recall: 0.3649, F1 Score: 0.3557, Latency: 0.005948 seconds per image\n",
      "Epoch 3 Execution Time: 928.8777 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.1292, Test Loss: 3.3408, Test Accuracy: 22.55%\n",
      "Precision: 0.2587, Recall: 0.2215, F1 Score: 0.2012, Latency: 0.006764 seconds per image\n",
      "Epoch 1 Execution Time: 1455.7250 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.9547, Test Loss: 2.7833, Test Accuracy: 32.88%\n",
      "Precision: 0.3669, Recall: 0.3239, F1 Score: 0.3147, Latency: 0.006375 seconds per image\n",
      "Epoch 2 Execution Time: 1444.1193 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.4015, Test Loss: 2.5064, Test Accuracy: 38.52%\n",
      "Precision: 0.4221, Recall: 0.3787, F1 Score: 0.3741, Latency: 0.006817 seconds per image\n",
      "Epoch 3 Execution Time: 1149.1075 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0827, Test Loss: 3.3814, Test Accuracy: 22.67%\n",
      "Precision: 0.2471, Recall: 0.2232, F1 Score: 0.1985, Latency: 0.004661 seconds per image\n",
      "Epoch 1 Execution Time: 733.8846 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 3.0383, Test Loss: 2.8852, Test Accuracy: 31.51%\n",
      "Precision: 0.3349, Recall: 0.3093, F1 Score: 0.2947, Latency: 0.004425 seconds per image\n",
      "Epoch 2 Execution Time: 746.2782 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.5624, Test Loss: 2.6291, Test Accuracy: 36.82%\n",
      "Precision: 0.3919, Recall: 0.3628, F1 Score: 0.3534, Latency: 0.004486 seconds per image\n",
      "Epoch 3 Execution Time: 742.1563 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.2290, Test Loss: 3.4757, Test Accuracy: 20.50%\n",
      "Precision: 0.2354, Recall: 0.1998, F1 Score: 0.1774, Latency: 0.003868 seconds per image\n",
      "Epoch 1 Execution Time: 785.9281 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 3.0534, Test Loss: 2.8825, Test Accuracy: 31.62%\n",
      "Precision: 0.3450, Recall: 0.3116, F1 Score: 0.2963, Latency: 0.003895 seconds per image\n",
      "Epoch 2 Execution Time: 706.7405 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.4912, Test Loss: 2.6069, Test Accuracy: 36.62%\n",
      "Precision: 0.4043, Recall: 0.3607, F1 Score: 0.3519, Latency: 0.003978 seconds per image\n",
      "Epoch 3 Execution Time: 708.9249 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.9595, Test Loss: 3.1827, Test Accuracy: 26.70%\n",
      "Precision: 0.2828, Recall: 0.2624, F1 Score: 0.2401, Latency: 0.003531 seconds per image\n",
      "Epoch 1 Execution Time: 556.4388 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.8543, Test Loss: 2.7662, Test Accuracy: 34.02%\n",
      "Precision: 0.3765, Recall: 0.3358, F1 Score: 0.3313, Latency: 0.003581 seconds per image\n",
      "Epoch 2 Execution Time: 558.9476 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.3602, Test Loss: 2.5480, Test Accuracy: 38.28%\n",
      "Precision: 0.4096, Recall: 0.3768, F1 Score: 0.3677, Latency: 0.003563 seconds per image\n",
      "Epoch 3 Execution Time: 560.9277 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.2298, Test Loss: 2.2319, Test Accuracy: 43.80%\n",
      "Precision: 0.4784, Recall: 0.4326, F1 Score: 0.4186, Latency: 0.003619 seconds per image\n",
      "Epoch 1 Execution Time: 598.8458 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.7956, Test Loss: 1.8159, Test Accuracy: 53.40%\n",
      "Precision: 0.5608, Recall: 0.5277, F1 Score: 0.5237, Latency: 0.003611 seconds per image\n",
      "Epoch 2 Execution Time: 599.3941 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 1.2362, Test Loss: 1.7385, Test Accuracy: 55.62%\n",
      "Precision: 0.5798, Recall: 0.5512, F1 Score: 0.5479, Latency: 0.003716 seconds per image\n",
      "Epoch 3 Execution Time: 600.8641 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 2.3599, Test Loss: 1.5284, Test Accuracy: 59.25%\n",
      "Precision: 0.6324, Recall: 0.5885, F1 Score: 0.5826, Latency: 0.003823 seconds per image\n",
      "Epoch 1 Execution Time: 634.4619 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.1132, Test Loss: 1.3268, Test Accuracy: 64.71%\n",
      "Precision: 0.6662, Recall: 0.6413, F1 Score: 0.6395, Latency: 0.003823 seconds per image\n",
      "Epoch 2 Execution Time: 633.5753 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 0.6437, Test Loss: 1.4019, Test Accuracy: 64.28%\n",
      "Precision: 0.6641, Recall: 0.6374, F1 Score: 0.6376, Latency: 0.003810 seconds per image\n",
      "Epoch 3 Execution Time: 634.7224 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 1.1903, Test Loss: 0.7921, Test Accuracy: 77.84%\n",
      "Precision: 0.7910, Recall: 0.7753, F1 Score: 0.7729, Latency: 0.004628 seconds per image\n",
      "Epoch 1 Execution Time: 891.4563 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 0.5818, Test Loss: 0.8921, Test Accuracy: 75.87%\n",
      "Precision: 0.7775, Recall: 0.7556, F1 Score: 0.7544, Latency: 0.004544 seconds per image\n",
      "Epoch 2 Execution Time: 890.2684 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 0.4470, Test Loss: 0.9002, Test Accuracy: 76.65%\n",
      "Precision: 0.7815, Recall: 0.7633, F1 Score: 0.7639, Latency: 0.004493 seconds per image\n",
      "Epoch 3 Execution Time: 887.0947 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 2.7994, Test Loss: 1.9665, Test Accuracy: 49.94%\n",
      "Precision: 0.5449, Recall: 0.4943, F1 Score: 0.4873, Latency: 0.003564 seconds per image\n",
      "Epoch 1 Execution Time: 545.3170 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.5379, Test Loss: 1.6479, Test Accuracy: 56.94%\n",
      "Precision: 0.5992, Recall: 0.5641, F1 Score: 0.5602, Latency: 0.003560 seconds per image\n",
      "Epoch 2 Execution Time: 545.9635 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 1.0295, Test Loss: 1.6437, Test Accuracy: 58.39%\n",
      "Precision: 0.6021, Recall: 0.5791, F1 Score: 0.5755, Latency: 0.003556 seconds per image\n",
      "Epoch 3 Execution Time: 545.6868 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 2.8288, Test Loss: 1.7959, Test Accuracy: 53.25%\n",
      "Precision: 0.5724, Recall: 0.5277, F1 Score: 0.5219, Latency: 0.003852 seconds per image\n",
      "Epoch 1 Execution Time: 671.9864 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.3920, Test Loss: 1.4990, Test Accuracy: 60.38%\n",
      "Precision: 0.6345, Recall: 0.5981, F1 Score: 0.5948, Latency: 0.003821 seconds per image\n",
      "Epoch 2 Execution Time: 672.5688 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 0.8715, Test Loss: 1.4437, Test Accuracy: 62.13%\n",
      "Precision: 0.6399, Recall: 0.6165, F1 Score: 0.6151, Latency: 0.003965 seconds per image\n",
      "Epoch 3 Execution Time: 677.0312 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 2.1959, Test Loss: 1.3074, Test Accuracy: 64.54%\n",
      "Precision: 0.6809, Recall: 0.6414, F1 Score: 0.6435, Latency: 0.003967 seconds per image\n",
      "Epoch 1 Execution Time: 686.9947 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 0.9252, Test Loss: 1.1908, Test Accuracy: 67.74%\n",
      "Precision: 0.7032, Recall: 0.6732, F1 Score: 0.6715, Latency: 0.003881 seconds per image\n",
      "Epoch 2 Execution Time: 683.9346 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 0.5467, Test Loss: 1.2561, Test Accuracy: 68.01%\n",
      "Precision: 0.7039, Recall: 0.6758, F1 Score: 0.6761, Latency: 0.004009 seconds per image\n",
      "Epoch 3 Execution Time: 696.0156 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.2767, Test Loss: 3.5855, Test Accuracy: 18.93%\n",
      "Precision: 0.2135, Recall: 0.1852, F1 Score: 0.1626, Latency: 0.003847 seconds per image\n",
      "Epoch 1 Execution Time: 658.7659 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 3.2174, Test Loss: 2.9993, Test Accuracy: 29.03%\n",
      "Precision: 0.3144, Recall: 0.2855, F1 Score: 0.2691, Latency: 0.003832 seconds per image\n",
      "Epoch 2 Execution Time: 653.4838 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.7084, Test Loss: 2.7339, Test Accuracy: 34.55%\n",
      "Precision: 0.3669, Recall: 0.3396, F1 Score: 0.3274, Latency: 0.003862 seconds per image\n",
      "Epoch 3 Execution Time: 655.2410 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.8623, Test Loss: 3.0583, Test Accuracy: 28.43%\n",
      "Precision: 0.3048, Recall: 0.2788, F1 Score: 0.2579, Latency: 0.003664 seconds per image\n",
      "Epoch 1 Execution Time: 571.7332 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.7446, Test Loss: 2.6661, Test Accuracy: 36.15%\n",
      "Precision: 0.3869, Recall: 0.3548, F1 Score: 0.3419, Latency: 0.003565 seconds per image\n",
      "Epoch 2 Execution Time: 570.7410 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.2455, Test Loss: 2.4744, Test Accuracy: 40.22%\n",
      "Precision: 0.4311, Recall: 0.3965, F1 Score: 0.3864, Latency: 0.003488 seconds per image\n",
      "Epoch 3 Execution Time: 570.0281 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.8698, Test Loss: 3.0600, Test Accuracy: 28.25%\n",
      "Precision: 0.3018, Recall: 0.2779, F1 Score: 0.2600, Latency: 0.004032 seconds per image\n",
      "Epoch 1 Execution Time: 700.2838 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.7075, Test Loss: 2.6259, Test Accuracy: 36.72%\n",
      "Precision: 0.3948, Recall: 0.3611, F1 Score: 0.3542, Latency: 0.003998 seconds per image\n",
      "Epoch 2 Execution Time: 699.1466 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.1587, Test Loss: 2.4086, Test Accuracy: 40.99%\n",
      "Precision: 0.4414, Recall: 0.4043, F1 Score: 0.4008, Latency: 0.003908 seconds per image\n",
      "Epoch 3 Execution Time: 694.8712 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.0106, Test Loss: 2.1170, Test Accuracy: 46.21%\n",
      "Precision: 0.5049, Recall: 0.4577, F1 Score: 0.4498, Latency: 0.003460 seconds per image\n",
      "Epoch 1 Execution Time: 528.8706 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.7332, Test Loss: 1.7978, Test Accuracy: 53.74%\n",
      "Precision: 0.5625, Recall: 0.5318, F1 Score: 0.5254, Latency: 0.003489 seconds per image\n",
      "Epoch 2 Execution Time: 530.9318 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 1.2076, Test Loss: 1.8037, Test Accuracy: 54.88%\n",
      "Precision: 0.5780, Recall: 0.5430, F1 Score: 0.5408, Latency: 0.003482 seconds per image\n",
      "Epoch 3 Execution Time: 536.0707 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 2.8802, Test Loss: 2.0614, Test Accuracy: 47.55%\n",
      "Precision: 0.5181, Recall: 0.4673, F1 Score: 0.4566, Latency: 0.003510 seconds per image\n",
      "Epoch 1 Execution Time: 544.2113 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.6049, Test Loss: 1.7510, Test Accuracy: 54.73%\n",
      "Precision: 0.5746, Recall: 0.5398, F1 Score: 0.5360, Latency: 0.003510 seconds per image\n",
      "Epoch 2 Execution Time: 544.5329 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 1.0698, Test Loss: 1.7762, Test Accuracy: 55.14%\n",
      "Precision: 0.5909, Recall: 0.5463, F1 Score: 0.5480, Latency: 0.003504 seconds per image\n",
      "Epoch 3 Execution Time: 544.5285 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.2072, Test Loss: 2.3319, Test Accuracy: 41.81%\n",
      "Precision: 0.4491, Recall: 0.4127, F1 Score: 0.3997, Latency: 0.003417 seconds per image\n",
      "Epoch 1 Execution Time: 520.1884 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 1.9441, Test Loss: 1.9937, Test Accuracy: 48.82%\n",
      "Precision: 0.5284, Recall: 0.4829, F1 Score: 0.4788, Latency: 0.003410 seconds per image\n",
      "Epoch 2 Execution Time: 519.2624 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 1.4321, Test Loss: 1.9377, Test Accuracy: 51.39%\n",
      "Precision: 0.5530, Recall: 0.5080, F1 Score: 0.5046, Latency: 0.003420 seconds per image\n",
      "Epoch 3 Execution Time: 519.8019 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 1.2641, Test Loss: 0.8651, Test Accuracy: 75.35%\n",
      "Precision: 0.7731, Recall: 0.7485, F1 Score: 0.7466, Latency: 0.004176 seconds per image\n",
      "Epoch 1 Execution Time: 782.9327 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 0.5713, Test Loss: 0.8665, Test Accuracy: 75.74%\n",
      "Precision: 0.7768, Recall: 0.7536, F1 Score: 0.7520, Latency: 0.004268 seconds per image\n",
      "Epoch 2 Execution Time: 787.0908 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 0.3867, Test Loss: 0.9642, Test Accuracy: 74.97%\n",
      "Precision: 0.7646, Recall: 0.7454, F1 Score: 0.7453, Latency: 0.004212 seconds per image\n",
      "Epoch 3 Execution Time: 785.0365 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.9335, Test Loss: 3.1612, Test Accuracy: 26.73%\n",
      "Precision: 0.2952, Recall: 0.2631, F1 Score: 0.2507, Latency: 0.003507 seconds per image\n",
      "Epoch 1 Execution Time: 516.5498 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.8567, Test Loss: 2.7883, Test Accuracy: 33.61%\n",
      "Precision: 0.3756, Recall: 0.3302, F1 Score: 0.3168, Latency: 0.003492 seconds per image\n",
      "Epoch 2 Execution Time: 516.4542 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.3937, Test Loss: 2.6267, Test Accuracy: 37.01%\n",
      "Precision: 0.3977, Recall: 0.3647, F1 Score: 0.3585, Latency: 0.003480 seconds per image\n",
      "Epoch 3 Execution Time: 516.0281 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0208, Test Loss: 3.2567, Test Accuracy: 24.82%\n",
      "Precision: 0.2759, Recall: 0.2430, F1 Score: 0.2219, Latency: 0.004276 seconds per image\n",
      "Epoch 1 Execution Time: 798.1858 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.8881, Test Loss: 2.7622, Test Accuracy: 33.53%\n",
      "Precision: 0.3669, Recall: 0.3302, F1 Score: 0.3175, Latency: 0.004213 seconds per image\n",
      "Epoch 2 Execution Time: 795.3225 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.3591, Test Loss: 2.5482, Test Accuracy: 37.96%\n",
      "Precision: 0.4164, Recall: 0.3743, F1 Score: 0.3697, Latency: 0.004270 seconds per image\n",
      "Epoch 3 Execution Time: 796.7553 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 4.0240, Test Loss: 3.3346, Test Accuracy: 23.62%\n",
      "Precision: 0.2641, Recall: 0.2315, F1 Score: 0.2154, Latency: 0.003565 seconds per image\n",
      "Epoch 1 Execution Time: 596.9237 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.9322, Test Loss: 2.7952, Test Accuracy: 33.33%\n",
      "Precision: 0.3665, Recall: 0.3270, F1 Score: 0.3125, Latency: 0.003692 seconds per image\n",
      "Epoch 2 Execution Time: 598.4641 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.4173, Test Loss: 2.4844, Test Accuracy: 39.56%\n",
      "Precision: 0.4147, Recall: 0.3886, F1 Score: 0.3778, Latency: 0.003705 seconds per image\n",
      "Epoch 3 Execution Time: 600.2309 seconds\n",
      "\n",
      "Epoch 1/3, Train Loss: 3.8748, Test Loss: 3.2003, Test Accuracy: 25.04%\n",
      "Precision: 0.2962, Recall: 0.2456, F1 Score: 0.2332, Latency: 0.003820 seconds per image\n",
      "Epoch 1 Execution Time: 671.6024 seconds\n",
      "\n",
      "Epoch 2/3, Train Loss: 2.7457, Test Loss: 2.6319, Test Accuracy: 36.59%\n",
      "Precision: 0.3950, Recall: 0.3599, F1 Score: 0.3483, Latency: 0.003803 seconds per image\n",
      "Epoch 2 Execution Time: 671.0501 seconds\n",
      "\n",
      "Epoch 3/3, Train Loss: 2.2077, Test Loss: 2.4028, Test Accuracy: 40.82%\n",
      "Precision: 0.4308, Recall: 0.4021, F1 Score: 0.3957, Latency: 0.003769 seconds per image\n",
      "Epoch 3 Execution Time: 666.6906 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicViT' object has no attribute 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 273\u001b[39m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal Latency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatency\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds per image\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Run the full training process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[43mtrain_all_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 254\u001b[39m, in \u001b[36mtrain_all_configurations\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    251\u001b[39m architectures = rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages)\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# Print the architectures being considered for the next stage\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArchitectures considered for the next stage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43march\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march[\u001b[32m0\u001b[39m].num_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march[\u001b[32m0\u001b[39m].mlp_ratio\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march[\u001b[32m0\u001b[39m].embed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39march\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39marchitectures]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    256\u001b[39m architectures = architectures[:\u001b[38;5;28mlen\u001b[39m(architectures) // \u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# Keep top half\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop Half Architectures after Stage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(architectures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m architectures\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicViT' object has no attribute 'depth'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import os\n",
    "## already have defined train and test loader\n",
    "\n",
    "# Randomly sample a sub-network configuration\n",
    "def sample_subnetwork():\n",
    "    depth_choices = [4, 6, 8, 10, 12]\n",
    "    head_choices = [4, 8, 12, 16]\n",
    "    mlp_choices = [2.0, 4.0, 6.0]\n",
    "    embed_dim_choices = [768]  # Fixed embed_dim\n",
    "    \n",
    "    depth = random.choice(depth_choices)\n",
    "    num_heads = random.choice(head_choices)\n",
    "    mlp_ratio = random.choice(mlp_choices)\n",
    "    embed_dim = random.choice(embed_dim_choices)\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "\n",
    "\n",
    "# Create and load a sampled model\n",
    "def create_and_load_sampled_model(img_size=224, patch_size=16, num_classes=200, save_path='/home/pratibha/nas_vision/weights-hyperband'):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = sample_subnetwork()\n",
    "    \n",
    "    # Return the architecture parameters alongside the model\n",
    "    sampled_vit = DynamicViT(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim, \n",
    "                              depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=num_classes)\n",
    "    \n",
    "    pretrained_vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = sampled_vit.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    pretrained_loaded = len(filtered_dict) > 0\n",
    "    if pretrained_loaded:\n",
    "        print(f\"Pretrained weights loaded for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "    else:\n",
    "        print(f\"No pretrained weights matched for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "\n",
    "    sampled_vit.load_state_dict(filtered_dict, strict=False)\n",
    "\n",
    "    # Create a directory for each architecture\n",
    "    architecture_folder = os.path.join(save_path, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "    os.makedirs(architecture_folder, exist_ok=True)\n",
    "\n",
    "    checkpoint_filename = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "    \n",
    "    print(f\"Number of parameters for this architecture: {count_parameters(sampled_vit):,}\")\n",
    "    \n",
    "    # Save the model after sampling\n",
    "    torch.save(sampled_vit.state_dict(), checkpoint_filename)\n",
    "\n",
    "    return sampled_vit, checkpoint_filename, architecture_folder, depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "\n",
    "# Measure model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Test the sampled network on CIFAR-10\n",
    "def test_model_on_cifar10(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_accuracy, latency, y_true, y_pred\n",
    "\n",
    "# Rank architectures based on a weighted score\n",
    "def rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages, weight_accuracy=0.75, weight_latency=0.20, weight_memory=0.05):\n",
    "    scores = []\n",
    "    for i in range(len(architectures)):\n",
    "        score = (weight_accuracy * accuracy_scores[i]) - (weight_latency * latencies[i]) - (weight_memory * memory_usages[i])\n",
    "        scores.append(score)\n",
    "    ranked_architectures = sorted(zip(architectures, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [arch for arch, _ in ranked_architectures]\n",
    "\n",
    "# Fine-tune and test all configurations\n",
    "def finetune_model(sampled_model, train_loader, test_loader, num_epochs, checkpoint_filename):\n",
    "    optimizer = optim.Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    sampled_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch_time = time.time()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Test phase\n",
    "        test_loss, test_accuracy, latency, y_true, y_pred = test_model_on_cifar10(sampled_model, test_loader, criterion)\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        epoch_execution_time = time.time() - start_epoch_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Latency: {latency:.6f} seconds per image\")\n",
    "        print(f\"Epoch {epoch+1} Execution Time: {epoch_execution_time:.4f} seconds\\n\")\n",
    "\n",
    "    torch.save(sampled_model.state_dict(), checkpoint_filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main loop to sample, fine-tune, and test models\n",
    "def train_all_configurations():\n",
    "    architectures = []\n",
    "    checkpoint_base_path = '/home/pratibha/nas_vision/weights-hyperband'  # Base path for checkpoints\n",
    "    epochs = 3                                                                                        # Initial number of epochs\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect the device (GPU if available)\n",
    "\n",
    "    # To keep track of architecture combinations\n",
    "    seen_architectures = set()\n",
    "\n",
    "    for i in range(35):                                                                      # Initial 35 random architectures\n",
    "        print(f\"Sampling subnetwork {i+1}...\")\n",
    "        sampled_model, checkpoint_filename, architecture_folder, depth, num_heads, mlp_ratio, embed_dim = create_and_load_sampled_model(save_path=checkpoint_base_path)\n",
    "        \n",
    "        # Ensure no duplicate architecture by checking the combination\n",
    "        architecture_key = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        if architecture_key not in seen_architectures:\n",
    "            seen_architectures.add(architecture_key)\n",
    "            sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "            architectures.append((sampled_model, checkpoint_filename, architecture_folder))\n",
    "        else:\n",
    "            print(f\"Duplicate architecture {architecture_key} detected, skipping.\")\n",
    "            \n",
    "    while len(architectures) > 1:  # Continue until one architecture remains\n",
    "        print(f\"\\nTraining and ranking architectures. {len(architectures)} architectures remaining.\")\n",
    "\n",
    "        # Fine-tune and evaluate\n",
    "        test_losses = []\n",
    "        accuracy_scores = []\n",
    "        latencies = []\n",
    "        memory_usages = []\n",
    "        for sampled_model, checkpoint_filename, architecture_folder in architectures:\n",
    "            # Load the weights from the last stage\n",
    "            sampled_model.load_state_dict(torch.load(checkpoint_filename))\n",
    "            sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "            finetune_model(sampled_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=checkpoint_filename)\n",
    "            test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(sampled_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "            test_losses.append(test_loss)\n",
    "            accuracy_scores.append(test_accuracy)\n",
    "            latencies.append(latency)\n",
    "            memory_usages.append(count_parameters(sampled_model) * 4 / (1024**2))  # Memory in MB\n",
    "\n",
    "        # Rank architectures based on weighted score\n",
    "        architectures = rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages)\n",
    "        \n",
    "        # Print the architectures being considered for the next stage\n",
    "        print(f\"Architectures considered for the next stage: {[f'{arch[0].depth}-{arch[0].num_heads}-{arch[0].mlp_ratio}-{arch[0].embed_dim}' for arch in architectures]}\")\n",
    "\n",
    "        architectures = architectures[:len(architectures) // 2]  # Keep top half\n",
    "        print(f\"Top Half Architectures after Stage: {len(architectures)} architectures\")\n",
    "\n",
    "        # Double the epochs for the next stage\n",
    "        epochs *= 2\n",
    "\n",
    "    # Final test on the best architecture\n",
    "    print(f\"\\nTraining the best architecture for final evaluation.\")\n",
    "    best_model, best_checkpoint_filename, best_architecture_folder = architectures[0]\n",
    "    best_model = best_model.to(device)  # Move the best model to the selected device\n",
    "    finetune_model(best_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=best_checkpoint_filename)\n",
    "    test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(best_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Latency: {latency:.6f} seconds per image\")\n",
    "\n",
    "# Run the full training process\n",
    "train_all_configurations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from next time also print the score or rank of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import os\n",
    "## already have defined train and test loader\n",
    "\n",
    "# Randomly sample a sub-network configuration\n",
    "def sample_subnetwork():\n",
    "    depth_choices = [4, 6, 8, 10, 12]\n",
    "    head_choices = [4, 8, 12, 16]\n",
    "    mlp_choices = [2.0, 4.0, 6.0]\n",
    "    embed_dim_choices = [768]  # Fixed embed_dim\n",
    "    \n",
    "    depth = random.choice(depth_choices)\n",
    "    num_heads = random.choice(head_choices)\n",
    "    mlp_ratio = random.choice(mlp_choices)\n",
    "    embed_dim = random.choice(embed_dim_choices)\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "\n",
    "\n",
    "# Create and load a sampled model\n",
    "def create_and_load_sampled_model(img_size=224, patch_size=16, num_classes=200, save_path='/home/pratibha/nas_vision/weights-hyperband'):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = sample_subnetwork()\n",
    "    \n",
    "    # Return the architecture parameters alongside the model\n",
    "    sampled_vit = DynamicViT(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim, \n",
    "                              depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=num_classes)\n",
    "    \n",
    "    pretrained_vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = sampled_vit.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    pretrained_loaded = len(filtered_dict) > 0\n",
    "    if pretrained_loaded:\n",
    "        print(f\"Pretrained weights loaded for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "    else:\n",
    "        print(f\"No pretrained weights matched for model {depth}-{num_heads}-{mlp_ratio}-{embed_dim}.\")\n",
    "\n",
    "    sampled_vit.load_state_dict(filtered_dict, strict=False)\n",
    "\n",
    "    # Create a directory for each architecture\n",
    "    architecture_folder = os.path.join(save_path, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "    os.makedirs(architecture_folder, exist_ok=True)\n",
    "\n",
    "    checkpoint_filename = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "    \n",
    "    print(f\"Number of parameters for this architecture: {count_parameters(sampled_vit):,}\")\n",
    "    \n",
    "    # Save the model after sampling\n",
    "    torch.save(sampled_vit.state_dict(), checkpoint_filename)\n",
    "\n",
    "    return sampled_vit, checkpoint_filename, architecture_folder, depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "\n",
    "# Measure model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Test the sampled network on CIFAR-10\n",
    "def test_model_on_cifar10(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_accuracy, latency, y_true, y_pred\n",
    "\n",
    "# Rank architectures based on a weighted score\n",
    "def rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages, weight_accuracy=0.75, weight_latency=0.20, weight_memory=0.05):\n",
    "    scores = []\n",
    "    for i in range(len(architectures)):\n",
    "        score = (weight_accuracy * accuracy_scores[i]) - (weight_latency * latencies[i]) - (weight_memory * memory_usages[i])\n",
    "        scores.append(score)\n",
    "    ranked_architectures = sorted(zip(architectures, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [arch for arch, _ in ranked_architectures]\n",
    "\n",
    "# Fine-tune and test all configurations\n",
    "def finetune_model(sampled_model, train_loader, test_loader, num_epochs, checkpoint_filename):\n",
    "    optimizer = optim.Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    sampled_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch_time = time.time()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Test phase\n",
    "        test_loss, test_accuracy, latency, y_true, y_pred = test_model_on_cifar10(sampled_model, test_loader, criterion)\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        epoch_execution_time = time.time() - start_epoch_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Latency: {latency:.6f} seconds per image\")\n",
    "        print(f\"Epoch {epoch+1} Execution Time: {epoch_execution_time:.4f} seconds\\n\")\n",
    "\n",
    "    torch.save(sampled_model.state_dict(), checkpoint_filename)\n",
    "\n",
    "# Main loop to sample, fine-tune, and test models\n",
    "def train_all_configurations():\n",
    "    architectures = []\n",
    "    checkpoint_base_path = '/home/pratibha/nas_vision/weights-hyperband'  # Base path for checkpoints\n",
    "    epochs = 3                                                                                        # Initial number of epochs\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Detect the device (GPU if available)\n",
    "\n",
    "    # To keep track of architecture combinations\n",
    "    seen_architectures = set()\n",
    "\n",
    "    for i in range(25):  # Initial 25 random architectures\n",
    "        print(f\"Sampling subnetwork {i+1}...\")\n",
    "        sampled_model, checkpoint_filename, architecture_folder, depth, num_heads, mlp_ratio, embed_dim = create_and_load_sampled_model(save_path=checkpoint_base_path)\n",
    "        \n",
    "        # Ensure no duplicate architecture by checking the combination\n",
    "        architecture_key = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        if architecture_key not in seen_architectures:\n",
    "            seen_architectures.add(architecture_key)\n",
    "            sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "            architectures.append((sampled_model, checkpoint_filename, architecture_folder))\n",
    "        else:\n",
    "            print(f\"Duplicate architecture {architecture_key} detected, skipping.\")\n",
    "\n",
    "    while len(architectures) > 1:  # Continue until one architecture remains\n",
    "        print(f\"\\nTraining and ranking architectures. {len(architectures)} architectures remaining.\")\n",
    "\n",
    "        # Fine-tune and evaluate\n",
    "        test_losses = []\n",
    "        accuracy_scores = []\n",
    "        latencies = []\n",
    "        memory_usages = []\n",
    "        for sampled_model, checkpoint_filename, architecture_folder in architectures:\n",
    "            # Load the weights from the last stage\n",
    "            sampled_model.load_state_dict(torch.load(checkpoint_filename))\n",
    "            sampled_model = sampled_model.to(device)  # Move model to the selected device\n",
    "            finetune_model(sampled_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=checkpoint_filename)\n",
    "            test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(sampled_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "            test_losses.append(test_loss)\n",
    "            accuracy_scores.append(test_accuracy)\n",
    "            latencies.append(latency)\n",
    "            memory_usages.append(count_parameters(sampled_model) * 4 / (1024**2))  # Memory in MB\n",
    "\n",
    "            # After fine-tuning, save the model weights\n",
    "            torch.save(sampled_model.state_dict(), checkpoint_filename)\n",
    "            \n",
    "            # Free up GPU memory\n",
    "            del sampled_model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Rank architectures based on weighted score\n",
    "        architectures = rank_architectures(architectures, test_losses, accuracy_scores, latencies, memory_usages)\n",
    "        \n",
    "        # Print the architectures being considered for the next stage\n",
    "        print(f\"Architectures considered for the next stage: {[f'{arch[0].depth}-{arch[0].num_heads}-{arch[0].mlp_ratio}-{arch[0].embed_dim}' for arch in architectures]}\")\n",
    "\n",
    "        architectures = architectures[:len(architectures) // 2]  # Keep top half\n",
    "        print(f\"Top Half Architectures after Stage: {len(architectures)} architectures\")\n",
    "\n",
    "        # Double the epochs for the next stage\n",
    "        epochs *= 2\n",
    "\n",
    "    # Final test on the best architecture\n",
    "    print(f\"\\nTraining the best architecture for final evaluation.\")\n",
    "    best_model, best_checkpoint_filename, best_architecture_folder = architectures[0]\n",
    "    best_model = best_model.to(device)  # Move the best model to the selected device\n",
    "    finetune_model(best_model, train_loader, test_loader, num_epochs=epochs, checkpoint_filename=best_checkpoint_filename)\n",
    "    test_loss, test_accuracy, latency, _, _ = test_model_on_cifar10(best_model, test_loader, criterion=nn.CrossEntropyLoss())\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Final Latency: {latency:.6f} seconds per image\")\n",
    "    \n",
    "# Run the full training process\n",
    "train_all_configurations()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
