{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms (resize all images to 224x224)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "# Path to your ImageNet data\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "# Load ImageNet dataset and filter only the first 200 classes\n",
    "filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# Use only the first 200 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(filtered_dataset))\n",
    "test_size = len(filtered_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the number of samples in each set\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic vit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class DynamicPatchEmbed(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "#         self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.proj(x)\n",
    "#         return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class DynamicMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "#         self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.scale = (embed_dim // num_heads) ** -0.5\n",
    "#         self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "#         # Ensure that the number of heads divides the embedding dimension\n",
    "#         assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "#         return self.proj(x)\n",
    "\n",
    "# class MLPBlock(nn.Module):  \n",
    "#     def __init__(self, embed_dim, mlp_ratio):\n",
    "#         super().__init__()\n",
    "#         hidden_dim = int(embed_dim * mlp_ratio)\n",
    "#         self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "#         self.act = nn.GELU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "# class DynamicTransformerBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "#         #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "#         self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "#         #  Fix: Correct positional embedding key\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "\n",
    "#         # Add class token\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timm import create_model\n",
    "\n",
    "# # Load pretrained ViT-Base\n",
    "# pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "# pretrained_state_dict = pretrained_vit.state_dict()\n",
    "\n",
    "# # Initialize our super network\n",
    "# super_vit = DynamicViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=1000)\n",
    "\n",
    "# ## check this whether it is 1000 or 200 and finetune \n",
    "\n",
    "# # Filter matching weights\n",
    "# model_state_dict = super_vit.state_dict()\n",
    "# filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "# # Load pretrained weights\n",
    "# super_vit.load_state_dict(filtered_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just defining model again here for easily avaliability\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "        self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "        # Ensure that the number of heads divides the embedding dimension\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPBlock(nn.Module):  \n",
    "    def __init__(self, embed_dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "#         #  Fix: Correct positional embedding key\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "\n",
    "#         # Add class token\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)\n",
    "\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.depth = depth  # Store depth as an instance variable\n",
    "        self.num_heads = num_heads  # Store num_heads as an instance variable\n",
    "        self.mlp_ratio = mlp_ratio  # Store mlp_ratio as an instance variable\n",
    "        \n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "        # Fix: Correct positional embedding key\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "# from timm import create_model\n",
    "# import time\n",
    "\n",
    "# # Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/home/pratibha/nas_vision/weights-cifar5'\n",
    "# # SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# # Set the device (GPU if available, else CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # First-time loading pretrained weights for initialization\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     # Match keys between pretrained and current model\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "#     # Load pretrained weights\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# # Check if pretrained weights are loaded correctly\n",
    "# def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "#     pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     model_state_dict = model.state_dict()\n",
    "#     matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "#     if len(matching_keys) > 0:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "#     else:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# # Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "# def sample_subnetwork(seen_architectures):\n",
    "#     while True:\n",
    "#         depth = random.choice([4, 6, 8, 10, 12])\n",
    "#         num_heads = random.choice([4, 8, 12, 16])\n",
    "#         mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#         embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "#         architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "#         # Skip if architecture has already been sampled\n",
    "#         if architecture not in seen_architectures:\n",
    "#             seen_architectures.add(architecture)\n",
    "#             print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "#             # Create the model to calculate its number of parameters\n",
    "#             sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "#             num_params = count_parameters(sampled_model)\n",
    "#             print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "#             return architecture\n",
    "#         else:\n",
    "#             print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# # Count number of trainable parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # Evaluate architecture: accuracy, latency, and memory usage\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Start measuring inference latency\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             y_true.extend(labels.cpu().numpy())\n",
    "#             y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "#     # Measure total time for inference (latency)\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "#     # Compute accuracy\n",
    "#     accuracy = 100 * correct / total\n",
    "\n",
    "#     # Compute memory usage (rough estimation)\n",
    "#     memory_usage = estimate_memory_usage(model)\n",
    "\n",
    "#     # Compute average loss\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# # Estimate memory usage of a model during inference (rough estimation)\n",
    "# def estimate_memory_usage(model):\n",
    "#     # Create dummy input matching the expected shape of the input tensor\n",
    "#     dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "#     # Use torch.utils.benchmark to measure memory usage during inference\n",
    "#     start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "#     # Run the model once with the dummy input\n",
    "#     with torch.no_grad():\n",
    "#         model(dummy_input)\n",
    "    \n",
    "#     end_mem = torch.cuda.memory_allocated()\n",
    "#     memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "#     return memory_usage\n",
    "\n",
    "\n",
    "# def calculate_crowding_distance(population, test_loader):\n",
    "#     crowding_distances = [0] * len(population)\n",
    "#     num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "#     # Evaluate each architecture once, then reuse the results\n",
    "#     evaluated_results = []\n",
    "#     for arch in population:\n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "#                            depth=arch[0], num_heads=arch[1],\n",
    "#                            mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "#         accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#         memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "#         evaluated_results.append((accuracy, latency, memory))\n",
    "#         del model\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     for objective_index in range(num_objectives):\n",
    "#         sorted_indices = sorted(range(len(population)),\n",
    "#                                 key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "#         crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "#         for i in range(1, len(sorted_indices) - 1):\n",
    "#             prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "#             next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "#             distance = next_value - prev_value\n",
    "#             crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "#     return crowding_distances\n",
    "\n",
    "\n",
    "# def dominates(model1, model2, test_loader):\n",
    "#     # Evaluate both models on the test set\n",
    "#     accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "#     accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "#     # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "#     memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "#     memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "#     # Compare performance metrics\n",
    "#     dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "#     dominates_in_latency = latency1 <= latency2\n",
    "#     dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "#     # Return True if model1 dominates model2 in all aspects\n",
    "#     return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# # Mutation: Randomly mutate architecture's hyperparameters\n",
    "# def mutate(architecture):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "#     if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# # One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Optimized Pareto selection based on stored performance metrics\n",
    "# def pareto_selection(arch_performance):\n",
    "#     def dominates(perf1, perf2):\n",
    "#         acc1, lat1, mem1 = perf1\n",
    "#         acc2, lat2, mem2 = perf2\n",
    "#         return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, perf1 in arch_performance.items():\n",
    "#         dominated_count = 0\n",
    "#         for arch2, perf2 in arch_performance.items():\n",
    "#             if arch1 != arch2 and dominates(perf2, perf1):\n",
    "#                 dominated_count += 1\n",
    "#         ranks[arch1] = dominated_count\n",
    "\n",
    "#     # Sort architectures by rank (lower dominated_count = better)\n",
    "#     sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "#     return sorted_population\n",
    "\n",
    "# # Fine-tune model on dataset (train for a few epochs)\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "#     # Save the model after fine-tuning\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_top_ranked_models(population, arch_performance, generation):\n",
    "#     top_n = min(5, len(population))\n",
    "#     for idx, arch in enumerate(population[:top_n]):\n",
    "#         depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "#                            num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "\n",
    "#         architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "#         checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#         model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "#         top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "#         torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "#         acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "#         with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "#             f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "#             f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "#         print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "\n",
    "#     # Evaluate and store metrics only once per architecture per generation\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        \n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                num_classes=1000).to(device)\n",
    "\n",
    "#             # if generation == 0:\n",
    "#             #     load_pretrained_weights(model)\n",
    "#             # else:\n",
    "#             #     checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#             #     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#             #     print(f\"Loaded weights from previous generation for {arch}\")\n",
    "#             if generation == 0:\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#                 if os.path.exists(checkpoint_path):\n",
    "#                     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                     print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "#                 else:\n",
    "#                     print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "#                     load_pretrained_weights(model)\n",
    "\n",
    "\n",
    "#             check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "\n",
    "#             fine_tune_model(\n",
    "#                 model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder\n",
    "#             )\n",
    "\n",
    "#             # Evaluate once per architecture\n",
    "#             accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Save top models clearly ranked (1 = best)\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Generate offspring using crossover and mutation\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([population[i], population[i + 1]])\n",
    "\n",
    "#         offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "\n",
    "#         # Next-generation combines top parents and offspring\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1/2\n",
      "No previous weights, loading pretrained for (6, 8, 2.0, 768)\n",
      "Loaded pretrained weights into DynamicViT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m population\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Example run:\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# evolutionary_algorithm(5, 2, train_loader, test_loader)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mevolutionary_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mevolutionary_algorithm\u001b[39m\u001b[34m(population_size, generations, train_loader, test_loader)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo previous weights, loading pretrained for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m     load_pretrained_weights(model)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m perf[arch] = evaluate_architecture(model, test_loader)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model; torch.cuda.empty_cache()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mfine_tune_model\u001b[39m\u001b[34m(model, train_loader, test_loader, epochs, folder)\u001b[39m\n\u001b[32m     92\u001b[39m     loss.backward()\n\u001b[32m     93\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m acc, loss, lat, mem = evaluate_architecture(model, test_loader)\n\u001b[32m     97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "# from timm import create_model\n",
    "# import time\n",
    "\n",
    "# # Path to save models\n",
    "# SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol1'\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load pretrained ViT weights initially\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "#                      if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Loaded pretrained weights into {model.__class__.__name__}\")\n",
    "\n",
    "# # Count parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # Evaluate model clearly with corrected memory usage\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct, total, running_loss = 0, 0, 0.0\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)  # MB for FP32\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Loss: {test_loss:.4f}, Acc: {accuracy:.2f}%, Lat: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# # Mutate architecture\n",
    "# def mutate(arch):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#     if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# # Crossover architectures\n",
    "# def one_point_crossover(p1, p2):\n",
    "#     cp = random.randint(1, 3)\n",
    "#     return p1[:cp] + p2[cp:], p2[:cp] + p1[cp:]\n",
    "\n",
    "# # Pareto ranking\n",
    "# def pareto_selection(perf):\n",
    "#     def dominates(a, b):\n",
    "#         return all(x >= y for x, y in zip(a, b)) and any(x > y for x, y in zip(a, b))\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, p1 in perf.items():\n",
    "#         dominated = sum(dominates(p2, p1) for arch2, p2 in perf.items() if arch2 != arch1)\n",
    "#         ranks[arch1] = dominated\n",
    "#     return sorted(ranks, key=ranks.get)\n",
    "\n",
    "# # Fine-tune model\n",
    "# def fine_tune_model(model, train_loader, test_loader, epochs, folder):\n",
    "#     criterion, optimizer = nn.CrossEntropyLoss(), Adam(model.parameters(), lr=1e-4)\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = criterion(model(images), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         acc, loss, lat, mem = evaluate_architecture(model, test_loader)\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} done.\")\n",
    "\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "#     torch.save(model.state_dict(), os.path.join(folder, 'checkpoint.pth'))\n",
    "\n",
    "# # Evolutionary NAS main loop\n",
    "# def evolutionary_algorithm(population_size, generations, train_loader, test_loader):\n",
    "#     seen, perf = set(), {}\n",
    "#     population = [(random.choice([4,6,8,10,12]), random.choice([4,8,12,16]), random.choice([2.0,4.0,6.0]), 768)\n",
    "#                   for _ in range(population_size)]\n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nGeneration {gen+1}/{generations}\")\n",
    "\n",
    "#         for arch in population:\n",
    "#             folder = os.path.join(SAVE_PATH, f\"arch_{arch[0]}_{arch[1]}_{arch[2]}_{arch[3]}\")\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3], depth=arch[0],\n",
    "#                                num_heads=arch[1], mlp_ratio=arch[2], num_classes=200).to(device)\n",
    "\n",
    "#             ckpt = os.path.join(folder, 'checkpoint.pth')\n",
    "#             if os.path.exists(ckpt):\n",
    "#                 model.load_state_dict(torch.load(ckpt))\n",
    "#                 print(f\"Loaded previous weights for {arch}\")\n",
    "#             else:\n",
    "#                 print(f\"No previous weights, loading pretrained for {arch}\")\n",
    "#                 load_pretrained_weights(model)\n",
    "\n",
    "#             fine_tune_model(model, train_loader, test_loader, 3, folder)\n",
    "#             perf[arch] = evaluate_architecture(model, test_loader)\n",
    "#             del model; torch.cuda.empty_cache()\n",
    "\n",
    "#         population = pareto_selection(perf)\n",
    "\n",
    "#         for i, arch in enumerate(population[:5]):\n",
    "#             print(f\"Rank {i+1}: {arch}, Acc: {perf[arch][0]:.2f}%\")\n",
    "\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             child1, child2 = one_point_crossover(population[i], population[i+1])\n",
    "#             offspring.extend([mutate(child1), mutate(child2)])\n",
    "\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Example run:\n",
    "# # evolutionary_algorithm(5, 2, train_loader, test_loader)\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24 march  here ranking after first generation finetuning is not good see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=10, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 48,168,392\n",
      "Sampled architecture: Depth=8, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 38,714,312\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,600,968\n",
      "Sampled architecture: Depth=8, Num Heads=16, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 76,487,624\n",
      "Sampled architecture: Depth=12, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 114,282,440\n",
      "\n",
      "--- Generation 1/2 ---\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Generation 1: subnetwork model has loaded 96 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 3.3444, Test Accuracy: 22.95%, Latency: 0.004883 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 1/3, Loss: 6370.4857, Test Loss: 3.3444, Test Accuracy: 22.95%, Test Latency: 0.004883 seconds/image\n",
      "Test Loss: 2.8745, Test Accuracy: 31.93%, Latency: 0.004045 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 2/3, Loss: 4625.5930, Test Loss: 2.8745, Test Accuracy: 31.93%, Test Latency: 0.004045 seconds/image\n",
      "Test Loss: 2.5553, Test Accuracy: 37.52%, Latency: 0.003808 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 3/3, Loss: 3791.2766, Test Loss: 2.5553, Test Accuracy: 37.52%, Test Latency: 0.003808 seconds/image\n",
      "Test Loss: 2.5553, Test Accuracy: 37.52%, Latency: 0.003791 seconds/image, Memory Usage: 183.75 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Generation 1: subnetwork model has loaded 78 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 3.3784, Test Accuracy: 22.39%, Latency: 0.003679 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 1/3, Loss: 6438.8738, Test Loss: 3.3784, Test Accuracy: 22.39%, Test Latency: 0.003679 seconds/image\n",
      "Test Loss: 2.8835, Test Accuracy: 31.20%, Latency: 0.003688 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 2/3, Loss: 4744.8324, Test Loss: 2.8835, Test Accuracy: 31.20%, Test Latency: 0.003688 seconds/image\n",
      "Test Loss: 2.6277, Test Accuracy: 36.44%, Latency: 0.003660 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 3/3, Loss: 3934.2912, Test Loss: 2.6277, Test Accuracy: 36.44%, Test Latency: 0.003660 seconds/image\n",
      "Test Loss: 2.6277, Test Accuracy: 36.44%, Latency: 0.003658 seconds/image, Memory Usage: 147.68 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Generation 1: subnetwork model has loaded 102 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.2446, Test Accuracy: 65.68%, Latency: 0.003916 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 1/3, Loss: 3154.0360, Test Loss: 1.2446, Test Accuracy: 65.68%, Test Latency: 0.003916 seconds/image\n",
      "Test Loss: 1.1546, Test Accuracy: 68.57%, Latency: 0.003955 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 2/3, Loss: 1366.7031, Test Loss: 1.1546, Test Accuracy: 68.57%, Test Latency: 0.003955 seconds/image\n",
      "Test Loss: 1.2091, Test Accuracy: 68.70%, Latency: 0.003951 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 3/3, Loss: 812.7606, Test Loss: 1.2091, Test Accuracy: 68.70%, Test Latency: 0.003951 seconds/image\n",
      "Test Loss: 1.2091, Test Accuracy: 68.70%, Latency: 0.003857 seconds/image, Memory Usage: 219.73 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Generation 1: subnetwork model has loaded 78 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=16, MLP Ratio=6.0\n",
      "Test Loss: 3.2491, Test Accuracy: 24.85%, Latency: 0.004307 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 1/3, Loss: 6189.4380, Test Loss: 3.2491, Test Accuracy: 24.85%, Test Latency: 0.004307 seconds/image\n",
      "Test Loss: 2.7620, Test Accuracy: 33.72%, Latency: 0.004231 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 2/3, Loss: 4448.3663, Test Loss: 2.7620, Test Accuracy: 33.72%, Test Latency: 0.004231 seconds/image\n",
      "Test Loss: 2.4878, Test Accuracy: 38.75%, Latency: 0.004317 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 3/3, Loss: 3630.9758, Test Loss: 2.4878, Test Accuracy: 38.75%, Test Latency: 0.004317 seconds/image\n",
      "Test Loss: 2.4878, Test Accuracy: 38.75%, Latency: 0.004327 seconds/image, Memory Usage: 291.78 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Generation 1: subnetwork model has loaded 114 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=12, MLP Ratio=6.0\n",
      "Test Loss: 3.4032, Test Accuracy: 21.36%, Latency: 0.004814 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 1/3, Loss: 6496.2355, Test Loss: 3.4032, Test Accuracy: 21.36%, Test Latency: 0.004814 seconds/image\n",
      "Test Loss: 2.8307, Test Accuracy: 31.85%, Latency: 0.004824 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 2/3, Loss: 4686.1887, Test Loss: 2.8307, Test Accuracy: 31.85%, Test Latency: 0.004824 seconds/image\n",
      "Test Loss: 2.5640, Test Accuracy: 37.01%, Latency: 0.004917 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 3/3, Loss: 3843.6754, Test Loss: 2.5640, Test Accuracy: 37.01%, Test Latency: 0.004917 seconds/image\n",
      "Test Loss: 2.5640, Test Accuracy: 37.01%, Latency: 0.004950 seconds/image, Memory Usage: 435.95 MB\n",
      "Saved top-ranked model: Generation 1, Rank 1 (Acc=37.52%, Lat=0.003791, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 2 (Acc=36.44%, Lat=0.003658, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 3 (Acc=68.70%, Lat=0.003857, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 4 (Acc=38.75%, Lat=0.004327, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 5 (Acc=37.01%, Lat=0.004950, Mem=0.00MB)\n",
      "Crossover result: Child1=(10, 4, 2.0, 768), Child2=(8, 4, 2.0, 768)\n",
      "\n",
      "--- Generation 2/2 ---\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (10, 4, 2.0, 768).\n",
      "Generation 2: subnetwork model has loaded 96 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.3900, Test Accuracy: 41.53%, Latency: 0.003841 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 1/3, Loss: 3078.4997, Test Loss: 2.3900, Test Accuracy: 41.53%, Test Latency: 0.003841 seconds/image\n",
      "Test Loss: 2.3999, Test Accuracy: 42.37%, Latency: 0.003854 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 2/3, Loss: 2485.9753, Test Loss: 2.3999, Test Accuracy: 42.37%, Test Latency: 0.003854 seconds/image\n",
      "Test Loss: 2.6091, Test Accuracy: 41.08%, Latency: 0.003862 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 3/3, Loss: 1869.8208, Test Loss: 2.6091, Test Accuracy: 41.08%, Test Latency: 0.003862 seconds/image\n",
      "Test Loss: 2.6091, Test Accuracy: 41.08%, Latency: 0.003870 seconds/image, Memory Usage: 183.75 MB\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (8, 4, 2.0, 768).\n",
      "Generation 2: subnetwork model has loaded 78 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.4318, Test Accuracy: 40.44%, Latency: 0.003736 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 1/3, Loss: 3241.1761, Test Loss: 2.4318, Test Accuracy: 40.44%, Test Latency: 0.003736 seconds/image\n",
      "Test Loss: 2.4776, Test Accuracy: 40.91%, Latency: 0.003588 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 2/3, Loss: 2676.5730, Test Loss: 2.4776, Test Accuracy: 40.91%, Test Latency: 0.003588 seconds/image\n",
      "Test Loss: 2.5187, Test Accuracy: 41.71%, Latency: 0.003653 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 3/3, Loss: 2064.0875, Test Loss: 2.5187, Test Accuracy: 41.71%, Test Latency: 0.003653 seconds/image\n",
      "Test Loss: 2.5187, Test Accuracy: 41.71%, Latency: 0.003791 seconds/image, Memory Usage: 147.68 MB\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (10, 4, 2.0, 768).\n",
      "Generation 2: subnetwork model has loaded 96 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.8224, Test Accuracy: 41.04%, Latency: 0.003723 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 1/3, Loss: 1257.5861, Test Loss: 2.8224, Test Accuracy: 41.04%, Test Latency: 0.003723 seconds/image\n",
      "Test Loss: 3.1428, Test Accuracy: 40.16%, Latency: 0.003803 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 2/3, Loss: 796.3019, Test Loss: 3.1428, Test Accuracy: 40.16%, Test Latency: 0.003803 seconds/image\n",
      "Test Loss: 3.2871, Test Accuracy: 40.61%, Latency: 0.003737 seconds/image, Memory Usage: 183.75 MB\n",
      "Epoch 3/3, Loss: 565.8177, Test Loss: 3.2871, Test Accuracy: 40.61%, Test Latency: 0.003737 seconds/image\n",
      "Test Loss: 3.2871, Test Accuracy: 40.61%, Latency: 0.003780 seconds/image, Memory Usage: 183.75 MB\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (8, 4, 2.0, 768).\n",
      "Generation 2: subnetwork model has loaded 78 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.7973, Test Accuracy: 40.66%, Latency: 0.003563 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 1/3, Loss: 1416.9168, Test Loss: 2.7973, Test Accuracy: 40.66%, Test Latency: 0.003563 seconds/image\n",
      "Test Loss: 3.0716, Test Accuracy: 40.16%, Latency: 0.003668 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 2/3, Loss: 894.7742, Test Loss: 3.0716, Test Accuracy: 40.16%, Test Latency: 0.003668 seconds/image\n",
      "Test Loss: 3.2868, Test Accuracy: 39.73%, Latency: 0.003652 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 3/3, Loss: 595.8552, Test Loss: 3.2868, Test Accuracy: 39.73%, Test Latency: 0.003652 seconds/image\n",
      "Test Loss: 3.2868, Test Accuracy: 39.73%, Latency: 0.003780 seconds/image, Memory Usage: 147.68 MB\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (8, 8, 4.0, 768).\n",
      "Generation 2: subnetwork model has loaded 102 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.2923, Test Accuracy: 68.24%, Latency: 0.003983 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 1/3, Loss: 574.0237, Test Loss: 1.2923, Test Accuracy: 68.24%, Test Latency: 0.003983 seconds/image\n",
      "Test Loss: 1.4234, Test Accuracy: 67.90%, Latency: 0.003940 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 2/3, Loss: 356.7404, Test Loss: 1.4234, Test Accuracy: 67.90%, Test Latency: 0.003940 seconds/image\n",
      "Test Loss: 1.4802, Test Accuracy: 67.33%, Latency: 0.003932 seconds/image, Memory Usage: 219.73 MB\n",
      "Epoch 3/3, Loss: 323.9573, Test Loss: 1.4802, Test Accuracy: 67.33%, Test Latency: 0.003932 seconds/image\n",
      "Test Loss: 1.4802, Test Accuracy: 67.33%, Latency: 0.003999 seconds/image, Memory Usage: 219.73 MB\n",
      "Generation 2: Loaded fine-tuned weights from previous generation for (8, 16, 6.0, 768).\n",
      "Generation 2: subnetwork model has loaded 78 layers from pretrained weights.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=16, MLP Ratio=6.0\n",
      "Test Loss: 2.4232, Test Accuracy: 41.26%, Latency: 0.004200 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 1/3, Loss: 2831.4699, Test Loss: 2.4232, Test Accuracy: 41.26%, Test Latency: 0.004200 seconds/image\n",
      "Test Loss: 2.5283, Test Accuracy: 41.71%, Latency: 0.005200 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 2/3, Loss: 2112.9186, Test Loss: 2.5283, Test Accuracy: 41.71%, Test Latency: 0.005200 seconds/image\n",
      "Test Loss: 2.8337, Test Accuracy: 41.79%, Latency: 0.004240 seconds/image, Memory Usage: 291.78 MB\n",
      "Epoch 3/3, Loss: 1335.4027, Test Loss: 2.8337, Test Accuracy: 41.79%, Test Latency: 0.004240 seconds/image\n",
      "Test Loss: 2.8337, Test Accuracy: 41.79%, Latency: 0.004357 seconds/image, Memory Usage: 291.78 MB\n",
      "Saved top-ranked model: Generation 2, Rank 1 (Acc=40.61%, Lat=0.003780, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 2 (Acc=39.73%, Lat=0.003780, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 3 (Acc=67.33%, Lat=0.003999, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 4 (Acc=41.79%, Lat=0.004357, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 5 (Acc=37.01%, Lat=0.004950, Mem=0.00MB)\n",
      "Crossover result: Child1=(8, 4, 2.0, 768), Child2=(10, 4, 2.0, 768)\n",
      "Crossover result: Child1=(8, 16, 6.0, 768), Child2=(8, 8, 4.0, 768)\n",
      "Mutated architecture: Depth=12, Num Heads=4, MLP Ratio=4.0, Embed Dim=768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10, 4, 2.0, 768),\n",
       " (8, 4, 2.0, 768),\n",
       " (12, 4, 4.0, 768),\n",
       " (10, 4, 2.0, 768),\n",
       " (8, 16, 6.0, 768),\n",
       " (8, 8, 4.0, 768)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol1'\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([4, 6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200  \n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Evaluate architecture: accuracy, latency, and memory usage\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start measuring inference latency\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Measure total time for inference (latency)\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute memory usage (rough estimation)\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "    # Compute average loss\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "# Fine-tune model on dataset (train for a few epochs)\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "    # Save the model after fine-tuning\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "\n",
    "def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "\n",
    "    # Evaluate and store metrics only once per architecture per generation\n",
    "    arch_performance = {}\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        \n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "            # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "            #                    depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "            #                    num_classes=1000).to(device)\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                                num_classes=200).to(device)\n",
    "\n",
    "\n",
    "            # if generation == 0:\n",
    "            #     load_pretrained_weights(model)\n",
    "            # else:\n",
    "            #     checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "            #     model.load_state_dict(torch.load(checkpoint_path))\n",
    "            #     print(f\"Loaded weights from previous generation for {arch}\")\n",
    "            \n",
    "            if generation == 0:\n",
    "                load_pretrained_weights(model)\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    model.load_state_dict(torch.load(checkpoint_path))\n",
    "                    print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "                else:\n",
    "                    print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "                    load_pretrained_weights(model)\n",
    "\n",
    "            check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "\n",
    "            fine_tune_model(\n",
    "                model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder\n",
    "            )\n",
    "\n",
    "            # Evaluate once per architecture\n",
    "            accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "            arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        # Save top models clearly ranked (1 = best)\n",
    "        save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Generate offspring using crossover and mutation\n",
    "        offspring = []\n",
    "        for i in range(0, len(population)-1, 2):\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([population[i], population[i + 1]])\n",
    "\n",
    "        offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "\n",
    "        # Next-generation combines top parents and offspring\n",
    "        population = population[:len(population)//2] + offspring\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol1'\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([4, 6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200  \n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Evaluate architecture: accuracy, latency, and memory usage\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Start measuring inference latency\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             y_true.extend(labels.cpu().numpy())\n",
    "#             y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "#     # Measure total time for inference (latency)\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "#     # Compute accuracy\n",
    "#     accuracy = 100 * correct / total\n",
    "\n",
    "#     # Compute memory usage (rough estimation)\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "#     # Compute average loss\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Start measuring inference latency\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Measure total time for inference (latency)\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Compute memory usage (improved)\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "    if torch.cuda.is_available():\n",
    "        memory_usage += torch.cuda.max_memory_allocated() / (1024 ** 2)  # Add GPU memory usage\n",
    "    \n",
    "    # Compute average loss\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "# def estimate_memory_usage(model):\n",
    "#     # Create dummy input matching the expected shape of the input tensor\n",
    "#     dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "#     # Use torch.utils.benchmark to measure memory usage during inference\n",
    "#     start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "#     # Run the model once with the dummy input\n",
    "#     with torch.no_grad():\n",
    "#         model(dummy_input)\n",
    "    \n",
    "#     end_mem = torch.cuda.memory_allocated()\n",
    "#     memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "#     return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "# def pareto_selection(arch_performance):\n",
    "#     def dominates(perf1, perf2):\n",
    "#         acc1, lat1, mem1 = perf1\n",
    "#         acc2, lat2, mem2 = perf2\n",
    "#         return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, perf1 in arch_performance.items():\n",
    "#         dominated_count = 0\n",
    "#         for arch2, perf2 in arch_performance.items():\n",
    "#             if arch1 != arch2 and dominates(perf2, perf1):\n",
    "#                 dominated_count += 1\n",
    "#         ranks[arch1] = dominated_count\n",
    "\n",
    "#     # Sort architectures by rank (lower dominated_count = better)\n",
    "#     sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "#     return sorted_population\n",
    "\n",
    "def pareto_selection(arch_performance, population_size):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        # For accuracy, higher is better; for latency and memory, lower is better\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    # Initialize fronts\n",
    "    fronts = [[]]\n",
    "    dominated = {arch: set() for arch in arch_performance}\n",
    "    domination_count = {arch: 0 for arch in arch_performance}\n",
    "\n",
    "    # Calculate dominance relationships\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2:\n",
    "                if dominates(perf1, perf2):\n",
    "                    dominated[arch1].add(arch2)\n",
    "                elif dominates(perf2, perf1):\n",
    "                    domination_count[arch1] += 1\n",
    "        \n",
    "        # If not dominated by any other architecture, add to first front\n",
    "        if domination_count[arch1] == 0:\n",
    "            fronts[0].append(arch1)\n",
    "\n",
    "    # Calculate remaining fronts\n",
    "    i = 0\n",
    "    while len(fronts[i]) > 0:\n",
    "        next_front = []\n",
    "        for arch in fronts[i]:\n",
    "            for dominated_arch in dominated[arch]:\n",
    "                domination_count[dominated_arch] -= 1\n",
    "                if domination_count[dominated_arch] == 0:\n",
    "                    next_front.append(dominated_arch)\n",
    "        i += 1\n",
    "        fronts.append(next_front)\n",
    "\n",
    "    # Select architectures based on fronts and crowding distance\n",
    "    selected = []\n",
    "    for front in fronts:\n",
    "        if len(selected) + len(front) <= population_size:\n",
    "            selected.extend(front)\n",
    "        else:\n",
    "            # Calculate crowding distance for the current front\n",
    "            crowding_distance = {}\n",
    "            for arch in front:\n",
    "                crowding_distance[arch] = 0\n",
    "            \n",
    "            # For each objective (accuracy, latency, memory)\n",
    "            for obj_idx in range(3):\n",
    "                # Sort front by the current objective\n",
    "                if obj_idx == 0:  # Accuracy (higher is better)\n",
    "                    sorted_front = sorted(front, key=lambda x: arch_performance[x][obj_idx], reverse=True)\n",
    "                else:  # Latency and memory (lower is better)\n",
    "                    sorted_front = sorted(front, key=lambda x: arch_performance[x][obj_idx])\n",
    "                \n",
    "                # Set boundary points to infinity\n",
    "                crowding_distance[sorted_front[0]] = float('inf')\n",
    "                crowding_distance[sorted_front[-1]] = float('inf')\n",
    "                \n",
    "                # Calculate crowding distance for intermediate points\n",
    "                for i in range(1, len(sorted_front) - 1):\n",
    "                    if crowding_distance[sorted_front[i]] != float('inf'):\n",
    "                        # Normalize by the range of the objective\n",
    "                        obj_range = arch_performance[sorted_front[0]][obj_idx] - arch_performance[sorted_front[-1]][obj_idx]\n",
    "                        if obj_range != 0:\n",
    "                            prev_val = arch_performance[sorted_front[i-1]][obj_idx]\n",
    "                            next_val = arch_performance[sorted_front[i+1]][obj_idx]\n",
    "                            crowding_distance[sorted_front[i]] += abs(next_val - prev_val) / obj_range\n",
    "            \n",
    "            # Sort by crowding distance (higher is better)\n",
    "            sorted_front = sorted(front, key=lambda x: crowding_distance[x], reverse=True)\n",
    "            selected.extend(sorted_front[:population_size - len(selected)])\n",
    "            break\n",
    "\n",
    "    return selected\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "def plot_training_progress(train_losses, test_losses, test_accuracies, depth, num_heads, mlp_ratio):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.title('Train Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "    plt.title('Test Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, test_accuracies, 'g-', label='Test Accuracy')\n",
    "    plt.title('Test Accuracy vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.suptitle(f'Training Progress: Depth={depth}, Heads={num_heads}, MLP Ratio={mlp_ratio}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{768}\")\n",
    "    os.makedirs(architecture_folder, exist_ok=True)\n",
    "    plt.savefig(os.path.join(architecture_folder, 'training_progress.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "# Fine-tune model on dataset (train for a few epochs)\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "#     # Save the model after fine-tuning\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=5, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Execution Time: {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plot_training_progress(train_losses, test_losses, test_accuracies, sampled_model.depth, sampled_model.num_heads, sampled_model.mlp_ratio)\n",
    "    \n",
    "    # Save the model\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    \n",
    "    return sampled_model\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        \n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "\n",
    "# def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "\n",
    "#     # Evaluate and store metrics only once per architecture per generation\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        \n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "            \n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                 depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                 num_classes=200).to(device)\n",
    "\n",
    "\n",
    "            \n",
    "#             if generation == 0:\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#                 if os.path.exists(checkpoint_path):\n",
    "#                     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                     print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "#                 else:\n",
    "#                     print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "#                     load_pretrained_weights(model)\n",
    "\n",
    "#             check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "\n",
    "#             fine_tune_model(\n",
    "#                 model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder\n",
    "#             )\n",
    "\n",
    "#             # Evaluate once per architecture\n",
    "#             accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Save top models clearly ranked (1 = best)\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Generate offspring using crossover and mutation\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([population[i], population[i + 1]])\n",
    "\n",
    "#         offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "\n",
    "#         # Next-generation combines top parents and offspring\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    \n",
    "    # Evaluate and store metrics only once per architecture per generation\n",
    "    arch_performance = {}\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        generation_start_time = time.time()\n",
    "        \n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            \n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                              depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                              num_classes=200).to(device)\n",
    "            \n",
    "            if generation == 0:\n",
    "                load_pretrained_weights(model)\n",
    "            else:\n",
    "                checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    model.load_state_dict(torch.load(checkpoint_path))\n",
    "                    print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "                else:\n",
    "                    print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "                    load_pretrained_weights(model)\n",
    "            \n",
    "            check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "            \n",
    "            # Time the fine-tuning process\n",
    "            arch_start_time = time.time()\n",
    "            fine_tune_model(\n",
    "                model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder\n",
    "            )\n",
    "            arch_time = time.time() - arch_start_time\n",
    "            \n",
    "            # Evaluate once per architecture\n",
    "            accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "            arch_performance[arch] = (accuracy, latency, memory)\n",
    "            \n",
    "            print(f\"Architecture {arch} total execution time: {arch_time:.2f} seconds\")\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Pareto selection with population size\n",
    "        population = pareto_selection(arch_performance, population_size)\n",
    "        \n",
    "        # Save top models clearly ranked (1 = best)\n",
    "        save_top_ranked_models(population, arch_performance, generation)\n",
    "        \n",
    "        # Generate offspring using crossover and mutation\n",
    "        offspring = []\n",
    "        for i in range(0, len(population)-1, 2):\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([population[i], population[i + 1]])\n",
    "        \n",
    "        offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "        \n",
    "        # Next-generation combines top parents and offspring\n",
    "        population = population[:len(population)//2] + offspring\n",
    "        \n",
    "        generation_time = time.time() - generation_start_time\n",
    "        print(f\"Generation {generation + 1} completed in {generation_time:.2f} seconds\")\n",
    "    \n",
    "    return population\n",
    "\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pareto_selection(arch_performance, population_size):\n",
    "#     def dominates(perf1, perf2):\n",
    "#         acc1, lat1, mem1 = perf1\n",
    "#         acc2, lat2, mem2 = perf2\n",
    "#         # For accuracy, higher is better; for latency and memory, lower is better\n",
    "#         return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "#     # Initialize fronts\n",
    "#     fronts = [[]]\n",
    "#     dominated = {arch: set() for arch in arch_performance}\n",
    "#     domination_count = {arch: 0 for arch in arch_performance}\n",
    "\n",
    "#     # Calculate dominance relationships\n",
    "#     for arch1, perf1 in arch_performance.items():\n",
    "#         for arch2, perf2 in arch_performance.items():\n",
    "#             if arch1 != arch2:\n",
    "#                 if dominates(perf1, perf2):\n",
    "#                     dominated[arch1].add(arch2)\n",
    "#                 elif dominates(perf2, perf1):\n",
    "#                     domination_count[arch1] += 1\n",
    "        \n",
    "#         # If not dominated by any other architecture, add to first front\n",
    "#         if domination_count[arch1] == 0:\n",
    "#             fronts[0].append(arch1)\n",
    "\n",
    "#     # Calculate remaining fronts\n",
    "#     i = 0\n",
    "#     while len(fronts[i]) > 0:\n",
    "#         next_front = []\n",
    "#         for arch in fronts[i]:\n",
    "#             for dominated_arch in dominated[arch]:\n",
    "#                 domination_count[dominated_arch] -= 1\n",
    "#                 if domination_count[dominated_arch] == 0:\n",
    "#                     next_front.append(dominated_arch)\n",
    "#         i += 1\n",
    "#         fronts.append(next_front)\n",
    "\n",
    "#     # Select architectures based on fronts and crowding distance\n",
    "#     selected = []\n",
    "#     for front in fronts:\n",
    "#         if len(selected) + len(front) <= population_size:\n",
    "#             selected.extend(front)\n",
    "#         else:\n",
    "#             # Calculate crowding distance for the current front\n",
    "#             crowding_distance = {}\n",
    "#             for arch in front:\n",
    "#                 crowding_distance[arch] = 0\n",
    "            \n",
    "#             # For each objective (accuracy, latency, memory)\n",
    "#             for obj_idx in range(3):\n",
    "#                 # Sort front by the current objective\n",
    "#                 if obj_idx == 0:  # Accuracy (higher is better)\n",
    "#                     sorted_front = sorted(front, key=lambda x: arch_performance[x][obj_idx], reverse=True)\n",
    "#                 else:  # Latency and memory (lower is better)\n",
    "#                     sorted_front = sorted(front, key=lambda x: arch_performance[x][obj_idx])\n",
    "                \n",
    "#                 # Set boundary points to infinity\n",
    "#                 crowding_distance[sorted_front[0]] = float('inf')\n",
    "#                 crowding_distance[sorted_front[-1]] = float('inf')\n",
    "                \n",
    "#                 # Calculate crowding distance for intermediate points\n",
    "#                 for i in range(1, len(sorted_front) - 1):\n",
    "#                     if crowding_distance[sorted_front[i]] != float('inf'):\n",
    "#                         # Normalize by the range of the objective\n",
    "#                         obj_range = arch_performance[sorted_front[0]][obj_idx] - arch_performance[sorted_front[-1]][obj_idx]\n",
    "#                         if obj_range != 0:\n",
    "#                             prev_val = arch_performance[sorted_front[i-1]][obj_idx]\n",
    "#                             next_val = arch_performance[sorted_front[i+1]][obj_idx]\n",
    "#                             crowding_distance[sorted_front[i]] += abs(next_val - prev_val) / obj_range\n",
    "            \n",
    "#             # Sort by crowding distance (higher is better)\n",
    "#             sorted_front = sorted(front, key=lambda x: crowding_distance[x], reverse=True)\n",
    "#             selected.extend(sorted_front[:population_size - len(selected)])\n",
    "#             break\n",
    "\n",
    "#     return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use below functins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_pareto_front(arch_performance):\n",
    "#     accuracies = [v[0] for v in arch_performance.values()]\n",
    "#     latencies = [v[1] for v in arch_performance.values()]\n",
    "#     memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "#     # Accuracy vs Latency\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.scatter(latencies, accuracies, c='blue')\n",
    "#     plt.xlabel('Latency (s/image)')\n",
    "#     plt.ylabel('Accuracy (%)')\n",
    "#     plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Accuracy vs Memory\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.scatter(memories, accuracies, c='green')\n",
    "#     plt.xlabel('Memory (MB)')\n",
    "#     plt.ylabel('Accuracy (%)')\n",
    "#     plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call after your last generation completes:\n",
    "# plot_pareto_front(arch_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evolutionary_algorithm(population_size=10, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     prev_best_accuracy = 0\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "#         # Fine-tuning and evaluation\n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                num_classes=200).to(device)\n",
    "\n",
    "#             checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "#             # Load weights only once clearly\n",
    "#             if generation == 0 or not os.path.exists(checkpoint_path):\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                 print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "\n",
    "#             fine_tune_model(model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder)\n",
    "\n",
    "#             accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#             memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Saving clearly ranked top models\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Check Pareto front convergence (stopping criteria)\n",
    "#         current_best_accuracy = arch_performance[population[0]][0]\n",
    "#         if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "#             no_improvement_count += 1\n",
    "#             print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "#             if no_improvement_count >= 2:\n",
    "#                 print(\"Pareto front has converged. Stopping early.\")\n",
    "#                 break\n",
    "#         else:\n",
    "#             no_improvement_count = 0\n",
    "#         prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "#         # Generate offspring\n",
    "#         next_population = population[:len(population)//2]  # Only top half\n",
    "#         offspring = []\n",
    "\n",
    "#         for i in range(0, len(next_population)-1, 2):\n",
    "#             parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(parent1, parent2)\n",
    "#                 print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([parent1, parent2])\n",
    "\n",
    "#         # Mutation with clear logging\n",
    "#         mutated_offspring = []\n",
    "#         for child in offspring:\n",
    "#             if random.random() < mutation_rate:\n",
    "#                 original_child = child\n",
    "#                 child = mutate(child)\n",
    "#                 print(f\"Mutated from {original_child} to {child}\")\n",
    "#             mutated_offspring.append(child)\n",
    "\n",
    "#         population = next_population + mutated_offspring\n",
    "\n",
    "#     # Plot Pareto Front at end\n",
    "#     plot_pareto_front(arch_performance)\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Call the algorithm\n",
    "# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "# from timm import create_model\n",
    "# import time\n",
    "\n",
    "# # Path to save models\n",
    "# SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol1'\n",
    "\n",
    "# # Device setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load pretrained ViT weights initially\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items()\n",
    "#                      if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Loaded pretrained weights into {model.__class__.__name__}\")\n",
    "\n",
    "# # Count parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # Evaluate model clearly with corrected memory usage\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct, total, running_loss = 0, 0, 0.0\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)  # MB for FP32\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Loss: {test_loss:.4f}, Acc: {accuracy:.2f}%, Lat: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# # Mutate architecture\n",
    "# def mutate(arch):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#     if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# # Crossover architectures\n",
    "# def one_point_crossover(p1, p2):\n",
    "#     cp = random.randint(1, 3)\n",
    "#     return p1[:cp] + p2[cp:], p2[:cp] + p1[cp:]\n",
    "\n",
    "# # Pareto ranking\n",
    "# def pareto_selection(perf):\n",
    "#     def dominates(a, b):\n",
    "#         return all(x >= y for x, y in zip(a, b)) and any(x > y for x, y in zip(a, b))\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, p1 in perf.items():\n",
    "#         dominated = sum(dominates(p2, p1) for arch2, p2 in perf.items() if arch2 != arch1)\n",
    "#         ranks[arch1] = dominated\n",
    "#     return sorted(ranks, key=ranks.get)\n",
    "\n",
    "# # Fine-tune model\n",
    "# def fine_tune_model(model, train_loader, test_loader, epochs, folder):\n",
    "#     criterion, optimizer = nn.CrossEntropyLoss(), Adam(model.parameters(), lr=1e-4)\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = criterion(model(images), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         acc, loss, lat, mem = evaluate_architecture(model, test_loader)\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} done.\")\n",
    "\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "#     torch.save(model.state_dict(), os.path.join(folder, 'checkpoint.pth'))\n",
    "\n",
    "# # Evolutionary NAS main loop\n",
    "# def evolutionary_algorithm(population_size, generations, train_loader, test_loader):\n",
    "#     seen, perf = set(), {}\n",
    "#     population = [(random.choice([4,6,8,10,12]), random.choice([4,8,12,16]), random.choice([2.0,4.0,6.0]), 768)\n",
    "#                   for _ in range(population_size)]\n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nGeneration {gen+1}/{generations}\")\n",
    "\n",
    "#         for arch in population:\n",
    "#             folder = os.path.join(SAVE_PATH, f\"arch_{arch[0]}_{arch[1]}_{arch[2]}_{arch[3]}\")\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3], depth=arch[0],\n",
    "#                                num_heads=arch[1], mlp_ratio=arch[2], num_classes=200).to(device)\n",
    "\n",
    "#             ckpt = os.path.join(folder, 'checkpoint.pth')\n",
    "#             if os.path.exists(ckpt):\n",
    "#                 model.load_state_dict(torch.load(ckpt))\n",
    "#                 print(f\"Loaded previous weights for {arch}\")\n",
    "#             else:\n",
    "#                 print(f\"No previous weights, loading pretrained for {arch}\")\n",
    "#                 load_pretrained_weights(model)\n",
    "\n",
    "#             fine_tune_model(model, train_loader, test_loader, 3, folder)\n",
    "#             perf[arch] = evaluate_architecture(model, test_loader)\n",
    "#             del model; torch.cuda.empty_cache()\n",
    "\n",
    "#         population = pareto_selection(perf)\n",
    "\n",
    "#         for i, arch in enumerate(population[:5]):\n",
    "#             print(f\"Rank {i+1}: {arch}, Acc: {perf[arch][0]:.2f}%\")\n",
    "\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             child1, child2 = one_point_crossover(population[i], population[i+1])\n",
    "#             offspring.extend([mutate(child1), mutate(child2)])\n",
    "\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Example run:\n",
    "# # evolutionary_algorithm(5, 2, train_loader, test_loader)\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Call after your last generation completes:\n",
    "# plot_pareto_front(arch_performance)\n",
    "\n",
    "\n",
    "# def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "\n",
    "#     # Evaluate and store metrics only once per architecture per generation\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        \n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "#             # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#             #                    depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#             #                    num_classes=1000).to(device)\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                 depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                 num_classes=200).to(device)\n",
    "\n",
    "\n",
    "#             # if generation == 0:\n",
    "#             #     load_pretrained_weights(model)\n",
    "#             # else:\n",
    "#             #     checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#             #     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#             #     print(f\"Loaded weights from previous generation for {arch}\")\n",
    "            \n",
    "#             if generation == 0:\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#                 if os.path.exists(checkpoint_path):\n",
    "#                     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                     print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "#                 else:\n",
    "#                     print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "#                     load_pretrained_weights(model)\n",
    "\n",
    "#             check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "\n",
    "#             fine_tune_model(\n",
    "#                 model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder\n",
    "#             )\n",
    "\n",
    "#             # Evaluate once per architecture\n",
    "#             accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Save top models clearly ranked (1 = best)\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Generate offspring using crossover and mutation\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([population[i], population[i + 1]])\n",
    "\n",
    "#         offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "\n",
    "#         # Next-generation combines top parents and offspring\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# def evolutionary_algorithm(population_size=10, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     prev_best_accuracy = 0\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "#         # Fine-tuning and evaluation\n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                num_classes=200).to(device)\n",
    "\n",
    "#             checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "#             # Load weights only once clearly\n",
    "#             if generation == 0 or not os.path.exists(checkpoint_path):\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                 print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "\n",
    "#             fine_tune_model(model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder)\n",
    "\n",
    "#             accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#             memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Saving clearly ranked top models\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Check Pareto front convergence (stopping criteria)\n",
    "#         current_best_accuracy = arch_performance[population[0]][0]\n",
    "#         if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "#             no_improvement_count += 1\n",
    "#             print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "#             if no_improvement_count >= 2:\n",
    "#                 print(\"Pareto front has converged. Stopping early.\")\n",
    "#                 break\n",
    "#         else:\n",
    "#             no_improvement_count = 0\n",
    "#         prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "#         # Generate offspring\n",
    "#         next_population = population[:len(population)//2]  # Only top half\n",
    "#         offspring = []\n",
    "\n",
    "#         for i in range(0, len(next_population)-1, 2):\n",
    "#             parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(parent1, parent2)\n",
    "#                 print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([parent1, parent2])\n",
    "\n",
    "#         # Mutation with clear logging\n",
    "#         mutated_offspring = []\n",
    "#         for child in offspring:\n",
    "#             if random.random() < mutation_rate:\n",
    "#                 original_child = child\n",
    "#                 child = mutate(child)\n",
    "#                 print(f\"Mutated from {original_child} to {child}\")\n",
    "#             mutated_offspring.append(child)\n",
    "\n",
    "#         population = next_population + mutated_offspring\n",
    "\n",
    "#     # Plot Pareto Front at end\n",
    "#     plot_pareto_front(arch_performance)\n",
    "\n",
    "#     return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25 march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## after below run above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=10, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 71,776,712\n",
      "Sampled architecture: Depth=12, Num Heads=4, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 114,282,440\n",
      "Sampled architecture: Depth=4, Num Heads=4, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,249,480\n",
      "Sampled architecture: Depth=10, Num Heads=4, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 95,385,032\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,590,216\n",
      "Sampled architecture: Depth=4, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 19,806,152\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 38,714,312\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 85,952,456\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 71,776,712\n",
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 43,425,224\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.0156, Test Accuracy: 71.26%, Latency: 0.004171 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 2378.7215, Test Loss: 1.0156, Test Accuracy: 71.26%, Test Latency: 0.004171 seconds/image\n",
      "Test Loss: 0.9877, Test Accuracy: 72.63%, Latency: 0.004071 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 1065.9891, Test Loss: 0.9877, Test Accuracy: 72.63%, Test Latency: 0.004071 seconds/image\n",
      "Test Loss: 1.0304, Test Accuracy: 73.04%, Latency: 0.004173 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 707.1706, Test Loss: 1.0304, Test Accuracy: 73.04%, Test Latency: 0.004173 seconds/image\n",
      "Test Loss: 1.0304, Test Accuracy: 73.04%, Latency: 0.004081 seconds/image, Memory Usage: 273.81 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=4, MLP Ratio=6.0\n",
      "Test Loss: 3.6842, Test Accuracy: 17.01%, Latency: 0.004779 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 1/3, Loss: 6953.3091, Test Loss: 3.6842, Test Accuracy: 17.01%, Test Latency: 0.004779 seconds/image\n",
      "Test Loss: 3.0513, Test Accuracy: 27.74%, Latency: 0.005770 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 2/3, Loss: 5152.3928, Test Loss: 3.0513, Test Accuracy: 27.74%, Test Latency: 0.005770 seconds/image\n",
      "Test Loss: 2.7357, Test Accuracy: 33.64%, Latency: 0.006769 seconds/image, Memory Usage: 435.95 MB\n",
      "Epoch 3/3, Loss: 4263.8944, Test Loss: 2.7357, Test Accuracy: 33.64%, Test Latency: 0.006769 seconds/image\n",
      "Test Loss: 2.7357, Test Accuracy: 33.64%, Latency: 0.006635 seconds/image, Memory Usage: 435.95 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 2.4233, Test Accuracy: 39.59%, Latency: 0.003359 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 5116.8079, Test Loss: 2.4233, Test Accuracy: 39.59%, Test Latency: 0.003359 seconds/image\n",
      "Test Loss: 2.0218, Test Accuracy: 48.15%, Latency: 0.003370 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 3118.5446, Test Loss: 2.0218, Test Accuracy: 48.15%, Test Latency: 0.003370 seconds/image\n",
      "Test Loss: 1.9485, Test Accuracy: 50.51%, Latency: 0.003360 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 2306.0868, Test Loss: 1.9485, Test Accuracy: 50.51%, Test Latency: 0.003360 seconds/image\n",
      "Test Loss: 1.9485, Test Accuracy: 50.51%, Latency: 0.003361 seconds/image, Memory Usage: 111.58 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=4, MLP Ratio=6.0\n",
      "Test Loss: 3.4978, Test Accuracy: 20.52%, Latency: 0.004460 seconds/image, Memory Usage: 363.87 MB\n",
      "Epoch 1/3, Loss: 6641.4626, Test Loss: 3.4978, Test Accuracy: 20.52%, Test Latency: 0.004460 seconds/image\n",
      "Test Loss: 2.9593, Test Accuracy: 29.34%, Latency: 0.004375 seconds/image, Memory Usage: 363.87 MB\n",
      "Epoch 2/3, Loss: 4894.4533, Test Loss: 2.9593, Test Accuracy: 29.34%, Test Latency: 0.004375 seconds/image\n",
      "Test Loss: 2.6392, Test Accuracy: 35.86%, Latency: 0.004370 seconds/image, Memory Usage: 363.87 MB\n",
      "Epoch 3/3, Loss: 4020.8896, Test Loss: 2.6392, Test Accuracy: 35.86%, Test Latency: 0.004370 seconds/image\n",
      "Test Loss: 2.6392, Test Accuracy: 35.86%, Latency: 0.004481 seconds/image, Memory Usage: 363.87 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=16, MLP Ratio=6.0\n",
      "Test Loss: 3.3424, Test Accuracy: 22.87%, Latency: 0.004039 seconds/image, Memory Usage: 219.69 MB\n",
      "Epoch 1/3, Loss: 6322.5748, Test Loss: 3.3424, Test Accuracy: 22.87%, Test Latency: 0.004039 seconds/image\n",
      "Test Loss: 2.7865, Test Accuracy: 33.14%, Latency: 0.005291 seconds/image, Memory Usage: 219.69 MB\n",
      "Epoch 2/3, Loss: 4575.6322, Test Loss: 2.7865, Test Accuracy: 33.14%, Test Latency: 0.005291 seconds/image\n",
      "Test Loss: 2.5860, Test Accuracy: 37.58%, Latency: 0.005275 seconds/image, Memory Usage: 219.69 MB\n",
      "Epoch 3/3, Loss: 3755.8569, Test Loss: 2.5860, Test Accuracy: 37.58%, Test Latency: 0.005275 seconds/image\n",
      "Test Loss: 2.5860, Test Accuracy: 37.58%, Latency: 0.005212 seconds/image, Memory Usage: 219.69 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 3.5580, Test Accuracy: 19.58%, Latency: 0.003846 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 1/3, Loss: 6570.0503, Test Loss: 3.5580, Test Accuracy: 19.58%, Test Latency: 0.003846 seconds/image\n",
      "Test Loss: 3.1121, Test Accuracy: 26.92%, Latency: 0.003819 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 2/3, Loss: 5056.0951, Test Loss: 3.1121, Test Accuracy: 26.92%, Test Latency: 0.003819 seconds/image\n",
      "Test Loss: 2.8596, Test Accuracy: 31.61%, Latency: 0.003663 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 3/3, Loss: 4342.0827, Test Loss: 2.8596, Test Accuracy: 31.61%, Test Latency: 0.003663 seconds/image\n",
      "Test Loss: 2.8596, Test Accuracy: 31.61%, Latency: 0.003365 seconds/image, Memory Usage: 75.55 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 3.1245, Test Accuracy: 27.35%, Latency: 0.003726 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 1/3, Loss: 6035.4412, Test Loss: 3.1245, Test Accuracy: 27.35%, Test Latency: 0.003726 seconds/image\n",
      "Test Loss: 2.6014, Test Accuracy: 36.72%, Latency: 0.003728 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 2/3, Loss: 4240.6780, Test Loss: 2.6014, Test Accuracy: 36.72%, Test Latency: 0.003728 seconds/image\n",
      "Test Loss: 2.4601, Test Accuracy: 40.41%, Latency: 0.003755 seconds/image, Memory Usage: 147.68 MB\n",
      "Epoch 3/3, Loss: 3425.6952, Test Loss: 2.4601, Test Accuracy: 40.41%, Test Latency: 0.003755 seconds/image\n",
      "Test Loss: 2.4601, Test Accuracy: 40.41%, Latency: 0.003627 seconds/image, Memory Usage: 147.68 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 0.8786, Test Accuracy: 75.50%, Latency: 0.004348 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 1/3, Loss: 2089.7746, Test Loss: 0.8786, Test Accuracy: 75.50%, Test Latency: 0.004348 seconds/image\n",
      "Test Loss: 0.9033, Test Accuracy: 75.45%, Latency: 0.004309 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 2/3, Loss: 995.5637, Test Loss: 0.9033, Test Accuracy: 75.45%, Test Latency: 0.004309 seconds/image\n",
      "Test Loss: 0.9349, Test Accuracy: 75.38%, Latency: 0.004318 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 3/3, Loss: 755.9184, Test Loss: 0.9349, Test Accuracy: 75.38%, Test Latency: 0.004318 seconds/image\n",
      "Test Loss: 0.9349, Test Accuracy: 75.38%, Latency: 0.004432 seconds/image, Memory Usage: 327.88 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 0.9696, Test Accuracy: 72.64%, Latency: 0.006043 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 2282.6433, Test Loss: 0.9696, Test Accuracy: 72.64%, Test Latency: 0.006043 seconds/image\n",
      "Test Loss: 0.9113, Test Accuracy: 74.74%, Latency: 0.005948 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 985.8649, Test Loss: 0.9113, Test Accuracy: 74.74%, Test Latency: 0.005948 seconds/image\n",
      "Test Loss: 0.9975, Test Accuracy: 73.91%, Latency: 0.007669 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 628.3671, Test Loss: 0.9975, Test Accuracy: 73.91%, Test Latency: 0.007669 seconds/image\n",
      "Test Loss: 0.9975, Test Accuracy: 73.91%, Latency: 0.007765 seconds/image, Memory Usage: 273.81 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.5807, Test Accuracy: 57.96%, Latency: 0.005529 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 1/3, Loss: 3850.8331, Test Loss: 1.5807, Test Accuracy: 57.96%, Test Latency: 0.005529 seconds/image\n",
      "Test Loss: 1.3809, Test Accuracy: 63.19%, Latency: 0.005566 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 2/3, Loss: 1862.7111, Test Loss: 1.3809, Test Accuracy: 63.19%, Test Latency: 0.005566 seconds/image\n",
      "Test Loss: 1.4173, Test Accuracy: 63.32%, Latency: 0.006066 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 3/3, Loss: 1147.2540, Test Loss: 1.4173, Test Accuracy: 63.32%, Test Latency: 0.006066 seconds/image\n",
      "Test Loss: 1.4173, Test Accuracy: 63.32%, Latency: 0.006016 seconds/image, Memory Usage: 165.65 MB\n",
      "Saved top-ranked model: Generation 1, Rank 1 (Acc=73.04%, Lat=0.004081, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 2 (Acc=50.51%, Lat=0.003361, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 3 (Acc=31.61%, Lat=0.003365, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 4 (Acc=75.38%, Lat=0.004432, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 1, Rank 5 (Acc=73.91%, Lat=0.007765, Mem=0.00MB)\n",
      "Crossover result: Child1=(10, 8, 4.0, 768), Child2=(4, 4, 4.0, 768)\n",
      "Crossover parents: (10, 8, 4.0, 768) & (4, 4, 4.0, 768)\n",
      "Crossover result: Child1=(12, 8, 4.0, 768), Child2=(4, 4, 2.0, 768)\n",
      "Crossover parents: (4, 4, 2.0, 768) & (12, 8, 4.0, 768)\n",
      "\n",
      "--- Generation 2/5 ---\n",
      "Loaded weights from previous generation for architecture (10, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.1050, Test Accuracy: 73.07%, Latency: 0.007344 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 540.6379, Test Loss: 1.1050, Test Accuracy: 73.07%, Test Latency: 0.007344 seconds/image\n",
      "Test Loss: 1.2209, Test Accuracy: 71.81%, Latency: 0.007558 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 382.5404, Test Loss: 1.2209, Test Accuracy: 71.81%, Test Latency: 0.007558 seconds/image\n",
      "Test Loss: 1.2293, Test Accuracy: 71.84%, Latency: 0.007531 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 324.5787, Test Loss: 1.2293, Test Accuracy: 71.84%, Test Latency: 0.007531 seconds/image\n",
      "Test Loss: 1.2293, Test Accuracy: 71.84%, Latency: 0.007495 seconds/image, Memory Usage: 273.81 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 2.0660, Test Accuracy: 50.31%, Latency: 0.005305 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 1645.3406, Test Loss: 2.0660, Test Accuracy: 50.31%, Test Latency: 0.005305 seconds/image\n",
      "Test Loss: 2.1762, Test Accuracy: 49.93%, Latency: 0.005308 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 1138.2203, Test Loss: 2.1762, Test Accuracy: 49.93%, Test Latency: 0.005308 seconds/image\n",
      "Test Loss: 2.3603, Test Accuracy: 49.70%, Latency: 0.005303 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 781.4366, Test Loss: 2.3603, Test Accuracy: 49.70%, Test Latency: 0.005303 seconds/image\n",
      "Test Loss: 2.3603, Test Accuracy: 49.70%, Latency: 0.005337 seconds/image, Memory Usage: 111.58 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 2.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.6642, Test Accuracy: 35.58%, Latency: 0.005014 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 1/3, Loss: 3742.3356, Test Loss: 2.6642, Test Accuracy: 35.58%, Test Latency: 0.005014 seconds/image\n",
      "Test Loss: 2.6636, Test Accuracy: 36.65%, Latency: 0.004981 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 2/3, Loss: 3257.5924, Test Loss: 2.6636, Test Accuracy: 36.65%, Test Latency: 0.004981 seconds/image\n",
      "Test Loss: 2.6843, Test Accuracy: 37.27%, Latency: 0.005007 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 3/3, Loss: 2761.5297, Test Loss: 2.6843, Test Accuracy: 37.27%, Test Latency: 0.005007 seconds/image\n",
      "Test Loss: 2.6843, Test Accuracy: 37.27%, Latency: 0.005113 seconds/image, Memory Usage: 75.55 MB\n",
      "Loaded weights from previous generation for architecture (12, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 0.9615, Test Accuracy: 75.67%, Latency: 0.005929 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 1/3, Loss: 610.0757, Test Loss: 0.9615, Test Accuracy: 75.67%, Test Latency: 0.005929 seconds/image\n",
      "Test Loss: 1.0412, Test Accuracy: 75.04%, Latency: 0.006087 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 2/3, Loss: 475.1242, Test Loss: 1.0412, Test Accuracy: 75.04%, Test Latency: 0.006087 seconds/image\n",
      "Test Loss: 1.1362, Test Accuracy: 73.51%, Latency: 0.004380 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 3/3, Loss: 388.3327, Test Loss: 1.1362, Test Accuracy: 73.51%, Test Latency: 0.004380 seconds/image\n",
      "Test Loss: 1.1362, Test Accuracy: 73.51%, Latency: 0.004349 seconds/image, Memory Usage: 327.88 MB\n",
      "Loaded weights from previous generation for architecture (10, 16, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 1.0619, Test Accuracy: 73.84%, Latency: 0.004210 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 480.5502, Test Loss: 1.0619, Test Accuracy: 73.84%, Test Latency: 0.004210 seconds/image\n",
      "Test Loss: 1.1619, Test Accuracy: 73.20%, Latency: 0.004209 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 332.3676, Test Loss: 1.1619, Test Accuracy: 73.20%, Test Latency: 0.004209 seconds/image\n",
      "Test Loss: 1.2666, Test Accuracy: 71.37%, Latency: 0.004262 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 285.5492, Test Loss: 1.2666, Test Accuracy: 71.37%, Test Latency: 0.004262 seconds/image\n",
      "Test Loss: 1.2666, Test Accuracy: 71.37%, Latency: 0.004334 seconds/image, Memory Usage: 273.81 MB\n",
      "Loaded weights from previous generation for architecture (10, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.2853, Test Accuracy: 72.22%, Latency: 0.004126 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 315.8931, Test Loss: 1.2853, Test Accuracy: 72.22%, Test Latency: 0.004126 seconds/image\n",
      "Test Loss: 1.3151, Test Accuracy: 71.40%, Latency: 0.004223 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 254.5375, Test Loss: 1.3151, Test Accuracy: 71.40%, Test Latency: 0.004223 seconds/image\n",
      "Test Loss: 1.3457, Test Accuracy: 71.61%, Latency: 0.004181 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 238.3937, Test Loss: 1.3457, Test Accuracy: 71.61%, Test Latency: 0.004181 seconds/image\n",
      "Test Loss: 1.3457, Test Accuracy: 71.61%, Latency: 0.004139 seconds/image, Memory Usage: 273.81 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 2.5698, Test Accuracy: 48.90%, Latency: 0.003436 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 638.1800, Test Loss: 2.5698, Test Accuracy: 48.90%, Test Latency: 0.003436 seconds/image\n",
      "Test Loss: 2.7428, Test Accuracy: 48.56%, Latency: 0.003504 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 427.9674, Test Loss: 2.7428, Test Accuracy: 48.56%, Test Latency: 0.003504 seconds/image\n",
      "Test Loss: 2.8650, Test Accuracy: 48.01%, Latency: 0.003402 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 391.1638, Test Loss: 2.8650, Test Accuracy: 48.01%, Test Latency: 0.003402 seconds/image\n",
      "Test Loss: 2.8650, Test Accuracy: 48.01%, Latency: 0.003460 seconds/image, Memory Usage: 111.58 MB\n",
      "Loaded weights from previous generation for architecture (12, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.1708, Test Accuracy: 73.87%, Latency: 0.004439 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 1/3, Loss: 366.5175, Test Loss: 1.1708, Test Accuracy: 73.87%, Test Latency: 0.004439 seconds/image\n",
      "Test Loss: 1.1901, Test Accuracy: 73.51%, Latency: 0.004414 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 2/3, Loss: 298.3766, Test Loss: 1.1901, Test Accuracy: 73.51%, Test Latency: 0.004414 seconds/image\n",
      "Test Loss: 1.2109, Test Accuracy: 74.12%, Latency: 0.004404 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 3/3, Loss: 268.0354, Test Loss: 1.2109, Test Accuracy: 74.12%, Test Latency: 0.004404 seconds/image\n",
      "Test Loss: 1.2109, Test Accuracy: 74.12%, Latency: 0.004419 seconds/image, Memory Usage: 327.88 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 2.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 2.7987, Test Accuracy: 37.54%, Latency: 0.003331 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 1/3, Loss: 2178.8341, Test Loss: 2.7987, Test Accuracy: 37.54%, Test Latency: 0.003331 seconds/image\n",
      "Test Loss: 3.0008, Test Accuracy: 37.20%, Latency: 0.003323 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 2/3, Loss: 1682.9435, Test Loss: 3.0008, Test Accuracy: 37.20%, Test Latency: 0.003323 seconds/image\n",
      "Test Loss: 3.2823, Test Accuracy: 36.37%, Latency: 0.003316 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 3/3, Loss: 1217.9096, Test Loss: 3.2823, Test Accuracy: 36.37%, Test Latency: 0.003316 seconds/image\n",
      "Test Loss: 3.2823, Test Accuracy: 36.37%, Latency: 0.003328 seconds/image, Memory Usage: 75.55 MB\n",
      "Saved top-ranked model: Generation 2, Rank 1 (Acc=71.61%, Lat=0.004139, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 2 (Acc=48.01%, Lat=0.003460, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 3 (Acc=36.37%, Lat=0.003328, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 4 (Acc=74.12%, Lat=0.004419, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 2, Rank 5 (Acc=63.32%, Lat=0.006016, Mem=0.00MB)\n",
      "Minimal improvement detected: -1.43%\n",
      "Crossover result: Child1=(4, 4, 4.0, 768), Child2=(10, 8, 4.0, 768)\n",
      "Crossover parents: (10, 8, 4.0, 768) & (4, 4, 4.0, 768)\n",
      "Crossover result: Child1=(4, 4, 4.0, 768), Child2=(12, 8, 2.0, 768)\n",
      "Crossover parents: (4, 4, 2.0, 768) & (12, 8, 4.0, 768)\n",
      "\n",
      "--- Generation 3/5 ---\n",
      "Loaded weights from previous generation for architecture (10, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.4015, Test Accuracy: 71.72%, Latency: 0.004106 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 243.6747, Test Loss: 1.4015, Test Accuracy: 71.72%, Test Latency: 0.004106 seconds/image\n",
      "Test Loss: 1.4222, Test Accuracy: 71.19%, Latency: 0.004070 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 205.0095, Test Loss: 1.4222, Test Accuracy: 71.19%, Test Latency: 0.004070 seconds/image\n",
      "Test Loss: 1.3763, Test Accuracy: 72.15%, Latency: 0.004119 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 198.6418, Test Loss: 1.3763, Test Accuracy: 72.15%, Test Latency: 0.004119 seconds/image\n",
      "Test Loss: 1.3763, Test Accuracy: 72.15%, Latency: 0.004195 seconds/image, Memory Usage: 273.81 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 2.8798, Test Accuracy: 48.80%, Latency: 0.003489 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 393.3169, Test Loss: 2.8798, Test Accuracy: 48.80%, Test Latency: 0.003489 seconds/image\n",
      "Test Loss: 2.9697, Test Accuracy: 48.15%, Latency: 0.003663 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 301.6979, Test Loss: 2.9697, Test Accuracy: 48.15%, Test Latency: 0.003663 seconds/image\n",
      "Test Loss: 3.0500, Test Accuracy: 48.29%, Latency: 0.003508 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 305.4492, Test Loss: 3.0500, Test Accuracy: 48.29%, Test Latency: 0.003508 seconds/image\n",
      "Test Loss: 3.0500, Test Accuracy: 48.29%, Latency: 0.003499 seconds/image, Memory Usage: 111.58 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 2.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 3.4192, Test Accuracy: 36.32%, Latency: 0.003346 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 1/3, Loss: 944.8781, Test Loss: 3.4192, Test Accuracy: 36.32%, Test Latency: 0.003346 seconds/image\n",
      "Test Loss: 3.7190, Test Accuracy: 35.39%, Latency: 0.003765 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 2/3, Loss: 614.1878, Test Loss: 3.7190, Test Accuracy: 35.39%, Test Latency: 0.003765 seconds/image\n",
      "Test Loss: 3.8993, Test Accuracy: 35.07%, Latency: 0.003858 seconds/image, Memory Usage: 75.55 MB\n",
      "Epoch 3/3, Loss: 520.6865, Test Loss: 3.8993, Test Accuracy: 35.07%, Test Latency: 0.003858 seconds/image\n",
      "Test Loss: 3.8993, Test Accuracy: 35.07%, Latency: 0.003858 seconds/image, Memory Usage: 75.55 MB\n",
      "Loaded weights from previous generation for architecture (12, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.2301, Test Accuracy: 73.53%, Latency: 0.006081 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 1/3, Loss: 261.7027, Test Loss: 1.2301, Test Accuracy: 73.53%, Test Latency: 0.006081 seconds/image\n",
      "Test Loss: 1.3227, Test Accuracy: 72.34%, Latency: 0.006013 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 2/3, Loss: 232.8844, Test Loss: 1.3227, Test Accuracy: 72.34%, Test Latency: 0.006013 seconds/image\n",
      "Test Loss: 1.3269, Test Accuracy: 73.04%, Latency: 0.006054 seconds/image, Memory Usage: 327.88 MB\n",
      "Epoch 3/3, Loss: 212.4765, Test Loss: 1.3269, Test Accuracy: 73.04%, Test Latency: 0.006054 seconds/image\n",
      "Test Loss: 1.3269, Test Accuracy: 73.04%, Latency: 0.006167 seconds/image, Memory Usage: 327.88 MB\n",
      "Loaded weights from previous generation for architecture (6, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.5427, Test Accuracy: 62.81%, Latency: 0.004632 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 1/3, Loss: 752.7455, Test Loss: 1.5427, Test Accuracy: 62.81%, Test Latency: 0.004632 seconds/image\n",
      "Test Loss: 1.7280, Test Accuracy: 61.44%, Latency: 0.004727 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 2/3, Loss: 444.5236, Test Loss: 1.7280, Test Accuracy: 61.44%, Test Latency: 0.004727 seconds/image\n",
      "Test Loss: 1.8335, Test Accuracy: 61.16%, Latency: 0.004824 seconds/image, Memory Usage: 165.65 MB\n",
      "Epoch 3/3, Loss: 358.8346, Test Loss: 1.8335, Test Accuracy: 61.16%, Test Latency: 0.004824 seconds/image\n",
      "Test Loss: 1.8335, Test Accuracy: 61.16%, Latency: 0.004687 seconds/image, Memory Usage: 165.65 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 3.0405, Test Accuracy: 48.73%, Latency: 0.004132 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 314.5253, Test Loss: 3.0405, Test Accuracy: 48.73%, Test Latency: 0.004132 seconds/image\n",
      "Test Loss: 3.1114, Test Accuracy: 48.34%, Latency: 0.004117 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 250.6307, Test Loss: 3.1114, Test Accuracy: 48.34%, Test Latency: 0.004117 seconds/image\n",
      "Test Loss: 3.1370, Test Accuracy: 48.93%, Latency: 0.003449 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 244.3369, Test Loss: 3.1370, Test Accuracy: 48.93%, Test Latency: 0.003449 seconds/image\n",
      "Test Loss: 3.1370, Test Accuracy: 48.93%, Latency: 0.003447 seconds/image, Memory Usage: 111.58 MB\n",
      "Loaded weights from previous generation for architecture (10, 8, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 1.4212, Test Accuracy: 72.06%, Latency: 0.004040 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 1/3, Loss: 200.0362, Test Loss: 1.4212, Test Accuracy: 72.06%, Test Latency: 0.004040 seconds/image\n",
      "Test Loss: 1.4997, Test Accuracy: 70.88%, Latency: 0.004125 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 2/3, Loss: 178.5446, Test Loss: 1.4997, Test Accuracy: 70.88%, Test Latency: 0.004125 seconds/image\n",
      "Test Loss: 1.5278, Test Accuracy: 71.42%, Latency: 0.004125 seconds/image, Memory Usage: 273.81 MB\n",
      "Epoch 3/3, Loss: 168.0255, Test Loss: 1.5278, Test Accuracy: 71.42%, Test Latency: 0.004125 seconds/image\n",
      "Test Loss: 1.5278, Test Accuracy: 71.42%, Latency: 0.004115 seconds/image, Memory Usage: 273.81 MB\n",
      "Loaded weights from previous generation for architecture (4, 4, 4.0, 768)\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 3.0610, Test Accuracy: 48.83%, Latency: 0.003412 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 1/3, Loss: 259.0381, Test Loss: 3.0610, Test Accuracy: 48.83%, Test Latency: 0.003412 seconds/image\n",
      "Test Loss: 3.1684, Test Accuracy: 48.59%, Latency: 0.003467 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 2/3, Loss: 218.2362, Test Loss: 3.1684, Test Accuracy: 48.59%, Test Latency: 0.003467 seconds/image\n",
      "Test Loss: 3.2177, Test Accuracy: 48.33%, Latency: 0.003475 seconds/image, Memory Usage: 111.58 MB\n",
      "Epoch 3/3, Loss: 210.6931, Test Loss: 3.2177, Test Accuracy: 48.33%, Test Latency: 0.003475 seconds/image\n",
      "Test Loss: 3.2177, Test Accuracy: 48.33%, Latency: 0.003452 seconds/image, Memory Usage: 111.58 MB\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 3.2813, Test Accuracy: 23.50%, Latency: 0.004169 seconds/image, Memory Usage: 219.81 MB\n",
      "Epoch 1/3, Loss: 6315.3725, Test Loss: 3.2813, Test Accuracy: 23.50%, Test Latency: 0.004169 seconds/image\n",
      "Test Loss: 2.7060, Test Accuracy: 33.85%, Latency: 0.003928 seconds/image, Memory Usage: 219.81 MB\n",
      "Epoch 2/3, Loss: 4438.3785, Test Loss: 2.7060, Test Accuracy: 33.85%, Test Latency: 0.003928 seconds/image\n",
      "Test Loss: 2.4194, Test Accuracy: 40.49%, Latency: 0.004056 seconds/image, Memory Usage: 219.81 MB\n",
      "Epoch 3/3, Loss: 3585.9797, Test Loss: 2.4194, Test Accuracy: 40.49%, Test Latency: 0.004056 seconds/image\n",
      "Test Loss: 2.4194, Test Accuracy: 40.49%, Latency: 0.004001 seconds/image, Memory Usage: 219.81 MB\n",
      "Saved top-ranked model: Generation 3, Rank 1 (Acc=71.42%, Lat=0.004115, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 3, Rank 2 (Acc=48.33%, Lat=0.003452, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 3, Rank 3 (Acc=35.07%, Lat=0.003858, Mem=0.00MB)\n",
      "Saved top-ranked model: Generation 3, Rank 4 (Acc=73.04%, Lat=0.006167, Mem=0.00MB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 50239104 vs 50238992",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/30: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 570\u001b[39m\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m population\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# Run the evolutionary algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[43mevolutionary_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# Call the algorithm\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Run the evolutionary algorithm\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 525\u001b[39m, in \u001b[36mevolutionary_algorithm\u001b[39m\u001b[34m(population_size, generations, mutation_rate, crossover_rate, train_loader, test_loader)\u001b[39m\n\u001b[32m    522\u001b[39m population = pareto_selection(arch_performance)\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# Saving top-ranked models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[43msave_top_ranked_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43march_performance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# Check for Pareto front convergence (early stopping criteria)\u001b[39;00m\n\u001b[32m    528\u001b[39m current_best_accuracy = arch_performance[population[\u001b[32m0\u001b[39m]][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 280\u001b[39m, in \u001b[36msave_top_ranked_models\u001b[39m\u001b[34m(population, arch_performance, generation)\u001b[39m\n\u001b[32m    277\u001b[39m model.load_state_dict(torch.load(checkpoint_path))\n\u001b[32m    279\u001b[39m top_model_path = os.path.join(SAVE_PATH, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtop_ranked_model_gen\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rank_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m acc, lat, mem = arch_performance[arch]\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:784\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:626] . unexpected pos 50239104 vs 50238992"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol2'\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([4, 6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200  \n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Evaluate architecture: accuracy, latency, and memory usage\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start measuring inference latency\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Measure total time for inference (latency)\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute memory usage (rough estimation)\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "    # Compute average loss\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "# Fine-tune model on dataset (train for a few epochs)\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "    # Save the model after fine-tuning\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=10, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=200).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder)\n",
    "\n",
    "            accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        # Saving top-ranked models\n",
    "        save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# Call the algorithm\n",
    "# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just defining model again here for easily avaliability\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class DynamicPatchEmbed(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "#         self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.proj(x)\n",
    "#         return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "# class DynamicMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "#         self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.scale = (embed_dim // num_heads) ** -0.5\n",
    "#         self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "#         # Ensure that the number of heads divides the embedding dimension\n",
    "#         assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "#         return self.proj(x)\n",
    "\n",
    "# class MLPBlock(nn.Module):  \n",
    "#     def __init__(self, embed_dim, mlp_ratio):\n",
    "#         super().__init__()\n",
    "#         hidden_dim = int(embed_dim * mlp_ratio)\n",
    "#         self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "#         self.act = nn.GELU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "# class DynamicTransformerBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "#         #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "#         self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.depth = depth  # Store depth as an instance variable\n",
    "#         self.num_heads = num_heads  # Store num_heads as an instance variable\n",
    "#         self.mlp_ratio = mlp_ratio  # Store mlp_ratio as an instance variable\n",
    "        \n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "#         # Fix: Correct positional embedding key\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "\n",
    "#         # Add class token\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)\n",
    "\n",
    "\n",
    "# # Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol1'\n",
    "# # SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# # Set the device (GPU if available, else CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # First-time loading pretrained weights for initialization\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     # Match keys between pretrained and current model\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "#     # Load pretrained weights\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# # Check if pretrained weights are loaded correctly\n",
    "# def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "#     pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     model_state_dict = model.state_dict()\n",
    "#     matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "#     if len(matching_keys) > 0:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "#     else:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# # Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "# def sample_subnetwork(seen_architectures):\n",
    "#     while True:\n",
    "#         depth = random.choice([4, 6, 8, 10, 12])\n",
    "#         num_heads = random.choice([4, 8, 12, 16])\n",
    "#         mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#         embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "#         architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "#         # Skip if architecture has already been sampled\n",
    "#         if architecture not in seen_architectures:\n",
    "#             seen_architectures.add(architecture)\n",
    "#             print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "#             # Create the model to calculate its number of parameters\n",
    "#             # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "#             sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                         depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "#                                         num_classes=200  \n",
    "#                                     )\n",
    "#             num_params = count_parameters(sampled_model)\n",
    "#             print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "#             return architecture\n",
    "#         else:\n",
    "#             print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# # Count number of trainable parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # Evaluate architecture: accuracy, latency, and memory usage\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Start measuring inference latency\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#             y_true.extend(labels.cpu().numpy())\n",
    "#             y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "#     # Measure total time for inference (latency)\n",
    "#     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "#     # Compute accuracy\n",
    "#     accuracy = 100 * correct / total\n",
    "\n",
    "#     # Compute memory usage (rough estimation)\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "#     # Compute average loss\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "#     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# def calculate_crowding_distance(population, test_loader):\n",
    "#     crowding_distances = [0] * len(population)\n",
    "#     num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "#     # Evaluate each architecture once, then reuse the results\n",
    "#     evaluated_results = []\n",
    "#     for arch in population:\n",
    "#         # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "#         #                    depth=arch[0], num_heads=arch[1],\n",
    "#         #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "#                             depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "#                             num_classes=200).to(device)\n",
    "\n",
    "#         accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#         memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "#         evaluated_results.append((accuracy, latency, memory))\n",
    "#         del model\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     for objective_index in range(num_objectives):\n",
    "#         sorted_indices = sorted(range(len(population)),\n",
    "#                                 key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "#         crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "#         for i in range(1, len(sorted_indices) - 1):\n",
    "#             prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "#             next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "#             distance = next_value - prev_value\n",
    "#             crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "#     return crowding_distances\n",
    "\n",
    "\n",
    "# def dominates(model1, model2, test_loader):\n",
    "#     # Evaluate both models on the test set\n",
    "#     accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "#     accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "#     # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "#     memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "#     memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "#     # Compare performance metrics\n",
    "#     dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "#     dominates_in_latency = latency1 <= latency2\n",
    "#     dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "#     # Return True if model1 dominates model2 in all aspects\n",
    "#     return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# # Mutation: Randomly mutate architecture's hyperparameters\n",
    "# def mutate(architecture):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "#     if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# # One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "\n",
    "# # Optimized Pareto selection based on stored performance metrics\n",
    "# def pareto_selection(arch_performance):\n",
    "#     def dominates(perf1, perf2):\n",
    "#         acc1, lat1, mem1 = perf1\n",
    "#         acc2, lat2, mem2 = perf2\n",
    "#         return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, perf1 in arch_performance.items():\n",
    "#         dominated_count = 0\n",
    "#         for arch2, perf2 in arch_performance.items():\n",
    "#             if arch1 != arch2 and dominates(perf2, perf1):\n",
    "#                 dominated_count += 1\n",
    "#         ranks[arch1] = dominated_count\n",
    "\n",
    "#     # Sort architectures by rank (lower dominated_count = better)\n",
    "#     sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "#     return sorted_population\n",
    "\n",
    "# # Fine-tune model on dataset (train for a few epochs)\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "#     # Save the model after fine-tuning\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "# def save_top_ranked_models(population, arch_performance, generation):\n",
    "#     top_n = min(5, len(population))\n",
    "#     for idx, arch in enumerate(population[:top_n]):\n",
    "#         depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "       \n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "#                             num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "#                             num_classes=200).to(device)\n",
    "\n",
    "\n",
    "#         architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "#         checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#         model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "#         top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "#         torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "#         acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "#         # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "#         #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "#         #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "#         with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "#             f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "#             f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "#         print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "\n",
    "# def evolutionary_algorithm(population_size=5, generations=2, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "\n",
    "#     # Evaluate and store metrics only once per architecture per generation\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "        \n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "\n",
    "           \n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                 depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                 num_classes=200).to(device)\n",
    "            \n",
    "#             if generation == 0:\n",
    "#                 load_pretrained_weights(model)\n",
    "#             else:\n",
    "#                 checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#                 if os.path.exists(checkpoint_path):\n",
    "#                     model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                     print(f\"Generation {generation + 1}: Loaded fine-tuned weights from previous generation for {arch}.\")\n",
    "#                 else:\n",
    "#                     print(f\"Generation {generation + 1}: Fine-tuned weights not found for {arch}. Loading pretrained ViT weights.\")\n",
    "#                     load_pretrained_weights(model)\n",
    "\n",
    "#             check_pretrained_weights(model, generation=generation, model_type=\"subnetwork\")\n",
    "\n",
    "#             fine_tune_model(\n",
    "#                 model, train_loader, test_loader, epochs=3, architecture_folder=architecture_folder\n",
    "#             )\n",
    "\n",
    "#             # Evaluate once per architecture\n",
    "#             accuracy, _, latency, memory = evaluate_architecture(model, test_loader)\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         # Save top models clearly ranked (1 = best)\n",
    "#         save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Generate offspring using crossover and mutation\n",
    "#         offspring = []\n",
    "#         for i in range(0, len(population)-1, 2):\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(population[i], population[i + 1])\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([population[i], population[i + 1]])\n",
    "\n",
    "#         offspring = [mutate(child) if random.random() < mutation_rate else child for child in offspring]\n",
    "\n",
    "#         # Next-generation combines top parents and offspring\n",
    "#         population = population[:len(population)//2] + offspring\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# Saved top-ranked model: Generation 1, Rank 1 (Acc=37.52%, Lat=0.003791, Mem=0.00MB)\n",
    "# Saved top-ranked model: Generation 1, Rank 2 (Acc=36.44%, Lat=0.003658, Mem=0.00MB)\n",
    "# Saved top-ranked model: Generation 1, Rank 3 (Acc=68.70%, Lat=0.003857, Mem=0.00MB)\n",
    "# Saved top-ranked model: Generation 1, Rank 4 (Acc=38.75%, Lat=0.004327, Mem=0.00MB)\n",
    "# Saved top-ranked model: Generation 1, Rank 5 (Acc=37.01%, Lat=0.004950, Mem=0.00MB)\n",
    "# this is first generation but you can see here in output ranking is not good i think this is because the dominates funtion in pareto selection also here in hthis code you are not considering crowding distance in pareto selection correct it and also whenever each model is finetuned for 5 epochs draw its graph of train loss test loss ans test accuracy vs epochs immediately. also print execution time of each architecuture after each epoch analyze all the code and do all these modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what to include \n",
    "crowding distance in pareto selection funtion also check ranking funtion how models are ranked after finetuned because ranking has some error\n",
    "draw plot after every subnetwork is finetuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## may 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol28-may'\n",
    "\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200\n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(1,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top5]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(1,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top5_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-5 Acc: {top5_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top5_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([ 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-5 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=200).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-5 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### may 28 now we need to do pca on super net and then use evol algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bring everything here for easy avaliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "# Path to your ImageNet data\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "# Load ImageNet dataset and filter only the first 200 classes\n",
    "filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# Use only the first 200 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(filtered_dataset))\n",
    "test_size = len(filtered_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the number of samples in each set\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "#         #  Fix: Correct positional embedding key\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "\n",
    "#         # Add class token\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformer super net\n",
    "  ## implementing pca on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just defining model again here for easily avaliability\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "        self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "        # Ensure that the number of heads divides the embedding dimension\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPBlock(nn.Module):  \n",
    "    def __init__(self, embed_dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.depth = depth  # Store depth as an instance variable\n",
    "        self.num_heads = num_heads  # Store num_heads as an instance variable\n",
    "        self.mlp_ratio = mlp_ratio  # Store mlp_ratio as an instance variable\n",
    "        \n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "        # Fix: Correct positional embedding key\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear projecting layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "# Path to your ImageNet data\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "# Load ImageNet dataset and filter only the first 200 classes\n",
    "filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# Use only the first 200 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(filtered_dataset))\n",
    "test_size = len(filtered_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the number of samples in each set\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just defining model again here for easily avaliability\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim  # Track embedding dimension\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "        self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "        # Ensure that the number of heads divides the embedding dimension\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPBlock(nn.Module):  \n",
    "    def __init__(self, embed_dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim  # Store embed_dim as instance variable\n",
    "        self.depth = depth  # Store depth as an instance variable\n",
    "        self.num_heads = num_heads  # Store num_heads as an instance variable\n",
    "        self.mlp_ratio = mlp_ratio  # Store mlp_ratio as an instance variable\n",
    "        \n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "        # Fix: Correct positional embedding key\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evolutionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-time loading pretrained weights for initialization\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     # Match keys between pretrained and current model\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "#     # Load pretrained weights\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     # Get embedding dimensions\n",
    "#     pretrained_embed_dim = pretrained_state_dict['pos_embed'].shape[-1]\n",
    "#     current_embed_dim = model.embed_dim\n",
    "    \n",
    "#     # Create adaptation modules\n",
    "#     adaptation_modules = nn.ModuleDict()\n",
    "#     if pretrained_embed_dim != current_embed_dim:\n",
    "#         adaptation_modules['pos_embed_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "#         adaptation_modules['cls_token_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "    \n",
    "#     # Project pretrained weights\n",
    "#     filtered_dict = {}\n",
    "#     for k, v in pretrained_state_dict.items():\n",
    "#         if k == 'pos_embed' and pretrained_embed_dim != current_embed_dim:\n",
    "#             filtered_dict[k] = adaptation_modules['pos_embed_proj'](v)\n",
    "#         elif k == 'cls_token' and pretrained_embed_dim != current_embed_dim:\n",
    "#             filtered_dict[k] = adaptation_modules['cls_token_proj'](v)\n",
    "#         elif k in model.state_dict() and v.shape == model.state_dict[k].shape:\n",
    "#             filtered_dict[k] = v\n",
    "    \n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Loaded pretrained weights with {'adaptation' if pretrained_embed_dim != current_embed_dim else 'no'} projection\")\n",
    "\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "# def mutate(architecture):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "#     if random.random() < 0.5: depth = random.choice([ 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 43,425,224\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 37,497,320\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=12, Num Heads=4, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 21,742,664\n",
      "Sampled architecture: Depth=6, Num Heads=4, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 11,095,880\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 44,884,520\n",
      "Sampled architecture: Depth=10, Num Heads=8, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 19,046,120\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,639,432\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,655,560\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 18,193,736\n",
      "Sampled architecture: Depth=8, Num Heads=4, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 15,349,160\n",
      "Sampled architecture: Depth=8, Num Heads=4, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,644,808\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 19,046,120\n",
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,590,216\n",
      "Sampled architecture: Depth=10, Num Heads=4, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 24,099,656\n",
      "Sampled architecture: Depth=10, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 71,776,712\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 11,095,880\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 1.4989, Top-1 Acc: 78.57%, Top-3 Acc: 85.11%, Latency: 0.004063s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 2.3353\n",
      "| Test Loss: 1.4989\n",
      "| Top-1 Accuracy: 78.57%\n",
      "| Top-3 Accuracy: 85.11%\n",
      "| Latency: 0.004063s/img\n",
      "| Memory Usage: 165.65MB\n",
      "| MACs: 8499.22M\n",
      "| Epoch Time: 548.93s\n",
      "\n",
      "Test Loss: 1.3250, Top-1 Acc: 82.39%, Top-3 Acc: 87.96%, Latency: 0.003745s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 1.1001\n",
      "| Test Loss: 1.3250\n",
      "| Top-1 Accuracy: 82.39%\n",
      "| Top-3 Accuracy: 87.96%\n",
      "| Latency: 0.003745s/img\n",
      "| Memory Usage: 165.65MB\n",
      "| MACs: 8499.22M\n",
      "| Epoch Time: 531.17s\n",
      "\n",
      "Test Loss: 1.4143, Top-1 Acc: 81.73%, Top-3 Acc: 87.44%, Latency: 0.003837s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 0.6412\n",
      "| Test Loss: 1.4143\n",
      "| Top-1 Accuracy: 81.73%\n",
      "| Top-3 Accuracy: 87.44%\n",
      "| Latency: 0.003837s/img\n",
      "| Memory Usage: 165.65MB\n",
      "| MACs: 8499.22M\n",
      "| Epoch Time: 540.97s\n",
      "\n",
      "Test Loss: 1.5970, Top-1 Acc: 80.64%, Top-3 Acc: 86.52%, Latency: 0.003852s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 0.3629\n",
      "| Test Loss: 1.5970\n",
      "| Top-1 Accuracy: 80.64%\n",
      "| Top-3 Accuracy: 86.52%\n",
      "| Latency: 0.003852s/img\n",
      "| Memory Usage: 165.65MB\n",
      "| MACs: 8499.22M\n",
      "| Epoch Time: 533.56s\n",
      "\n",
      "Test Loss: 1.6747, Top-1 Acc: 80.56%, Top-3 Acc: 86.21%, Latency: 0.003817s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 0.2342\n",
      "| Test Loss: 1.6747\n",
      "| Top-1 Accuracy: 80.56%\n",
      "| Top-3 Accuracy: 86.21%\n",
      "| Latency: 0.003817s/img\n",
      "| Memory Usage: 165.65MB\n",
      "| MACs: 8499.22M\n",
      "| Epoch Time: 533.78s\n",
      "\n",
      "Test Loss: 1.6747, Top-1 Acc: 80.56%, Top-3 Acc: 86.21%, Latency: 0.003795s/img, Mem: 165.65MB\n",
      "MACs: 8499.22 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=12, MLP Ratio=6.0\n",
      "Test Loss: 4.0314, Top-1 Acc: 24.89%, Top-3 Acc: 32.89%, Latency: 0.003752s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5270\n",
      "| Test Loss: 4.0314\n",
      "| Top-1 Accuracy: 24.89%\n",
      "| Top-3 Accuracy: 32.89%\n",
      "| Latency: 0.003752s/img\n",
      "| Memory Usage: 143.04MB\n",
      "| MACs: 7358.29M\n",
      "| Epoch Time: 543.50s\n",
      "\n",
      "Test Loss: 3.5840, Top-1 Acc: 34.64%, Top-3 Acc: 43.76%, Latency: 0.003894s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7524\n",
      "| Test Loss: 3.5840\n",
      "| Top-1 Accuracy: 34.64%\n",
      "| Top-3 Accuracy: 43.76%\n",
      "| Latency: 0.003894s/img\n",
      "| Memory Usage: 143.04MB\n",
      "| MACs: 7358.29M\n",
      "| Epoch Time: 548.13s\n",
      "\n",
      "Test Loss: 3.3494, Top-1 Acc: 40.06%, Top-3 Acc: 48.80%, Latency: 0.003839s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.3233\n",
      "| Test Loss: 3.3494\n",
      "| Top-1 Accuracy: 40.06%\n",
      "| Top-3 Accuracy: 48.80%\n",
      "| Latency: 0.003839s/img\n",
      "| Memory Usage: 143.04MB\n",
      "| MACs: 7358.29M\n",
      "| Epoch Time: 547.62s\n",
      "\n",
      "Test Loss: 3.0623, Top-1 Acc: 46.07%, Top-3 Acc: 55.48%, Latency: 0.003809s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9561\n",
      "| Test Loss: 3.0623\n",
      "| Top-1 Accuracy: 46.07%\n",
      "| Top-3 Accuracy: 55.48%\n",
      "| Latency: 0.003809s/img\n",
      "| Memory Usage: 143.04MB\n",
      "| MACs: 7358.29M\n",
      "| Epoch Time: 549.39s\n",
      "\n",
      "Test Loss: 2.9409, Top-1 Acc: 49.13%, Top-3 Acc: 58.08%, Latency: 0.003795s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.6114\n",
      "| Test Loss: 2.9409\n",
      "| Top-1 Accuracy: 49.13%\n",
      "| Top-3 Accuracy: 58.08%\n",
      "| Latency: 0.003795s/img\n",
      "| Memory Usage: 143.04MB\n",
      "| MACs: 7358.29M\n",
      "| Epoch Time: 547.88s\n",
      "\n",
      "Test Loss: 2.9409, Top-1 Acc: 49.13%, Top-3 Acc: 58.08%, Latency: 0.003869s/img, Mem: 143.04MB\n",
      "MACs: 7358.29 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 4.0697, Top-1 Acc: 24.80%, Top-3 Acc: 32.84%, Latency: 0.003591s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5717\n",
      "| Test Loss: 4.0697\n",
      "| Top-1 Accuracy: 24.80%\n",
      "| Top-3 Accuracy: 32.84%\n",
      "| Latency: 0.003591s/img\n",
      "| Memory Usage: 82.94MB\n",
      "| MACs: 4258.23M\n",
      "| Epoch Time: 457.13s\n",
      "\n",
      "Test Loss: 3.6386, Top-1 Acc: 33.70%, Top-3 Acc: 42.66%, Latency: 0.003490s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7975\n",
      "| Test Loss: 3.6386\n",
      "| Top-1 Accuracy: 33.70%\n",
      "| Top-3 Accuracy: 42.66%\n",
      "| Latency: 0.003490s/img\n",
      "| Memory Usage: 82.94MB\n",
      "| MACs: 4258.23M\n",
      "| Epoch Time: 458.48s\n",
      "\n",
      "Test Loss: 3.2655, Top-1 Acc: 41.79%, Top-3 Acc: 51.11%, Latency: 0.003485s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.3624\n",
      "| Test Loss: 3.2655\n",
      "| Top-1 Accuracy: 41.79%\n",
      "| Top-3 Accuracy: 51.11%\n",
      "| Latency: 0.003485s/img\n",
      "| Memory Usage: 82.94MB\n",
      "| MACs: 4258.23M\n",
      "| Epoch Time: 456.74s\n",
      "\n",
      "Test Loss: 3.0922, Top-1 Acc: 45.56%, Top-3 Acc: 54.93%, Latency: 0.003459s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9997\n",
      "| Test Loss: 3.0922\n",
      "| Top-1 Accuracy: 45.56%\n",
      "| Top-3 Accuracy: 54.93%\n",
      "| Latency: 0.003459s/img\n",
      "| Memory Usage: 82.94MB\n",
      "| MACs: 4258.23M\n",
      "| Epoch Time: 450.66s\n",
      "\n",
      "Test Loss: 2.9481, Top-1 Acc: 48.95%, Top-3 Acc: 58.27%, Latency: 0.003439s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.6768\n",
      "| Test Loss: 2.9481\n",
      "| Top-1 Accuracy: 48.95%\n",
      "| Top-3 Accuracy: 58.27%\n",
      "| Latency: 0.003439s/img\n",
      "| Memory Usage: 82.94MB\n",
      "| MACs: 4258.23M\n",
      "| Epoch Time: 450.94s\n",
      "\n",
      "Test Loss: 2.9481, Top-1 Acc: 48.95%, Top-3 Acc: 58.27%, Latency: 0.003574s/img, Mem: 82.94MB\n",
      "MACs: 4258.23 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 3.9468, Top-1 Acc: 27.78%, Top-3 Acc: 35.75%, Latency: 0.003226s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.4721\n",
      "| Test Loss: 3.9468\n",
      "| Top-1 Accuracy: 27.78%\n",
      "| Top-3 Accuracy: 35.75%\n",
      "| Latency: 0.003226s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 371.03s\n",
      "\n",
      "Test Loss: 3.5257, Top-1 Acc: 36.73%, Top-3 Acc: 45.73%, Latency: 0.003226s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.6764\n",
      "| Test Loss: 3.5257\n",
      "| Top-1 Accuracy: 36.73%\n",
      "| Top-3 Accuracy: 45.73%\n",
      "| Latency: 0.003226s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 376.37s\n",
      "\n",
      "Test Loss: 3.2240, Top-1 Acc: 42.95%, Top-3 Acc: 52.16%, Latency: 0.003306s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2421\n",
      "| Test Loss: 3.2240\n",
      "| Top-1 Accuracy: 42.95%\n",
      "| Top-3 Accuracy: 52.16%\n",
      "| Latency: 0.003306s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 374.32s\n",
      "\n",
      "Test Loss: 3.0738, Top-1 Acc: 45.97%, Top-3 Acc: 55.22%, Latency: 0.003222s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9026\n",
      "| Test Loss: 3.0738\n",
      "| Top-1 Accuracy: 45.97%\n",
      "| Top-3 Accuracy: 55.22%\n",
      "| Latency: 0.003222s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 377.99s\n",
      "\n",
      "Test Loss: 2.9670, Top-1 Acc: 48.36%, Top-3 Acc: 57.57%, Latency: 0.003319s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5890\n",
      "| Test Loss: 2.9670\n",
      "| Top-1 Accuracy: 48.36%\n",
      "| Top-3 Accuracy: 57.57%\n",
      "| Latency: 0.003319s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 376.75s\n",
      "\n",
      "Test Loss: 2.9670, Top-1 Acc: 48.36%, Top-3 Acc: 57.57%, Latency: 0.003317s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=6.0\n",
      "Test Loss: 4.0861, Top-1 Acc: 23.77%, Top-3 Acc: 31.70%, Latency: 0.004015s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5250\n",
      "| Test Loss: 4.0861\n",
      "| Top-1 Accuracy: 23.77%\n",
      "| Top-3 Accuracy: 31.70%\n",
      "| Latency: 0.004015s/img\n",
      "| Memory Usage: 171.22MB\n",
      "| MACs: 8815.46M\n",
      "| Epoch Time: 576.84s\n",
      "\n",
      "Test Loss: 3.6625, Top-1 Acc: 32.97%, Top-3 Acc: 41.82%, Latency: 0.003893s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7779\n",
      "| Test Loss: 3.6625\n",
      "| Top-1 Accuracy: 32.97%\n",
      "| Top-3 Accuracy: 41.82%\n",
      "| Latency: 0.003893s/img\n",
      "| Memory Usage: 171.22MB\n",
      "| MACs: 8815.46M\n",
      "| Epoch Time: 578.43s\n",
      "\n",
      "Test Loss: 3.2866, Top-1 Acc: 41.33%, Top-3 Acc: 50.79%, Latency: 0.003937s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.3606\n",
      "| Test Loss: 3.2866\n",
      "| Top-1 Accuracy: 41.33%\n",
      "| Top-3 Accuracy: 50.79%\n",
      "| Latency: 0.003937s/img\n",
      "| Memory Usage: 171.22MB\n",
      "| MACs: 8815.46M\n",
      "| Epoch Time: 578.63s\n",
      "\n",
      "Test Loss: 3.0737, Top-1 Acc: 46.03%, Top-3 Acc: 55.30%, Latency: 0.003934s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 3.0029\n",
      "| Test Loss: 3.0737\n",
      "| Top-1 Accuracy: 46.03%\n",
      "| Top-3 Accuracy: 55.30%\n",
      "| Latency: 0.003934s/img\n",
      "| Memory Usage: 171.22MB\n",
      "| MACs: 8815.46M\n",
      "| Epoch Time: 584.47s\n",
      "\n",
      "Test Loss: 2.9953, Top-1 Acc: 47.78%, Top-3 Acc: 56.85%, Latency: 0.004151s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.6555\n",
      "| Test Loss: 2.9953\n",
      "| Top-1 Accuracy: 47.78%\n",
      "| Top-3 Accuracy: 56.85%\n",
      "| Latency: 0.004151s/img\n",
      "| Memory Usage: 171.22MB\n",
      "| MACs: 8815.46M\n",
      "| Epoch Time: 592.28s\n",
      "\n",
      "Test Loss: 2.9953, Top-1 Acc: 47.78%, Top-3 Acc: 56.85%, Latency: 0.003967s/img, Mem: 171.22MB\n",
      "MACs: 8815.46 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 3.9744, Top-1 Acc: 27.05%, Top-3 Acc: 35.09%, Latency: 0.003537s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.4956\n",
      "| Test Loss: 3.9744\n",
      "| Top-1 Accuracy: 27.05%\n",
      "| Top-3 Accuracy: 35.09%\n",
      "| Latency: 0.003537s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 466.46s\n",
      "\n",
      "Test Loss: 3.5374, Top-1 Acc: 35.76%, Top-3 Acc: 44.95%, Latency: 0.003767s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.6979\n",
      "| Test Loss: 3.5374\n",
      "| Top-1 Accuracy: 35.76%\n",
      "| Top-3 Accuracy: 44.95%\n",
      "| Latency: 0.003767s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 466.19s\n",
      "\n",
      "Test Loss: 3.2563, Top-1 Acc: 42.43%, Top-3 Acc: 51.36%, Latency: 0.003535s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2647\n",
      "| Test Loss: 3.2563\n",
      "| Top-1 Accuracy: 42.43%\n",
      "| Top-3 Accuracy: 51.36%\n",
      "| Latency: 0.003535s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 465.81s\n",
      "\n",
      "Test Loss: 3.1286, Top-1 Acc: 45.07%, Top-3 Acc: 54.38%, Latency: 0.003707s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9047\n",
      "| Test Loss: 3.1286\n",
      "| Top-1 Accuracy: 45.07%\n",
      "| Top-3 Accuracy: 54.38%\n",
      "| Latency: 0.003707s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 469.82s\n",
      "\n",
      "Test Loss: 2.9483, Top-1 Acc: 48.98%, Top-3 Acc: 58.01%, Latency: 0.003761s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5752\n",
      "| Test Loss: 2.9483\n",
      "| Top-1 Accuracy: 48.98%\n",
      "| Top-3 Accuracy: 58.01%\n",
      "| Latency: 0.003761s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 474.44s\n",
      "\n",
      "Test Loss: 2.9483, Top-1 Acc: 48.98%, Top-3 Acc: 58.01%, Latency: 0.003641s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=16, MLP Ratio=6.0\n",
      "Test Loss: 3.9975, Top-1 Acc: 26.43%, Top-3 Acc: 34.80%, Latency: 0.003631s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5120\n",
      "| Test Loss: 3.9975\n",
      "| Top-1 Accuracy: 26.43%\n",
      "| Top-3 Accuracy: 34.80%\n",
      "| Latency: 0.003631s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 449.70s\n",
      "\n",
      "Test Loss: 3.5521, Top-1 Acc: 35.99%, Top-3 Acc: 44.72%, Latency: 0.003617s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7275\n",
      "| Test Loss: 3.5521\n",
      "| Top-1 Accuracy: 35.99%\n",
      "| Top-3 Accuracy: 44.72%\n",
      "| Latency: 0.003617s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 435.30s\n",
      "\n",
      "Test Loss: 3.2863, Top-1 Acc: 41.37%, Top-3 Acc: 50.62%, Latency: 0.003675s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.3051\n",
      "| Test Loss: 3.2863\n",
      "| Top-1 Accuracy: 41.37%\n",
      "| Top-3 Accuracy: 50.62%\n",
      "| Latency: 0.003675s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 435.12s\n",
      "\n",
      "Test Loss: 3.1461, Top-1 Acc: 44.53%, Top-3 Acc: 53.85%, Latency: 0.003595s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9537\n",
      "| Test Loss: 3.1461\n",
      "| Top-1 Accuracy: 44.53%\n",
      "| Top-3 Accuracy: 53.85%\n",
      "| Latency: 0.003595s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 437.48s\n",
      "\n",
      "Test Loss: 3.0796, Top-1 Acc: 46.29%, Top-3 Acc: 55.40%, Latency: 0.003416s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.6233\n",
      "| Test Loss: 3.0796\n",
      "| Top-1 Accuracy: 46.29%\n",
      "| Top-3 Accuracy: 55.40%\n",
      "| Latency: 0.003416s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 430.37s\n",
      "\n",
      "Test Loss: 3.0796, Top-1 Acc: 46.29%, Top-3 Acc: 55.40%, Latency: 0.003433s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 4.0442, Top-1 Acc: 25.55%, Top-3 Acc: 33.51%, Latency: 0.003564s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5453\n",
      "| Test Loss: 4.0442\n",
      "| Top-1 Accuracy: 25.55%\n",
      "| Top-3 Accuracy: 33.51%\n",
      "| Latency: 0.003564s/img\n",
      "| Memory Usage: 55.91MB\n",
      "| MACs: 2858.44M\n",
      "| Epoch Time: 445.36s\n",
      "\n",
      "Test Loss: 3.6096, Top-1 Acc: 34.63%, Top-3 Acc: 43.29%, Latency: 0.003482s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7627\n",
      "| Test Loss: 3.6096\n",
      "| Top-1 Accuracy: 34.63%\n",
      "| Top-3 Accuracy: 43.29%\n",
      "| Latency: 0.003482s/img\n",
      "| Memory Usage: 55.91MB\n",
      "| MACs: 2858.44M\n",
      "| Epoch Time: 447.41s\n",
      "\n",
      "Test Loss: 3.3033, Top-1 Acc: 41.10%, Top-3 Acc: 50.26%, Latency: 0.003472s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.3224\n",
      "| Test Loss: 3.3033\n",
      "| Top-1 Accuracy: 41.10%\n",
      "| Top-3 Accuracy: 50.26%\n",
      "| Latency: 0.003472s/img\n",
      "| Memory Usage: 55.91MB\n",
      "| MACs: 2858.44M\n",
      "| Epoch Time: 461.86s\n",
      "\n",
      "Test Loss: 3.0943, Top-1 Acc: 45.75%, Top-3 Acc: 55.13%, Latency: 0.003689s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9652\n",
      "| Test Loss: 3.0943\n",
      "| Top-1 Accuracy: 45.75%\n",
      "| Top-3 Accuracy: 55.13%\n",
      "| Latency: 0.003689s/img\n",
      "| Memory Usage: 55.91MB\n",
      "| MACs: 2858.44M\n",
      "| Epoch Time: 453.14s\n",
      "\n",
      "Test Loss: 2.9767, Top-1 Acc: 48.46%, Top-3 Acc: 57.59%, Latency: 0.003543s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.6321\n",
      "| Test Loss: 2.9767\n",
      "| Top-1 Accuracy: 48.46%\n",
      "| Top-3 Accuracy: 57.59%\n",
      "| Latency: 0.003543s/img\n",
      "| Memory Usage: 55.91MB\n",
      "| MACs: 2858.44M\n",
      "| Epoch Time: 453.69s\n",
      "\n",
      "Test Loss: 2.9767, Top-1 Acc: 48.46%, Top-3 Acc: 57.59%, Latency: 0.003492s/img, Mem: 55.91MB\n",
      "MACs: 2858.44 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 3.9930, Top-1 Acc: 25.96%, Top-3 Acc: 34.42%, Latency: 0.003555s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5124\n",
      "| Test Loss: 3.9930\n",
      "| Top-1 Accuracy: 25.96%\n",
      "| Top-3 Accuracy: 34.42%\n",
      "| Latency: 0.003555s/img\n",
      "| Memory Usage: 69.40MB\n",
      "| MACs: 3558.19M\n",
      "| Epoch Time: 481.66s\n",
      "\n",
      "Test Loss: 3.5653, Top-1 Acc: 35.60%, Top-3 Acc: 44.48%, Latency: 0.003640s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.7001\n",
      "| Test Loss: 3.5653\n",
      "| Top-1 Accuracy: 35.60%\n",
      "| Top-3 Accuracy: 44.48%\n",
      "| Latency: 0.003640s/img\n",
      "| Memory Usage: 69.40MB\n",
      "| MACs: 3558.19M\n",
      "| Epoch Time: 482.72s\n",
      "\n",
      "Test Loss: 3.2765, Top-1 Acc: 41.65%, Top-3 Acc: 50.76%, Latency: 0.003708s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2647\n",
      "| Test Loss: 3.2765\n",
      "| Top-1 Accuracy: 41.65%\n",
      "| Top-3 Accuracy: 50.76%\n",
      "| Latency: 0.003708s/img\n",
      "| Memory Usage: 69.40MB\n",
      "| MACs: 3558.19M\n",
      "| Epoch Time: 482.21s\n",
      "\n",
      "Test Loss: 3.0544, Top-1 Acc: 46.56%, Top-3 Acc: 55.60%, Latency: 0.003823s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.9024\n",
      "| Test Loss: 3.0544\n",
      "| Top-1 Accuracy: 46.56%\n",
      "| Top-3 Accuracy: 55.60%\n",
      "| Latency: 0.003823s/img\n",
      "| Memory Usage: 69.40MB\n",
      "| MACs: 3558.19M\n",
      "| Epoch Time: 511.37s\n",
      "\n",
      "Test Loss: 2.9160, Top-1 Acc: 49.90%, Top-3 Acc: 59.01%, Latency: 0.003865s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5599\n",
      "| Test Loss: 2.9160\n",
      "| Top-1 Accuracy: 49.90%\n",
      "| Top-3 Accuracy: 59.01%\n",
      "| Latency: 0.003865s/img\n",
      "| Memory Usage: 69.40MB\n",
      "| MACs: 3558.19M\n",
      "| Epoch Time: 530.51s\n",
      "\n",
      "Test Loss: 2.9160, Top-1 Acc: 49.90%, Top-3 Acc: 59.01%, Latency: 0.003791s/img, Mem: 69.40MB\n",
      "MACs: 3558.19 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=4, MLP Ratio=2.0\n",
      "Test Loss: 4.0021, Top-1 Acc: 26.39%, Top-3 Acc: 34.47%, Latency: 0.003443s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.5089\n",
      "| Test Loss: 4.0021\n",
      "| Top-1 Accuracy: 26.39%\n",
      "| Top-3 Accuracy: 34.47%\n",
      "| Latency: 0.003443s/img\n",
      "| Memory Usage: 58.55MB\n",
      "| MACs: 2987.16M\n",
      "| Epoch Time: 433.72s\n",
      "\n",
      "Test Loss: 3.4957, Top-1 Acc: 37.38%, Top-3 Acc: 46.42%, Latency: 0.003436s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.6955\n",
      "| Test Loss: 3.4957\n",
      "| Top-1 Accuracy: 37.38%\n",
      "| Top-3 Accuracy: 46.42%\n",
      "| Latency: 0.003436s/img\n",
      "| Memory Usage: 58.55MB\n",
      "| MACs: 2987.16M\n",
      "| Epoch Time: 422.06s\n",
      "\n",
      "Test Loss: 3.2219, Top-1 Acc: 43.06%, Top-3 Acc: 52.19%, Latency: 0.003536s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2409\n",
      "| Test Loss: 3.2219\n",
      "| Top-1 Accuracy: 43.06%\n",
      "| Top-3 Accuracy: 52.19%\n",
      "| Latency: 0.003536s/img\n",
      "| Memory Usage: 58.55MB\n",
      "| MACs: 2987.16M\n",
      "| Epoch Time: 419.96s\n",
      "\n",
      "Test Loss: 3.0399, Top-1 Acc: 46.93%, Top-3 Acc: 56.15%, Latency: 0.003573s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.8838\n",
      "| Test Loss: 3.0399\n",
      "| Top-1 Accuracy: 46.93%\n",
      "| Top-3 Accuracy: 56.15%\n",
      "| Latency: 0.003573s/img\n",
      "| Memory Usage: 58.55MB\n",
      "| MACs: 2987.16M\n",
      "| Epoch Time: 416.83s\n",
      "\n",
      "Test Loss: 2.9425, Top-1 Acc: 49.18%, Top-3 Acc: 58.18%, Latency: 0.003428s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5505\n",
      "| Test Loss: 2.9425\n",
      "| Top-1 Accuracy: 49.18%\n",
      "| Top-3 Accuracy: 58.18%\n",
      "| Latency: 0.003428s/img\n",
      "| Memory Usage: 58.55MB\n",
      "| MACs: 2987.16M\n",
      "| Epoch Time: 418.26s\n",
      "\n",
      "Test Loss: 2.9425, Top-1 Acc: 49.18%, Top-3 Acc: 58.18%, Latency: 0.003568s/img, Mem: 58.55MB\n",
      "MACs: 2987.16 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=4, MLP Ratio=4.0\n",
      "Test Loss: 3.9631, Top-1 Acc: 27.09%, Top-3 Acc: 35.20%, Latency: 0.003506s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.4921\n",
      "| Test Loss: 3.9631\n",
      "| Top-1 Accuracy: 27.09%\n",
      "| Top-3 Accuracy: 35.20%\n",
      "| Latency: 0.003506s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 411.98s\n",
      "\n",
      "Test Loss: 3.5016, Top-1 Acc: 36.60%, Top-3 Acc: 46.12%, Latency: 0.003500s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.6751\n",
      "| Test Loss: 3.5016\n",
      "| Top-1 Accuracy: 36.60%\n",
      "| Top-3 Accuracy: 46.12%\n",
      "| Latency: 0.003500s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 410.66s\n",
      "\n",
      "Test Loss: 3.2402, Top-1 Acc: 42.56%, Top-3 Acc: 51.82%, Latency: 0.003567s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2389\n",
      "| Test Loss: 3.2402\n",
      "| Top-1 Accuracy: 42.56%\n",
      "| Top-3 Accuracy: 51.82%\n",
      "| Latency: 0.003567s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 409.66s\n",
      "\n",
      "Test Loss: 3.0057, Top-1 Acc: 47.44%, Top-3 Acc: 56.48%, Latency: 0.003411s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.8809\n",
      "| Test Loss: 3.0057\n",
      "| Top-1 Accuracy: 47.44%\n",
      "| Top-3 Accuracy: 56.48%\n",
      "| Latency: 0.003411s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 410.30s\n",
      "\n",
      "Test Loss: 2.9235, Top-1 Acc: 49.24%, Top-3 Acc: 58.53%, Latency: 0.003549s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5629\n",
      "| Test Loss: 2.9235\n",
      "| Top-1 Accuracy: 49.24%\n",
      "| Top-3 Accuracy: 58.53%\n",
      "| Latency: 0.003549s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 410.22s\n",
      "\n",
      "Test Loss: 2.9235, Top-1 Acc: 49.24%, Top-3 Acc: 58.53%, Latency: 0.003447s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=12, MLP Ratio=2.0\n",
      "Test Loss: 3.9417, Top-1 Acc: 27.57%, Top-3 Acc: 35.69%, Latency: 0.003683s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 4.4820\n",
      "| Test Loss: 3.9417\n",
      "| Top-1 Accuracy: 27.57%\n",
      "| Top-3 Accuracy: 35.69%\n",
      "| Latency: 0.003683s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 486.88s\n",
      "\n",
      "Test Loss: 3.5549, Top-1 Acc: 35.47%, Top-3 Acc: 44.30%, Latency: 0.003732s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 3.6724\n",
      "| Test Loss: 3.5549\n",
      "| Top-1 Accuracy: 35.47%\n",
      "| Top-3 Accuracy: 44.30%\n",
      "| Latency: 0.003732s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 503.52s\n",
      "\n",
      "Test Loss: 3.2474, Top-1 Acc: 42.72%, Top-3 Acc: 51.46%, Latency: 0.003753s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 3.2299\n",
      "| Test Loss: 3.2474\n",
      "| Top-1 Accuracy: 42.72%\n",
      "| Top-3 Accuracy: 51.46%\n",
      "| Latency: 0.003753s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 509.16s\n",
      "\n",
      "Test Loss: 3.0447, Top-1 Acc: 46.65%, Top-3 Acc: 55.63%, Latency: 0.003757s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 2.8622\n",
      "| Test Loss: 3.0447\n",
      "| Top-1 Accuracy: 46.65%\n",
      "| Top-3 Accuracy: 55.63%\n",
      "| Latency: 0.003757s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 508.53s\n",
      "\n",
      "Test Loss: 2.9471, Top-1 Acc: 49.31%, Top-3 Acc: 58.46%, Latency: 0.003700s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 2.5115\n",
      "| Test Loss: 2.9471\n",
      "| Top-1 Accuracy: 49.31%\n",
      "| Top-3 Accuracy: 58.46%\n",
      "| Latency: 0.003700s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 483.29s\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:626] . unexpected pos 68828736 vs 68828624",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/90: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 552\u001b[39m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m population\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# Run the evolutionary algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[43mevolutionary_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 485\u001b[39m, in \u001b[36mevolutionary_algorithm\u001b[39m\u001b[34m(population_size, generations, mutation_rate, crossover_rate, train_loader, test_loader)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m     load_pretrained_weights(model)\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43marchitecture_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\u001b[39;00m\n\u001b[32m    488\u001b[39m accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 395\u001b[39m, in \u001b[36mfine_tune_model\u001b[39m\u001b[34m(sampled_model, train_loader, test_loader, epochs, architecture_folder)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m architecture_folder:\n\u001b[32m    394\u001b[39m     os.makedirs(architecture_folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoint.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sampled_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/serialization.py:784\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:626] . unexpected pos 68828736 vs 68828624"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "SAVE_PATH = '/SN02DATA/nas_vision/evol_img200-wts'\n",
    "\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Get embedding dimensions\n",
    "    pretrained_embed_dim = pretrained_state_dict['pos_embed'].shape[-1]\n",
    "    current_embed_dim = model.embed_dim\n",
    "    \n",
    "    # Create adaptation modules\n",
    "    adaptation_modules = nn.ModuleDict()\n",
    "    if pretrained_embed_dim != current_embed_dim:\n",
    "        adaptation_modules['pos_embed_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "        adaptation_modules['cls_token_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "    \n",
    "    # Project pretrained weights\n",
    "    filtered_dict = {}\n",
    "    for k, v in pretrained_state_dict.items():\n",
    "        if k == 'pos_embed' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['pos_embed_proj'](v)\n",
    "        elif k == 'cls_token' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['cls_token_proj'](v)\n",
    "        elif k in model.state_dict() and v.shape == model.state_dict()[k].shape:  # FIXED HERE\n",
    "            filtered_dict[k] = v\n",
    "    \n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Loaded pretrained weights with {'adaptation' if pretrained_embed_dim != current_embed_dim else 'no'} projection\")\n",
    "    \n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        # embed_dim = 768  # Fixed embedding dimension\n",
    "        embed_dim = random.choice([384, 480, 768])  # Variable embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200\n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(3,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top3]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(3,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top3_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-3 Acc: {top3_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top3_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    mutation_choices = [\n",
    "        (random.choice([6, 8, 10, 12]), 'depth'),\n",
    "        (random.choice([4, 8, 12, 16]), 'num_heads'),\n",
    "        (random.choice([2.0, 4.0, 6.0]), 'mlp_ratio'),\n",
    "        (random.choice([384, 480, 768]), 'embed_dim')\n",
    "    ]\n",
    "    \n",
    "    # Mutate at least one parameter\n",
    "    while True:\n",
    "        for new_val, param in mutation_choices:\n",
    "            if random.random() < 0.5:\n",
    "                if param == 'depth': depth = new_val\n",
    "                elif param == 'num_heads': num_heads = new_val\n",
    "                elif param == 'mlp_ratio': mlp_ratio = new_val\n",
    "                elif param == 'embed_dim': embed_dim = new_val\n",
    "        if (depth, num_heads, mlp_ratio, embed_dim) != architecture:\n",
    "            break\n",
    "            \n",
    "    return (depth, num_heads, mlp_ratio, embed_dim)                            ## check whether tuple is returned or not\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.randint(0, 3)\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-3 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))                                             ## how top n is taken ??????????\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=200).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-3 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=16, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evolutionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,590,216\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 71,776,712\n",
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,260,232\n",
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 11,095,880\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=4.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,644,808\n",
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,639,432\n",
      "Sampled architecture: Depth=8, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,600,968\n",
      "Sampled architecture: Depth=12, Num Heads=12, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 57,622,472\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 12,287,816\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 19,046,120\n",
      "Sampled architecture: Depth=8, Num Heads=12, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 22,729,640\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 12,287,816\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,655,560\n",
      "Sampled architecture: Depth=8, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 38,714,312\n",
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,639,432\n",
      "Sampled architecture: Depth=6, Num Heads=4, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 17,187,560\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=12, MLP Ratio=6.0\n",
      "Test Loss: 3.3514, Top-1 Acc: 39.84%, Top-3 Acc: 49.16%, Latency: 0.004180s/img, Mem: 219.69MB\n",
      "MACs: 11293.36 M\n",
      "\n",
      "Epoch 1/3 Summary:\n",
      "| Training Loss: 4.0516\n",
      "| Test Loss: 3.3514\n",
      "| Top-1 Accuracy: 39.84%\n",
      "| Top-3 Accuracy: 49.16%\n",
      "| Latency: 0.004180s/img\n",
      "| Memory Usage: 219.69MB\n",
      "| MACs: 11293.36M\n",
      "| Epoch Time: 586.25s\n",
      "\n",
      "Test Loss: 2.7951, Top-1 Acc: 51.66%, Top-3 Acc: 60.84%, Latency: 0.004028s/img, Mem: 219.69MB\n",
      "MACs: 11293.36 M\n",
      "\n",
      "Epoch 2/3 Summary:\n",
      "| Training Loss: 2.9273\n",
      "| Test Loss: 2.7951\n",
      "| Top-1 Accuracy: 51.66%\n",
      "| Top-3 Accuracy: 60.84%\n",
      "| Latency: 0.004028s/img\n",
      "| Memory Usage: 219.69MB\n",
      "| MACs: 11293.36M\n",
      "| Epoch Time: 591.31s\n",
      "\n",
      "Test Loss: 2.5685, Top-1 Acc: 57.07%, Top-3 Acc: 66.01%, Latency: 0.004153s/img, Mem: 219.69MB\n",
      "MACs: 11293.36 M\n",
      "\n",
      "Epoch 3/3 Summary:\n",
      "| Training Loss: 2.4268\n",
      "| Test Loss: 2.5685\n",
      "| Top-1 Accuracy: 57.07%\n",
      "| Top-3 Accuracy: 66.01%\n",
      "| Latency: 0.004153s/img\n",
      "| Memory Usage: 219.69MB\n",
      "| MACs: 11293.36M\n",
      "| Epoch Time: 596.23s\n",
      "\n",
      "Test Loss: 2.5685, Top-1 Acc: 57.07%, Top-3 Acc: 66.01%, Latency: 0.004191s/img, Mem: 219.69MB\n",
      "MACs: 11293.36 M\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 0.9214, Top-1 Acc: 89.33%, Top-3 Acc: 93.45%, Latency: 0.004396s/img, Mem: 273.81MB\n",
      "MACs: 14088.10 M\n",
      "\n",
      "Epoch 1/3 Summary:\n",
      "| Training Loss: 1.4534\n",
      "| Test Loss: 0.9214\n",
      "| Top-1 Accuracy: 89.33%\n",
      "| Top-3 Accuracy: 93.45%\n",
      "| Latency: 0.004396s/img\n",
      "| Memory Usage: 273.81MB\n",
      "| MACs: 14088.10M\n",
      "| Epoch Time: 702.37s\n",
      "\n",
      "Test Loss: 0.9288, Top-1 Acc: 89.32%, Top-3 Acc: 93.41%, Latency: 0.036257s/img, Mem: 273.81MB\n",
      "MACs: 14088.10 M\n",
      "\n",
      "Epoch 2/3 Summary:\n",
      "| Training Loss: 0.6309\n",
      "| Test Loss: 0.9288\n",
      "| Top-1 Accuracy: 89.32%\n",
      "| Top-3 Accuracy: 93.41%\n",
      "| Latency: 0.036257s/img\n",
      "| Memory Usage: 273.81MB\n",
      "| MACs: 14088.10M\n",
      "| Epoch Time: 1512.37s\n",
      "\n",
      "Test Loss: 1.0024, Top-1 Acc: 88.73%, Top-3 Acc: 92.84%, Latency: 0.123228s/img, Mem: 273.81MB\n",
      "MACs: 14088.10 M\n",
      "\n",
      "Epoch 3/3 Summary:\n",
      "| Training Loss: 0.4041\n",
      "| Test Loss: 1.0024\n",
      "| Top-1 Accuracy: 88.73%\n",
      "| Top-3 Accuracy: 92.84%\n",
      "| Latency: 0.123228s/img\n",
      "| Memory Usage: 273.81MB\n",
      "| MACs: 14088.10M\n",
      "| Epoch Time: 7921.68s\n",
      "\n",
      "Test Loss: 1.0024, Top-1 Acc: 88.73%, Top-3 Acc: 92.84%, Latency: 0.127986s/img, Mem: 273.81MB\n",
      "MACs: 14088.10 M\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 3.1843, Top-1 Acc: 43.45%, Top-3 Acc: 52.68%, Latency: 0.167183s/img, Mem: 111.62MB\n",
      "MACs: 5705.09 M\n",
      "\n",
      "Epoch 1/3 Summary:\n",
      "| Training Loss: 3.9427\n",
      "| Test Loss: 3.1843\n",
      "| Top-1 Accuracy: 43.45%\n",
      "| Top-3 Accuracy: 52.68%\n",
      "| Latency: 0.167183s/img\n",
      "| Memory Usage: 111.62MB\n",
      "| MACs: 5705.09M\n",
      "| Epoch Time: 12339.86s\n",
      "\n",
      "Test Loss: 2.7312, Top-1 Acc: 53.49%, Top-3 Acc: 62.52%, Latency: 0.016853s/img, Mem: 111.62MB\n",
      "MACs: 5705.09 M\n",
      "\n",
      "Epoch 2/3 Summary:\n",
      "| Training Loss: 2.8480\n",
      "| Test Loss: 2.7312\n",
      "| Top-1 Accuracy: 53.49%\n",
      "| Top-3 Accuracy: 62.52%\n",
      "| Latency: 0.016853s/img\n",
      "| Memory Usage: 111.62MB\n",
      "| MACs: 5705.09M\n",
      "| Epoch Time: 13182.23s\n",
      "\n",
      "Test Loss: 2.5039, Top-1 Acc: 58.15%, Top-3 Acc: 67.17%, Latency: 0.003979s/img, Mem: 111.62MB\n",
      "MACs: 5705.09 M\n",
      "\n",
      "Epoch 3/3 Summary:\n",
      "| Training Loss: 2.3463\n",
      "| Test Loss: 2.5039\n",
      "| Top-1 Accuracy: 58.15%\n",
      "| Top-3 Accuracy: 67.17%\n",
      "| Latency: 0.003979s/img\n",
      "| Memory Usage: 111.62MB\n",
      "| MACs: 5705.09M\n",
      "| Epoch Time: 771.70s\n",
      "\n",
      "Test Loss: 2.5039, Top-1 Acc: 58.15%, Top-3 Acc: 67.17%, Latency: 0.004063s/img, Mem: 111.62MB\n",
      "MACs: 5705.09 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=12, MLP Ratio=4.0\n",
      "Test Loss: 3.9821, Top-1 Acc: 26.93%, Top-3 Acc: 35.27%, Latency: 0.003735s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 4.4958\n",
      "| Test Loss: 3.9821\n",
      "| Top-1 Accuracy: 26.93%\n",
      "| Top-3 Accuracy: 35.27%\n",
      "| Latency: 0.003735s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 445.04s\n",
      "\n",
      "Test Loss: 3.5675, Top-1 Acc: 35.51%, Top-3 Acc: 44.54%, Latency: 0.003637s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.7009\n",
      "| Test Loss: 3.5675\n",
      "| Top-1 Accuracy: 35.51%\n",
      "| Top-3 Accuracy: 44.54%\n",
      "| Latency: 0.003637s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 443.43s\n",
      "\n",
      "Test Loss: 3.2709, Top-1 Acc: 41.94%, Top-3 Acc: 51.00%, Latency: 0.003556s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.2579\n",
      "| Test Loss: 3.2709\n",
      "| Top-1 Accuracy: 41.94%\n",
      "| Top-3 Accuracy: 51.00%\n",
      "| Latency: 0.003556s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 437.42s\n",
      "\n",
      "Test Loss: 3.0713, Top-1 Acc: 46.24%, Top-3 Acc: 55.66%, Latency: 0.003801s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 4/16 Summary:\n",
      "| Training Loss: 2.9049\n",
      "| Test Loss: 3.0713\n",
      "| Top-1 Accuracy: 46.24%\n",
      "| Top-3 Accuracy: 55.66%\n",
      "| Latency: 0.003801s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 429.90s\n",
      "\n",
      "Test Loss: 2.9903, Top-1 Acc: 48.17%, Top-3 Acc: 57.36%, Latency: 0.003839s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 5/16 Summary:\n",
      "| Training Loss: 2.5751\n",
      "| Test Loss: 2.9903\n",
      "| Top-1 Accuracy: 48.17%\n",
      "| Top-3 Accuracy: 57.36%\n",
      "| Latency: 0.003839s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 446.59s\n",
      "\n",
      "Test Loss: 2.9391, Top-1 Acc: 49.90%, Top-3 Acc: 58.96%, Latency: 0.003748s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 6/16 Summary:\n",
      "| Training Loss: 2.2313\n",
      "| Test Loss: 2.9391\n",
      "| Top-1 Accuracy: 49.90%\n",
      "| Top-3 Accuracy: 58.96%\n",
      "| Latency: 0.003748s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 446.14s\n",
      "\n",
      "Test Loss: 2.9935, Top-1 Acc: 49.67%, Top-3 Acc: 58.62%, Latency: 0.003534s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 7/16 Summary:\n",
      "| Training Loss: 1.8550\n",
      "| Test Loss: 2.9935\n",
      "| Top-1 Accuracy: 49.67%\n",
      "| Top-3 Accuracy: 58.62%\n",
      "| Latency: 0.003534s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 434.35s\n",
      "\n",
      "Test Loss: 3.0808, Top-1 Acc: 49.71%, Top-3 Acc: 58.41%, Latency: 0.003593s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 8/16 Summary:\n",
      "| Training Loss: 1.4344\n",
      "| Test Loss: 3.0808\n",
      "| Top-1 Accuracy: 49.71%\n",
      "| Top-3 Accuracy: 58.41%\n",
      "| Latency: 0.003593s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 430.63s\n",
      "\n",
      "Test Loss: 3.2739, Top-1 Acc: 48.71%, Top-3 Acc: 57.30%, Latency: 0.003673s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 9/16 Summary:\n",
      "| Training Loss: 0.9890\n",
      "| Test Loss: 3.2739\n",
      "| Top-1 Accuracy: 48.71%\n",
      "| Top-3 Accuracy: 57.30%\n",
      "| Latency: 0.003673s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 439.29s\n",
      "\n",
      "Test Loss: 3.6041, Top-1 Acc: 47.19%, Top-3 Acc: 56.06%, Latency: 0.003727s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 10/16 Summary:\n",
      "| Training Loss: 0.6049\n",
      "| Test Loss: 3.6041\n",
      "| Top-1 Accuracy: 47.19%\n",
      "| Top-3 Accuracy: 56.06%\n",
      "| Latency: 0.003727s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 443.88s\n",
      "\n",
      "Test Loss: 3.8518, Top-1 Acc: 46.73%, Top-3 Acc: 55.15%, Latency: 0.003733s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 11/16 Summary:\n",
      "| Training Loss: 0.3642\n",
      "| Test Loss: 3.8518\n",
      "| Top-1 Accuracy: 46.73%\n",
      "| Top-3 Accuracy: 55.15%\n",
      "| Latency: 0.003733s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 444.19s\n",
      "\n",
      "Test Loss: 4.0577, Top-1 Acc: 46.19%, Top-3 Acc: 54.54%, Latency: 0.003873s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 12/16 Summary:\n",
      "| Training Loss: 0.2632\n",
      "| Test Loss: 4.0577\n",
      "| Top-1 Accuracy: 46.19%\n",
      "| Top-3 Accuracy: 54.54%\n",
      "| Latency: 0.003873s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 447.21s\n",
      "\n",
      "Test Loss: 4.2553, Top-1 Acc: 45.95%, Top-3 Acc: 53.99%, Latency: 0.004014s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 13/16 Summary:\n",
      "| Training Loss: 0.2229\n",
      "| Test Loss: 4.2553\n",
      "| Top-1 Accuracy: 45.95%\n",
      "| Top-3 Accuracy: 53.99%\n",
      "| Latency: 0.004014s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 443.66s\n",
      "\n",
      "Test Loss: 4.4418, Top-1 Acc: 44.66%, Top-3 Acc: 53.01%, Latency: 0.004019s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 14/16 Summary:\n",
      "| Training Loss: 0.1903\n",
      "| Test Loss: 4.4418\n",
      "| Top-1 Accuracy: 44.66%\n",
      "| Top-3 Accuracy: 53.01%\n",
      "| Latency: 0.004019s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 453.09s\n",
      "\n",
      "Test Loss: 4.5373, Top-1 Acc: 45.19%, Top-3 Acc: 53.37%, Latency: 0.003626s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 15/16 Summary:\n",
      "| Training Loss: 0.1810\n",
      "| Test Loss: 4.5373\n",
      "| Top-1 Accuracy: 45.19%\n",
      "| Top-3 Accuracy: 53.37%\n",
      "| Latency: 0.003626s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 455.33s\n",
      "\n",
      "Test Loss: 4.5986, Top-1 Acc: 45.61%, Top-3 Acc: 53.83%, Latency: 0.003710s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "\n",
      "Epoch 16/16 Summary:\n",
      "| Training Loss: 0.1592\n",
      "| Test Loss: 4.5986\n",
      "| Top-1 Accuracy: 45.61%\n",
      "| Top-3 Accuracy: 53.83%\n",
      "| Latency: 0.003710s/img\n",
      "| Memory Usage: 42.33MB\n",
      "| MACs: 2158.10M\n",
      "| Epoch Time: 438.42s\n",
      "\n",
      "Test Loss: 4.5986, Top-1 Acc: 45.61%, Top-3 Acc: 53.83%, Latency: 0.003748s/img, Mem: 42.33MB\n",
      "MACs: 2158.10 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=8, MLP Ratio=4.0\n",
      "Test Loss: 4.0143, Top-1 Acc: 26.26%, Top-3 Acc: 34.57%, Latency: 0.003890s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 4.5422\n",
      "| Test Loss: 4.0143\n",
      "| Top-1 Accuracy: 26.26%\n",
      "| Top-3 Accuracy: 34.57%\n",
      "| Latency: 0.003890s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 469.09s\n",
      "\n",
      "Test Loss: 3.5768, Top-1 Acc: 35.24%, Top-3 Acc: 44.03%, Latency: 0.003884s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.7537\n",
      "| Test Loss: 3.5768\n",
      "| Top-1 Accuracy: 35.24%\n",
      "| Top-3 Accuracy: 44.03%\n",
      "| Latency: 0.003884s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 471.02s\n",
      "\n",
      "Test Loss: 3.2722, Top-1 Acc: 41.93%, Top-3 Acc: 50.92%, Latency: 0.003874s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.3082\n",
      "| Test Loss: 3.2722\n",
      "| Top-1 Accuracy: 41.93%\n",
      "| Top-3 Accuracy: 50.92%\n",
      "| Latency: 0.003874s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 466.72s\n",
      "\n",
      "Test Loss: 3.1133, Top-1 Acc: 45.36%, Top-3 Acc: 54.43%, Latency: 0.003890s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 4/16 Summary:\n",
      "| Training Loss: 2.9547\n",
      "| Test Loss: 3.1133\n",
      "| Top-1 Accuracy: 45.36%\n",
      "| Top-3 Accuracy: 54.43%\n",
      "| Latency: 0.003890s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 469.05s\n",
      "\n",
      "Test Loss: 2.9518, Top-1 Acc: 48.99%, Top-3 Acc: 57.97%, Latency: 0.003868s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 5/16 Summary:\n",
      "| Training Loss: 2.6314\n",
      "| Test Loss: 2.9518\n",
      "| Top-1 Accuracy: 48.99%\n",
      "| Top-3 Accuracy: 57.97%\n",
      "| Latency: 0.003868s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 467.69s\n",
      "\n",
      "Test Loss: 2.9446, Top-1 Acc: 49.28%, Top-3 Acc: 58.51%, Latency: 0.003881s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 6/16 Summary:\n",
      "| Training Loss: 2.2919\n",
      "| Test Loss: 2.9446\n",
      "| Top-1 Accuracy: 49.28%\n",
      "| Top-3 Accuracy: 58.51%\n",
      "| Latency: 0.003881s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 466.25s\n",
      "\n",
      "Test Loss: 2.9194, Top-1 Acc: 51.06%, Top-3 Acc: 59.86%, Latency: 0.003930s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 7/16 Summary:\n",
      "| Training Loss: 1.9241\n",
      "| Test Loss: 2.9194\n",
      "| Top-1 Accuracy: 51.06%\n",
      "| Top-3 Accuracy: 59.86%\n",
      "| Latency: 0.003930s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 467.67s\n",
      "\n",
      "Test Loss: 3.0175, Top-1 Acc: 50.19%, Top-3 Acc: 59.33%, Latency: 0.003806s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 8/16 Summary:\n",
      "| Training Loss: 1.5076\n",
      "| Test Loss: 3.0175\n",
      "| Top-1 Accuracy: 50.19%\n",
      "| Top-3 Accuracy: 59.33%\n",
      "| Latency: 0.003806s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 460.91s\n",
      "\n",
      "Test Loss: 3.2313, Top-1 Acc: 48.90%, Top-3 Acc: 57.86%, Latency: 0.003883s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 9/16 Summary:\n",
      "| Training Loss: 1.0582\n",
      "| Test Loss: 3.2313\n",
      "| Top-1 Accuracy: 48.90%\n",
      "| Top-3 Accuracy: 57.86%\n",
      "| Latency: 0.003883s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 464.33s\n",
      "\n",
      "Test Loss: 3.5072, Top-1 Acc: 48.19%, Top-3 Acc: 56.59%, Latency: 0.004057s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 10/16 Summary:\n",
      "| Training Loss: 0.6531\n",
      "| Test Loss: 3.5072\n",
      "| Top-1 Accuracy: 48.19%\n",
      "| Top-3 Accuracy: 56.59%\n",
      "| Latency: 0.004057s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 466.55s\n",
      "\n",
      "Test Loss: 3.7365, Top-1 Acc: 47.54%, Top-3 Acc: 55.96%, Latency: 0.004116s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 11/16 Summary:\n",
      "| Training Loss: 0.3838\n",
      "| Test Loss: 3.7365\n",
      "| Top-1 Accuracy: 47.54%\n",
      "| Top-3 Accuracy: 55.96%\n",
      "| Latency: 0.004116s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 488.51s\n",
      "\n",
      "Test Loss: 3.9764, Top-1 Acc: 47.48%, Top-3 Acc: 55.85%, Latency: 0.004061s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 12/16 Summary:\n",
      "| Training Loss: 0.2708\n",
      "| Test Loss: 3.9764\n",
      "| Top-1 Accuracy: 47.48%\n",
      "| Top-3 Accuracy: 55.85%\n",
      "| Latency: 0.004061s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 486.18s\n",
      "\n",
      "Test Loss: 4.2221, Top-1 Acc: 45.98%, Top-3 Acc: 53.81%, Latency: 0.003836s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 13/16 Summary:\n",
      "| Training Loss: 0.2290\n",
      "| Test Loss: 4.2221\n",
      "| Top-1 Accuracy: 45.98%\n",
      "| Top-3 Accuracy: 53.81%\n",
      "| Latency: 0.003836s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 476.44s\n",
      "\n",
      "Test Loss: 4.3304, Top-1 Acc: 46.48%, Top-3 Acc: 54.35%, Latency: 0.003947s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 14/16 Summary:\n",
      "| Training Loss: 0.2014\n",
      "| Test Loss: 4.3304\n",
      "| Top-1 Accuracy: 46.48%\n",
      "| Top-3 Accuracy: 54.35%\n",
      "| Latency: 0.003947s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 472.34s\n",
      "\n",
      "Test Loss: 4.4520, Top-1 Acc: 45.78%, Top-3 Acc: 53.88%, Latency: 0.003903s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 15/16 Summary:\n",
      "| Training Loss: 0.1800\n",
      "| Test Loss: 4.4520\n",
      "| Top-1 Accuracy: 45.78%\n",
      "| Top-3 Accuracy: 53.88%\n",
      "| Latency: 0.003903s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 474.53s\n",
      "\n",
      "Test Loss: 4.5106, Top-1 Acc: 46.88%, Top-3 Acc: 54.73%, Latency: 0.003859s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "\n",
      "Epoch 16/16 Summary:\n",
      "| Training Loss: 0.1675\n",
      "| Test Loss: 4.5106\n",
      "| Top-1 Accuracy: 46.88%\n",
      "| Top-3 Accuracy: 54.73%\n",
      "| Latency: 0.003859s/img\n",
      "| Memory Usage: 55.87MB\n",
      "| MACs: 2858.14M\n",
      "| Epoch Time: 473.27s\n",
      "\n",
      "Test Loss: 4.5106, Top-1 Acc: 46.88%, Top-3 Acc: 54.73%, Latency: 0.003854s/img, Mem: 55.87MB\n",
      "MACs: 2858.14 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=8, MLP Ratio=6.0\n",
      "Test Loss: 4.0278, Top-1 Acc: 25.37%, Top-3 Acc: 33.58%, Latency: 0.003706s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 4.5107\n",
      "| Test Loss: 4.0278\n",
      "| Top-1 Accuracy: 25.37%\n",
      "| Top-3 Accuracy: 33.58%\n",
      "| Latency: 0.003706s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.38s\n",
      "\n",
      "Test Loss: 3.5561, Top-1 Acc: 36.34%, Top-3 Acc: 45.24%, Latency: 0.003825s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.7157\n",
      "| Test Loss: 3.5561\n",
      "| Top-1 Accuracy: 36.34%\n",
      "| Top-3 Accuracy: 45.24%\n",
      "| Latency: 0.003825s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 445.06s\n",
      "\n",
      "Test Loss: 3.2512, Top-1 Acc: 42.39%, Top-3 Acc: 51.58%, Latency: 0.003759s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.2843\n",
      "| Test Loss: 3.2512\n",
      "| Top-1 Accuracy: 42.39%\n",
      "| Top-3 Accuracy: 51.58%\n",
      "| Latency: 0.003759s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.82s\n",
      "\n",
      "Test Loss: 3.1029, Top-1 Acc: 45.58%, Top-3 Acc: 54.76%, Latency: 0.003786s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 4/16 Summary:\n",
      "| Training Loss: 2.9354\n",
      "| Test Loss: 3.1029\n",
      "| Top-1 Accuracy: 45.58%\n",
      "| Top-3 Accuracy: 54.76%\n",
      "| Latency: 0.003786s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.80s\n",
      "\n",
      "Test Loss: 2.9738, Top-1 Acc: 48.64%, Top-3 Acc: 57.55%, Latency: 0.003740s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 5/16 Summary:\n",
      "| Training Loss: 2.6047\n",
      "| Test Loss: 2.9738\n",
      "| Top-1 Accuracy: 48.64%\n",
      "| Top-3 Accuracy: 57.55%\n",
      "| Latency: 0.003740s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 445.57s\n",
      "\n",
      "Test Loss: 2.9083, Top-1 Acc: 50.38%, Top-3 Acc: 59.24%, Latency: 0.003860s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 6/16 Summary:\n",
      "| Training Loss: 2.2680\n",
      "| Test Loss: 2.9083\n",
      "| Top-1 Accuracy: 50.38%\n",
      "| Top-3 Accuracy: 59.24%\n",
      "| Latency: 0.003860s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 443.77s\n",
      "\n",
      "Test Loss: 2.9386, Top-1 Acc: 50.64%, Top-3 Acc: 59.73%, Latency: 0.003801s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 7/16 Summary:\n",
      "| Training Loss: 1.8871\n",
      "| Test Loss: 2.9386\n",
      "| Top-1 Accuracy: 50.64%\n",
      "| Top-3 Accuracy: 59.73%\n",
      "| Latency: 0.003801s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 449.86s\n",
      "\n",
      "Test Loss: 2.9939, Top-1 Acc: 51.02%, Top-3 Acc: 59.54%, Latency: 0.003815s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 8/16 Summary:\n",
      "| Training Loss: 1.4476\n",
      "| Test Loss: 2.9939\n",
      "| Top-1 Accuracy: 51.02%\n",
      "| Top-3 Accuracy: 59.54%\n",
      "| Latency: 0.003815s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 449.66s\n",
      "\n",
      "Test Loss: 3.2067, Top-1 Acc: 50.12%, Top-3 Acc: 58.67%, Latency: 0.003790s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 9/16 Summary:\n",
      "| Training Loss: 0.9821\n",
      "| Test Loss: 3.2067\n",
      "| Top-1 Accuracy: 50.12%\n",
      "| Top-3 Accuracy: 58.67%\n",
      "| Latency: 0.003790s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 450.00s\n",
      "\n",
      "Test Loss: 3.5295, Top-1 Acc: 48.28%, Top-3 Acc: 56.77%, Latency: 0.003770s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 10/16 Summary:\n",
      "| Training Loss: 0.5766\n",
      "| Test Loss: 3.5295\n",
      "| Top-1 Accuracy: 48.28%\n",
      "| Top-3 Accuracy: 56.77%\n",
      "| Latency: 0.003770s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 451.93s\n",
      "\n",
      "Test Loss: 3.7715, Top-1 Acc: 48.06%, Top-3 Acc: 56.52%, Latency: 0.003807s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 11/16 Summary:\n",
      "| Training Loss: 0.3397\n",
      "| Test Loss: 3.7715\n",
      "| Top-1 Accuracy: 48.06%\n",
      "| Top-3 Accuracy: 56.52%\n",
      "| Latency: 0.003807s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.00s\n",
      "\n",
      "Test Loss: 3.9918, Top-1 Acc: 47.13%, Top-3 Acc: 55.63%, Latency: 0.003704s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 12/16 Summary:\n",
      "| Training Loss: 0.2523\n",
      "| Test Loss: 3.9918\n",
      "| Top-1 Accuracy: 47.13%\n",
      "| Top-3 Accuracy: 55.63%\n",
      "| Latency: 0.003704s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 448.41s\n",
      "\n",
      "Test Loss: 4.1701, Top-1 Acc: 46.55%, Top-3 Acc: 54.77%, Latency: 0.003837s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 13/16 Summary:\n",
      "| Training Loss: 0.2165\n",
      "| Test Loss: 4.1701\n",
      "| Top-1 Accuracy: 46.55%\n",
      "| Top-3 Accuracy: 54.77%\n",
      "| Latency: 0.003837s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 445.35s\n",
      "\n",
      "Test Loss: 4.2933, Top-1 Acc: 46.91%, Top-3 Acc: 55.03%, Latency: 0.003814s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 14/16 Summary:\n",
      "| Training Loss: 0.1873\n",
      "| Test Loss: 4.2933\n",
      "| Top-1 Accuracy: 46.91%\n",
      "| Top-3 Accuracy: 55.03%\n",
      "| Latency: 0.003814s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.99s\n",
      "\n",
      "Test Loss: 4.4454, Top-1 Acc: 46.63%, Top-3 Acc: 54.68%, Latency: 0.003752s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 15/16 Summary:\n",
      "| Training Loss: 0.1780\n",
      "| Test Loss: 4.4454\n",
      "| Top-1 Accuracy: 46.63%\n",
      "| Top-3 Accuracy: 54.68%\n",
      "| Latency: 0.003752s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 446.12s\n",
      "\n",
      "Test Loss: 4.5170, Top-1 Acc: 46.12%, Top-3 Acc: 54.15%, Latency: 0.003761s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "\n",
      "Epoch 16/16 Summary:\n",
      "| Training Loss: 0.1504\n",
      "| Test Loss: 4.5170\n",
      "| Top-1 Accuracy: 46.12%\n",
      "| Top-3 Accuracy: 54.15%\n",
      "| Latency: 0.003761s/img\n",
      "| Memory Usage: 55.85MB\n",
      "| MACs: 2857.99M\n",
      "| Epoch Time: 445.69s\n",
      "\n",
      "Test Loss: 4.5170, Top-1 Acc: 46.12%, Top-3 Acc: 54.15%, Latency: 0.003773s/img, Mem: 55.85MB\n",
      "MACs: 2857.99 M\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=16, MLP Ratio=4.0\n",
      "Test Loss: 1.2407, Top-1 Acc: 83.78%, Top-3 Acc: 89.30%, Latency: 0.004536s/img, Mem: 219.73MB\n",
      "MACs: 11293.66 M\n",
      "\n",
      "Epoch 1/3 Summary:\n",
      "| Training Loss: 1.9444\n",
      "| Test Loss: 1.2407\n",
      "| Top-1 Accuracy: 83.78%\n",
      "| Top-3 Accuracy: 89.30%\n",
      "| Latency: 0.004536s/img\n",
      "| Memory Usage: 219.73MB\n",
      "| MACs: 11293.66M\n",
      "| Epoch Time: 685.26s\n",
      "\n",
      "Test Loss: 1.0980, Top-1 Acc: 86.86%, Top-3 Acc: 91.52%, Latency: 0.005123s/img, Mem: 219.73MB\n",
      "MACs: 11293.66 M\n",
      "\n",
      "Epoch 2/3 Summary:\n",
      "| Training Loss: 0.8219\n",
      "| Test Loss: 1.0980\n",
      "| Top-1 Accuracy: 86.86%\n",
      "| Top-3 Accuracy: 91.52%\n",
      "| Latency: 0.005123s/img\n",
      "| Memory Usage: 219.73MB\n",
      "| MACs: 11293.66M\n",
      "| Epoch Time: 1532.80s\n",
      "\n",
      "Test Loss: 1.1613, Top-1 Acc: 86.26%, Top-3 Acc: 91.02%, Latency: 0.004424s/img, Mem: 219.73MB\n",
      "MACs: 11293.66 M\n",
      "\n",
      "Epoch 3/3 Summary:\n",
      "| Training Loss: 0.4718\n",
      "| Test Loss: 1.1613\n",
      "| Top-1 Accuracy: 86.26%\n",
      "| Top-3 Accuracy: 91.02%\n",
      "| Latency: 0.004424s/img\n",
      "| Memory Usage: 219.73MB\n",
      "| MACs: 11293.66M\n",
      "| Epoch Time: 674.18s\n",
      "\n",
      "Test Loss: 1.1613, Top-1 Acc: 86.26%, Top-3 Acc: 91.02%, Latency: 0.004388s/img, Mem: 219.73MB\n",
      "MACs: 11293.66 M\n",
      "Loaded pretrained weights with no projection\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=12, MLP Ratio=2.0\n",
      "Test Loss: 3.1986, Top-1 Acc: 42.57%, Top-3 Acc: 52.07%, Latency: 0.004804s/img, Mem: 219.81MB\n",
      "MACs: 11294.26 M\n",
      "\n",
      "Epoch 1/3 Summary:\n",
      "| Training Loss: 4.0394\n",
      "| Test Loss: 3.1986\n",
      "| Top-1 Accuracy: 42.57%\n",
      "| Top-3 Accuracy: 52.07%\n",
      "| Latency: 0.004804s/img\n",
      "| Memory Usage: 219.81MB\n",
      "| MACs: 11294.26M\n",
      "| Epoch Time: 728.56s\n",
      "\n",
      "Test Loss: 2.6938, Top-1 Acc: 53.85%, Top-3 Acc: 63.49%, Latency: 0.005333s/img, Mem: 219.81MB\n",
      "MACs: 11294.26 M\n",
      "\n",
      "Epoch 2/3 Summary:\n",
      "| Training Loss: 2.8335\n",
      "| Test Loss: 2.6938\n",
      "| Top-1 Accuracy: 53.85%\n",
      "| Top-3 Accuracy: 63.49%\n",
      "| Latency: 0.005333s/img\n",
      "| Memory Usage: 219.81MB\n",
      "| MACs: 11294.26M\n",
      "| Epoch Time: 770.47s\n",
      "\n",
      "Test Loss: 2.4489, Top-1 Acc: 59.51%, Top-3 Acc: 68.11%, Latency: 0.004596s/img, Mem: 219.81MB\n",
      "MACs: 11294.26 M\n",
      "\n",
      "Epoch 3/3 Summary:\n",
      "| Training Loss: 2.2716\n",
      "| Test Loss: 2.4489\n",
      "| Top-1 Accuracy: 59.51%\n",
      "| Top-3 Accuracy: 68.11%\n",
      "| Latency: 0.004596s/img\n",
      "| Memory Usage: 219.81MB\n",
      "| MACs: 11294.26M\n",
      "| Epoch Time: 721.33s\n",
      "\n",
      "Test Loss: 2.4489, Top-1 Acc: 59.51%, Top-3 Acc: 68.11%, Latency: 0.004483s/img, Mem: 219.81MB\n",
      "MACs: 11294.26 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=16, MLP Ratio=2.0\n",
      "Test Loss: 4.0010, Top-1 Acc: 26.93%, Top-3 Acc: 34.79%, Latency: 0.004000s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 4.5490\n",
      "| Test Loss: 4.0010\n",
      "| Top-1 Accuracy: 26.93%\n",
      "| Top-3 Accuracy: 34.79%\n",
      "| Latency: 0.004000s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 517.07s\n",
      "\n",
      "Test Loss: 3.4908, Top-1 Acc: 37.40%, Top-3 Acc: 46.16%, Latency: 0.003883s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.6934\n",
      "| Test Loss: 3.4908\n",
      "| Top-1 Accuracy: 37.40%\n",
      "| Top-3 Accuracy: 46.16%\n",
      "| Latency: 0.003883s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 507.10s\n",
      "\n",
      "Test Loss: 3.2265, Top-1 Acc: 42.85%, Top-3 Acc: 51.99%, Latency: 0.004012s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.2520\n",
      "| Test Loss: 3.2265\n",
      "| Top-1 Accuracy: 42.85%\n",
      "| Top-3 Accuracy: 51.99%\n",
      "| Latency: 0.004012s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 507.82s\n",
      "\n",
      "Test Loss: 3.0932, Top-1 Acc: 45.54%, Top-3 Acc: 54.82%, Latency: 0.005427s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 4/16 Summary:\n",
      "| Training Loss: 2.9027\n",
      "| Test Loss: 3.0932\n",
      "| Top-1 Accuracy: 45.54%\n",
      "| Top-3 Accuracy: 54.82%\n",
      "| Latency: 0.005427s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 1447.75s\n",
      "\n",
      "Test Loss: 3.0277, Top-1 Acc: 47.46%, Top-3 Acc: 56.51%, Latency: 0.004034s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 5/16 Summary:\n",
      "| Training Loss: 2.5839\n",
      "| Test Loss: 3.0277\n",
      "| Top-1 Accuracy: 47.46%\n",
      "| Top-3 Accuracy: 56.51%\n",
      "| Latency: 0.004034s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 582.19s\n",
      "\n",
      "Test Loss: 2.9816, Top-1 Acc: 49.24%, Top-3 Acc: 58.00%, Latency: 0.004305s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 6/16 Summary:\n",
      "| Training Loss: 2.2491\n",
      "| Test Loss: 2.9816\n",
      "| Top-1 Accuracy: 49.24%\n",
      "| Top-3 Accuracy: 58.00%\n",
      "| Latency: 0.004305s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 512.58s\n",
      "\n",
      "Test Loss: 2.9653, Top-1 Acc: 50.12%, Top-3 Acc: 58.98%, Latency: 0.003937s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 7/16 Summary:\n",
      "| Training Loss: 1.8851\n",
      "| Test Loss: 2.9653\n",
      "| Top-1 Accuracy: 50.12%\n",
      "| Top-3 Accuracy: 58.98%\n",
      "| Latency: 0.003937s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 525.45s\n",
      "\n",
      "Test Loss: 3.1048, Top-1 Acc: 49.36%, Top-3 Acc: 58.20%, Latency: 0.004597s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 8/16 Summary:\n",
      "| Training Loss: 1.4721\n",
      "| Test Loss: 3.1048\n",
      "| Top-1 Accuracy: 49.36%\n",
      "| Top-3 Accuracy: 58.20%\n",
      "| Latency: 0.004597s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 555.53s\n",
      "\n",
      "Test Loss: 3.2918, Top-1 Acc: 49.10%, Top-3 Acc: 57.51%, Latency: 0.003977s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 9/16 Summary:\n",
      "| Training Loss: 1.0427\n",
      "| Test Loss: 3.2918\n",
      "| Top-1 Accuracy: 49.10%\n",
      "| Top-3 Accuracy: 57.51%\n",
      "| Latency: 0.003977s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 523.90s\n",
      "\n",
      "Test Loss: 3.5666, Top-1 Acc: 47.64%, Top-3 Acc: 55.93%, Latency: 0.098039s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 10/16 Summary:\n",
      "| Training Loss: 0.6452\n",
      "| Test Loss: 3.5666\n",
      "| Top-1 Accuracy: 47.64%\n",
      "| Top-3 Accuracy: 55.93%\n",
      "| Latency: 0.098039s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 4609.97s\n",
      "\n",
      "Test Loss: 3.8064, Top-1 Acc: 47.00%, Top-3 Acc: 55.22%, Latency: 0.081203s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 11/16 Summary:\n",
      "| Training Loss: 0.3861\n",
      "| Test Loss: 3.8064\n",
      "| Top-1 Accuracy: 47.00%\n",
      "| Top-3 Accuracy: 55.22%\n",
      "| Latency: 0.081203s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 8957.48s\n",
      "\n",
      "Test Loss: 4.0633, Top-1 Acc: 45.85%, Top-3 Acc: 54.18%, Latency: 0.078336s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 12/16 Summary:\n",
      "| Training Loss: 0.2709\n",
      "| Test Loss: 4.0633\n",
      "| Top-1 Accuracy: 45.85%\n",
      "| Top-3 Accuracy: 54.18%\n",
      "| Latency: 0.078336s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 9307.10s\n",
      "\n",
      "Test Loss: 4.2490, Top-1 Acc: 46.07%, Top-3 Acc: 53.97%, Latency: 0.079715s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 13/16 Summary:\n",
      "| Training Loss: 0.2394\n",
      "| Test Loss: 4.2490\n",
      "| Top-1 Accuracy: 46.07%\n",
      "| Top-3 Accuracy: 53.97%\n",
      "| Latency: 0.079715s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 8156.29s\n",
      "\n",
      "Test Loss: 4.4346, Top-1 Acc: 45.54%, Top-3 Acc: 53.79%, Latency: 0.078428s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 14/16 Summary:\n",
      "| Training Loss: 0.1963\n",
      "| Test Loss: 4.4346\n",
      "| Top-1 Accuracy: 45.54%\n",
      "| Top-3 Accuracy: 53.79%\n",
      "| Latency: 0.078428s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 8166.78s\n",
      "\n",
      "Test Loss: 4.5065, Top-1 Acc: 45.49%, Top-3 Acc: 53.38%, Latency: 0.078891s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 15/16 Summary:\n",
      "| Training Loss: 0.1866\n",
      "| Test Loss: 4.5065\n",
      "| Top-1 Accuracy: 45.49%\n",
      "| Top-3 Accuracy: 53.38%\n",
      "| Latency: 0.078891s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 8073.68s\n",
      "\n",
      "Test Loss: 4.6866, Top-1 Acc: 44.45%, Top-3 Acc: 52.72%, Latency: 0.034702s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "\n",
      "Epoch 16/16 Summary:\n",
      "| Training Loss: 0.1617\n",
      "| Test Loss: 4.6866\n",
      "| Top-1 Accuracy: 44.45%\n",
      "| Top-3 Accuracy: 52.72%\n",
      "| Latency: 0.034702s/img\n",
      "| Memory Usage: 46.87MB\n",
      "| MACs: 2391.70M\n",
      "| Epoch Time: 5292.50s\n",
      "\n",
      "Test Loss: 4.6866, Top-1 Acc: 44.45%, Top-3 Acc: 52.72%, Latency: 0.031260s/img, Mem: 46.87MB\n",
      "MACs: 2391.70 M\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=10, Num Heads=12, MLP Ratio=2.0\n",
      "Test Loss: 3.9862, Top-1 Acc: 26.39%, Top-3 Acc: 34.51%, Latency: 0.003952s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 4.4847\n",
      "| Test Loss: 3.9862\n",
      "| Top-1 Accuracy: 26.39%\n",
      "| Top-3 Accuracy: 34.51%\n",
      "| Latency: 0.003952s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 1445.40s\n",
      "\n",
      "Test Loss: 3.4884, Top-1 Acc: 36.76%, Top-3 Acc: 45.64%, Latency: 0.004120s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.6583\n",
      "| Test Loss: 3.4884\n",
      "| Top-1 Accuracy: 36.76%\n",
      "| Top-3 Accuracy: 45.64%\n",
      "| Latency: 0.004120s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 524.54s\n",
      "\n",
      "Test Loss: 3.2155, Top-1 Acc: 43.13%, Top-3 Acc: 52.27%, Latency: 0.004398s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.2065\n",
      "| Test Loss: 3.2155\n",
      "| Top-1 Accuracy: 43.13%\n",
      "| Top-3 Accuracy: 52.27%\n",
      "| Latency: 0.004398s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 553.68s\n",
      "\n",
      "Test Loss: 3.0292, Top-1 Acc: 46.88%, Top-3 Acc: 56.32%, Latency: 0.004295s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 4/16 Summary:\n",
      "| Training Loss: 2.8369\n",
      "| Test Loss: 3.0292\n",
      "| Top-1 Accuracy: 46.88%\n",
      "| Top-3 Accuracy: 56.32%\n",
      "| Latency: 0.004295s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 562.14s\n",
      "\n",
      "Test Loss: 2.9162, Top-1 Acc: 49.57%, Top-3 Acc: 58.93%, Latency: 0.004283s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 5/16 Summary:\n",
      "| Training Loss: 2.4877\n",
      "| Test Loss: 2.9162\n",
      "| Top-1 Accuracy: 49.57%\n",
      "| Top-3 Accuracy: 58.93%\n",
      "| Latency: 0.004283s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 533.26s\n",
      "\n",
      "Test Loss: 2.9124, Top-1 Acc: 50.25%, Top-3 Acc: 59.29%, Latency: 0.003859s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 6/16 Summary:\n",
      "| Training Loss: 2.1144\n",
      "| Test Loss: 2.9124\n",
      "| Top-1 Accuracy: 50.25%\n",
      "| Top-3 Accuracy: 59.29%\n",
      "| Latency: 0.003859s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 519.32s\n",
      "\n",
      "Test Loss: 2.9578, Top-1 Acc: 50.99%, Top-3 Acc: 59.86%, Latency: 0.003998s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 7/16 Summary:\n",
      "| Training Loss: 1.6948\n",
      "| Test Loss: 2.9578\n",
      "| Top-1 Accuracy: 50.99%\n",
      "| Top-3 Accuracy: 59.86%\n",
      "| Latency: 0.003998s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 516.49s\n",
      "\n",
      "Test Loss: 3.1418, Top-1 Acc: 50.29%, Top-3 Acc: 58.97%, Latency: 0.003949s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 8/16 Summary:\n",
      "| Training Loss: 1.2147\n",
      "| Test Loss: 3.1418\n",
      "| Top-1 Accuracy: 50.29%\n",
      "| Top-3 Accuracy: 58.97%\n",
      "| Latency: 0.003949s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 526.30s\n",
      "\n",
      "Test Loss: 3.3647, Top-1 Acc: 48.98%, Top-3 Acc: 57.46%, Latency: 0.004015s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 9/16 Summary:\n",
      "| Training Loss: 0.7512\n",
      "| Test Loss: 3.3647\n",
      "| Top-1 Accuracy: 48.98%\n",
      "| Top-3 Accuracy: 57.46%\n",
      "| Latency: 0.004015s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 520.19s\n",
      "\n",
      "Test Loss: 3.6754, Top-1 Acc: 47.74%, Top-3 Acc: 56.44%, Latency: 0.004181s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 10/16 Summary:\n",
      "| Training Loss: 0.4168\n",
      "| Test Loss: 3.6754\n",
      "| Top-1 Accuracy: 47.74%\n",
      "| Top-3 Accuracy: 56.44%\n",
      "| Latency: 0.004181s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 527.03s\n",
      "\n",
      "Test Loss: 3.9580, Top-1 Acc: 46.70%, Top-3 Acc: 55.14%, Latency: 0.003990s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 11/16 Summary:\n",
      "| Training Loss: 0.2835\n",
      "| Test Loss: 3.9580\n",
      "| Top-1 Accuracy: 46.70%\n",
      "| Top-3 Accuracy: 55.14%\n",
      "| Latency: 0.003990s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 526.35s\n",
      "\n",
      "Test Loss: 4.1410, Top-1 Acc: 46.30%, Top-3 Acc: 54.78%, Latency: 0.093899s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 12/16 Summary:\n",
      "| Training Loss: 0.2406\n",
      "| Test Loss: 4.1410\n",
      "| Top-1 Accuracy: 46.30%\n",
      "| Top-3 Accuracy: 54.78%\n",
      "| Latency: 0.093899s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 11180.02s\n",
      "\n",
      "Test Loss: 4.2260, Top-1 Acc: 46.51%, Top-3 Acc: 54.97%, Latency: 0.077541s/img, Mem: 72.66MB\n",
      "MACs: 3715.84 M\n",
      "\n",
      "Epoch 13/16 Summary:\n",
      "| Training Loss: 0.1999\n",
      "| Test Loss: 4.2260\n",
      "| Top-1 Accuracy: 46.51%\n",
      "| Top-3 Accuracy: 54.97%\n",
      "| Latency: 0.077541s/img\n",
      "| Memory Usage: 72.66MB\n",
      "| MACs: 3715.84M\n",
      "| Epoch Time: 8033.75s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "SAVE_PATH = '/SN02DATA/nas_vision/evol_img200-wts'\n",
    "\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Get embedding dimensions\n",
    "    pretrained_embed_dim = pretrained_state_dict['pos_embed'].shape[-1]\n",
    "    current_embed_dim = model.embed_dim\n",
    "    \n",
    "    # Create adaptation modules\n",
    "    adaptation_modules = nn.ModuleDict()\n",
    "    if pretrained_embed_dim != current_embed_dim:\n",
    "        adaptation_modules['pos_embed_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "        adaptation_modules['cls_token_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "    \n",
    "    # Project pretrained weights\n",
    "    filtered_dict = {}\n",
    "    for k, v in pretrained_state_dict.items():\n",
    "        if k == 'pos_embed' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['pos_embed_proj'](v)\n",
    "        elif k == 'cls_token' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['cls_token_proj'](v)\n",
    "        elif k in model.state_dict() and v.shape == model.state_dict()[k].shape:  # FIXED HERE\n",
    "            filtered_dict[k] = v\n",
    "    \n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Loaded pretrained weights with {'adaptation' if pretrained_embed_dim != current_embed_dim else 'no'} projection\")\n",
    "    \n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        # embed_dim = 768  # Fixed embedding dimension\n",
    "        embed_dim = random.choice([384, 480, 768])  # Variable embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200\n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(3,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top3]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(3,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top3_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-3 Acc: {top3_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top3_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    mutation_choices = [\n",
    "        (random.choice([6, 8, 10, 12]), 'depth'),\n",
    "        (random.choice([4, 8, 12, 16]), 'num_heads'),\n",
    "        (random.choice([2.0, 4.0, 6.0]), 'mlp_ratio'),\n",
    "        (random.choice([384, 480, 768]), 'embed_dim')\n",
    "    ]\n",
    "    \n",
    "    # Mutate at least one parameter\n",
    "    while True:\n",
    "        for new_val, param in mutation_choices:\n",
    "            if random.random() < 0.5:\n",
    "                if param == 'depth': depth = new_val\n",
    "                elif param == 'num_heads': num_heads = new_val\n",
    "                elif param == 'mlp_ratio': mlp_ratio = new_val\n",
    "                elif param == 'embed_dim': embed_dim = new_val\n",
    "        if (depth, num_heads, mlp_ratio, embed_dim) != architecture:\n",
    "            break\n",
    "            \n",
    "    return (depth, num_heads, mlp_ratio, embed_dim)                            ## check whether tuple is returned or not\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.randint(0, 3)\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-3 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))                                             ## how top n is taken ??????????\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        # for arch in population:\n",
    "        #     depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        #     architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        #     checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "        #     model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "        #                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "        #                        num_classes=200).to(device)\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=200).to(device)\n",
    "\n",
    "            # Determine fine-tuning epochs based on embedding dimension\n",
    "            fine_tune_epochs = 16 if embed_dim != 768 else 3  # 768 is pretrained model's embed_dim\n",
    "            \n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            # fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=fine_tune_epochs, \n",
    "                           architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-3 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=16, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pca\n",
    "\n",
    "Step 1: Extract Patch Embeddings from Supernetwork\n",
    "Before applying PCA, pass a representative dataset through the DynamicPatchEmbed layer to get the high-dimensional (768-dim) embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_patch_embeddings(model, dataloader, num_samples=1000):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(tqdm(dataloader)):\n",
    "            if i * images.size(0) >= num_samples:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            x = model.patch_embed(images)  # Shape: (B, num_patches, 768)\n",
    "            x = x.flatten(2).transpose(1, 2)  # (B, num_patches, 768)\n",
    "            embeddings.append(x.reshape(-1, x.shape[-1]))  # (B*num_patches, 768)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu()  # (N, 768)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def fit_pca(embeddings, target_dim=384):\n",
    "    pca = PCA(n_components=target_dim)\n",
    "    pca.fit(embeddings.numpy())\n",
    "    return pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_pca_to_patch_embed(model, pca):\n",
    "#     # Original weights shape: (embed_dim, in_channels, kernel_size, kernel_size)\n",
    "#     old_proj_weight = model.patch_embed.proj.weight.data  # (768, 3, 16, 16)\n",
    "#     old_proj_bias = model.patch_embed.proj.bias.data      # (768,)\n",
    "\n",
    "#     # Reshape to (768, -1) for PCA\n",
    "#     weight_2d = old_proj_weight.view(768, -1).cpu().numpy()\n",
    "#     bias_1d = old_proj_bias.cpu().numpy()\n",
    "\n",
    "#     # Apply PCA transform\n",
    "#     new_weight = pca.transform(weight_2d)  # (768, 384)\n",
    "#     new_weight = torch.tensor(new_weight).view(384, 3, 16, 16)\n",
    "#     new_bias = torch.tensor(pca.transform(bias_1d.reshape(1, -1))[0])  # (384,)\n",
    "\n",
    "#     # Update model patch embedding to 384-dim\n",
    "#     model.patch_embed.proj = nn.Conv2d(3, 384, kernel_size=16, stride=16)\n",
    "#     model.patch_embed.num_patches = (model.patch_embed.proj.kernel_size[0] ** 2)\n",
    "\n",
    "#     # Assign new weights\n",
    "#     model.patch_embed.proj.weight.data.copy_(new_weight)\n",
    "#     model.patch_embed.proj.bias.data.copy_(new_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_patch_embed_weights(model, pca):\n",
    "    # Original weights\n",
    "    old_weight = model.patch_embed.proj.weight.data.view(768, -1).cpu().numpy()  # (768, 3*16*16)\n",
    "    old_bias = model.patch_embed.proj.bias.data.cpu().numpy()  # (768,)\n",
    "\n",
    "    # PCA transform\n",
    "    new_weight = pca.transform(old_weight)  # (768, 384)\n",
    "    new_weight = torch.tensor(new_weight).view(384, 3, 16, 16)\n",
    "\n",
    "    # Bias transform\n",
    "    new_bias = pca.transform(old_bias.reshape(1, -1))[0]  # (384,)\n",
    "    new_bias = torch.tensor(new_bias)\n",
    "\n",
    "    return new_weight, new_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_384(new_weight, new_bias, num_classes=200):\n",
    "    model_384 = DynamicViT(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,  # Must divide 384\n",
    "        mlp_ratio=4.0,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "    # Replace patch_embed weights\n",
    "    model_384.patch_embed.proj.weight.data.copy_(new_weight)\n",
    "    model_384.patch_embed.proj.bias.data.copy_(new_bias)\n",
    "\n",
    "    return model_384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights loaded into DynamicViT successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1556 [00:03<06:12,  4.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=128)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>PCA</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=128)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "load_pretrained_weights(model)\n",
    "\n",
    "# Extract patch embeddings from multiple images\n",
    "# Resulting shape: (num_images, num_patches=196, embed_dim=768)\n",
    "embeddings = extract_patch_embeddings(model, train_loader, num_samples=1000)  # (1000, 196, 768)\n",
    "\n",
    "# Reshape to (num_images * num_patches, 768)\n",
    "embeddings = embeddings.reshape(-1, embeddings.shape[-1])  # (1000 * 196, 768)\n",
    "\n",
    "# Fit PCA to reduce 768 → 128\n",
    "pca = PCA(n_components=384)\n",
    "pca.fit(embeddings.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 768 features, but PCA is expecting 196 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load pretrained weights\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# load_pretrained_weights(model)\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Transform weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m new_weight, new_bias = \u001b[43mtransform_patch_embed_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create new model\u001b[39;00m\n\u001b[32m     15\u001b[39m model_384 = create_vit_384(new_weight, new_bias)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtransform_patch_embed_weights\u001b[39m\u001b[34m(model, pca)\u001b[39m\n\u001b[32m      4\u001b[39m old_bias = model.patch_embed.proj.bias.data.cpu().numpy()  \u001b[38;5;66;03m# (768,)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# PCA transform\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m new_weight = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_weight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (768, 384)\u001b[39;00m\n\u001b[32m      8\u001b[39m new_weight = torch.tensor(new_weight).view(\u001b[32m384\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m16\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Bias transform\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/sklearn/decomposition/_base.py:138\u001b[39m, in \u001b[36m_BasePCA.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    134\u001b[39m xp, _ = get_namespace(X, \u001b[38;5;28mself\u001b[39m.components_, \u001b[38;5;28mself\u001b[39m.explained_variance_)\n\u001b[32m    136\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform(X, xp=xp, x_is_centered=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/sklearn/utils/validation.py:2965\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/sklearn/utils/validation.py:2829\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2830\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2831\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2832\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 768 features, but PCA is expecting 196 features as input."
     ]
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "# load_pretrained_weights(model)\n",
    "\n",
    "# # Extract embeddings\n",
    "# embeddings = extract_patch_embeddings(model, train_loader, num_samples=1000)\n",
    "\n",
    "\n",
    "# # Fit PCA\n",
    "# pca = fit_pca(embeddings, target_dim=128)\n",
    "\n",
    "# Transform weights\n",
    "new_weight, new_bias = transform_patch_embed_weights(model, pca)\n",
    "\n",
    "# Create new model\n",
    "model_384 = create_vit_384(new_weight, new_bias)\n",
    "\n",
    "# Evaluate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluate_model(model_384, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n",
      "✅ Pretrained weights loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1556 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Eval - Loss: 5.4428, Accuracy: 0.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.442845274037159, 0.5143040822886532)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import random\n",
    "# from timm import create_model  # For loading pretrained ViT\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # === Dataset ===\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "# ])\n",
    "# data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "# filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# train_size = int(0.8 * len(filtered_dataset))\n",
    "# test_size = len(filtered_dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# print(f\"Training set size: {len(train_dataset)}\")\n",
    "# print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# # === ViT Components ===\n",
    "# class DynamicPatchEmbed(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "#         self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.proj(x)\n",
    "#         return x.flatten(2).transpose(1, 2)\n",
    "\n",
    "# class DynamicMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "#         self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.scale = (embed_dim // num_heads) ** -0.5\n",
    "#         self.num_heads = num_heads\n",
    "#         assert embed_dim % num_heads == 0\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "#         return self.proj(x)\n",
    "\n",
    "# class MLPBlock(nn.Module):  \n",
    "#     def __init__(self, embed_dim, mlp_ratio):\n",
    "#         super().__init__()\n",
    "#         hidden_dim = int(embed_dim * mlp_ratio)\n",
    "#         self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "#         self.act = nn.GELU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "# class DynamicTransformerBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.mlp = MLPBlock(embed_dim, mlp_ratio)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x = x + self.pos_embed\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)\n",
    "\n",
    "# # === Pretrained weight loader ===\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(\"✅ Pretrained weights loaded.\")\n",
    "\n",
    "# # === PCA Embedding Extraction ===\n",
    "# # def extract_patch_embeddings(model, dataloader, num_samples=1000):\n",
    "# #     model.eval()\n",
    "# #     embeddings = []\n",
    "# #     with torch.no_grad():\n",
    "# #         for i, (images, _) in enumerate(tqdm(dataloader)):\n",
    "# #             if i * images.size(0) >= num_samples:\n",
    "# #                 break\n",
    "# #             images = images.to(device)\n",
    "# #             x = model.patch_embed(images)\n",
    "# #             x = x.flatten(2).transpose(1, 2)\n",
    "# #             embeddings.append(x.reshape(-1, x.shape[-1]))\n",
    "# #     return torch.cat(embeddings, dim=0).cpu()\n",
    "\n",
    "# # def extract_patch_embeddings(model, dataloader, max_samples=10000):\n",
    "# #     model.eval()\n",
    "# #     embeddings = []\n",
    "# #     collected = 0\n",
    "# #     with torch.no_grad():\n",
    "# #         for images, _ in tqdm(dataloader):\n",
    "# #             images = images.to(device)\n",
    "# #             patches = model.patch_embed(images)  # [B, C, H, W] -> [B, embed_dim, H/patch, W/patch]\n",
    "# #             B, C, H, W = patches.shape\n",
    "# #             patches = patches.permute(0, 2, 3, 1).reshape(-1, C)  # Flatten all patches\n",
    "# #             embeddings.append(patches.cpu())\n",
    "# #             collected += patches.shape[0]\n",
    "# #             if collected >= max_samples:\n",
    "# #                 break\n",
    "# #     return torch.cat(embeddings, dim=0)[:max_samples]\n",
    "# def extract_patch_embeddings(model, dataloader, max_samples=10000):\n",
    "#     model.eval()\n",
    "#     embeddings = []\n",
    "#     collected = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in tqdm(dataloader):\n",
    "#             images = images.to(device)\n",
    "#             patches = model.patch_embed(images)  # [B, num_patches, embed_dim]\n",
    "#             B, N, D = patches.shape\n",
    "#             patches = patches.reshape(-1, D)  # Flatten to [B*N, embed_dim]\n",
    "#             embeddings.append(patches.cpu())\n",
    "#             collected += patches.shape[0]\n",
    "#             if collected >= max_samples:\n",
    "#                 break\n",
    "#     return torch.cat(embeddings, dim=0)[:max_samples]\n",
    "\n",
    "\n",
    "# # === PCA weight transformation ===\n",
    "# def transform_patch_embed_weights(model, pca):\n",
    "#     old_weight = model.patch_embed.proj.weight.data.view(768, -1).cpu().numpy()\n",
    "#     old_bias = model.patch_embed.proj.bias.data.cpu().numpy()\n",
    "#     new_weight = pca.transform(old_weight)  # (384,)\n",
    "#     new_weight = torch.tensor(new_weight).view(384, 3, 16, 16)\n",
    "#     new_bias = pca.transform(old_bias.reshape(1, -1))[0]\n",
    "#     new_bias = torch.tensor(new_bias)\n",
    "#     return new_weight, new_bias\n",
    "\n",
    "# # === Create 384-dim ViT ===\n",
    "# def create_vit_384(new_weight, new_bias, num_classes=200):\n",
    "#     model_384 = DynamicViT(\n",
    "#         img_size=224,\n",
    "#         patch_size=16,\n",
    "#         embed_dim=384,\n",
    "#         depth=12,\n",
    "#         num_heads=6,\n",
    "#         mlp_ratio=4.0,\n",
    "#         num_classes=num_classes\n",
    "#     ).to(device)\n",
    "#     model_384.patch_embed.proj.weight.data.copy_(new_weight)\n",
    "#     model_384.patch_embed.proj.bias.data.copy_(new_bias)\n",
    "#     return model_384\n",
    "\n",
    "# # === Evaluate Model ===\n",
    "# def evaluate_model(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss, correct, total = 0.0, 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in dataloader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             total_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += predicted.eq(targets).sum().item()\n",
    "#     avg_loss = total_loss / total\n",
    "#     accuracy = 100. * correct / total\n",
    "#     print(f\"📊 Eval - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# # === Main Execution ===\n",
    "# model = DynamicViT(embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200).to(device)\n",
    "# load_pretrained_weights(model)\n",
    "# # embeddings = extract_patch_embeddings(model, train_loader, num_samples=1000)\n",
    "# # embeddings = embeddings.reshape(-1, embeddings.shape[-1])  # (1000 * 196, 768)\n",
    "\n",
    "# # pca = PCA(n_components=384)\n",
    "# # pca.fit(embeddings.numpy())\n",
    "\n",
    "# embeddings = extract_patch_embeddings(model, train_loader, max_samples=10000)\n",
    "# pca = PCA(n_components=384)\n",
    "# pca.fit(embeddings.numpy())\n",
    "\n",
    "\n",
    "# new_weight, new_bias = transform_patch_embed_weights(model, pca)\n",
    "# model_384 = create_vit_384(new_weight, new_bias)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# evaluate_model(model_384, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99548\n",
      "Test set size: 24888\n",
      "✅ Pretrained patch_embed weights loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Patch Embeddings:  10%|█         | 157/1556 [00:38<05:44,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding shape for PCA: torch.Size([1969408, 768])\n",
      "📊 Eval - Loss: 5.4709, Accuracy: 0.54%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.47090251757684, 0.5384120861459338)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Data Loading ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# === Dynamic ViT Model Definitions ===\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)  # [B, embed_dim, H', W']\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=200):\n",
    "        super().__init__()\n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # [B, C, H', W']\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n",
    "\n",
    "# === Pretrained Weight Loading ===\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained = timm.create_model(pretrained_model_name, pretrained=True)\n",
    "    model.patch_embed.proj.weight.data.copy_(pretrained.patch_embed.proj.weight.data)\n",
    "    model.patch_embed.proj.bias.data.copy_(pretrained.patch_embed.proj.bias.data)\n",
    "    print(\"✅ Pretrained patch_embed weights loaded.\")\n",
    "\n",
    "# === Patch Embedding Extraction ===\n",
    "def extract_patch_embeddings(model, dataloader, max_samples=10000):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    seen = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloader, desc=\"Extracting Patch Embeddings\"):\n",
    "            if seen >= max_samples:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            patches = model.patch_embed(images)  # [B, C, H', W']\n",
    "            B, C, H, W = patches.shape\n",
    "            patches = patches.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H'*W', C]\n",
    "            embeddings.append(patches.cpu())\n",
    "            seen += images.size(0)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# === PCA + Weight Transform ===\n",
    "def apply_pca_to_conv_weights(conv_layer, pca):\n",
    "    old_weight = conv_layer.weight.data.view(768, -1).cpu().numpy()\n",
    "    new_weight = pca.transform(old_weight)\n",
    "    new_weight = torch.tensor(new_weight).view(384, 3, 16, 16)\n",
    "    old_bias = conv_layer.bias.data.cpu().numpy()\n",
    "    new_bias = pca.transform(old_bias.reshape(1, -1))[0]\n",
    "    new_bias = torch.tensor(new_bias)\n",
    "    return new_weight, new_bias\n",
    "\n",
    "# === New 384-Dim Model ===\n",
    "def create_vit_384(new_weight, new_bias, num_classes=200):\n",
    "    model_384 = DynamicViT(embed_dim=384, depth=12, num_heads=6, num_classes=num_classes)\n",
    "    model_384.patch_embed.proj.weight.data.copy_(new_weight)\n",
    "    model_384.patch_embed.proj.bias.data.copy_(new_bias)\n",
    "    return model_384.to(device)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"📊 Eval - Loss: {total_loss / total:.4f}, Accuracy: {acc:.2f}%\")\n",
    "    return total_loss / total, acc\n",
    "\n",
    "# === Run Full Flow ===\n",
    "model = DynamicViT(embed_dim=768, num_classes=200).to(device)\n",
    "load_pretrained_weights(model)\n",
    "\n",
    "embeddings = extract_patch_embeddings(model, train_loader, max_samples=10000)\n",
    "print(\"✅ Embedding shape for PCA:\", embeddings.shape)  # Should be [N, 768]\n",
    "\n",
    "pca = PCA(n_components=384)\n",
    "pca.fit(embeddings.numpy())\n",
    "\n",
    "new_weight, new_bias = apply_pca_to_conv_weights(model.patch_embed.proj, pca)\n",
    "model_384 = create_vit_384(new_weight, new_bias)\n",
    "evaluate_model(model_384, test_loader, nn.CrossEntropyLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pretrained ViT-Base loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:  10%|█         | 157/1556 [00:37<05:33,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding shape for PCA: torch.Size([1960000, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 4D",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[32m    188\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_384\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, criterion)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m    159\u001b[39m     inputs, targets = inputs.to(device), targets.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     loss = criterion(outputs, targets)\n\u001b[32m    162\u001b[39m     total_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/timm/models/vision_transformer.py:853\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    855\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/timm/models/vision_transformer.py:827\u001b[39m, in \u001b[36mVisionTransformer.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._pos_embed(x)\n\u001b[32m    829\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.patch_drop(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/timm/layers/patch_embed.py:131\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    129\u001b[39m     pad_w = (\u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m1\u001b[39m] - W % \u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m1\u001b[39m]) % \u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m1\u001b[39m]\n\u001b[32m    130\u001b[39m     x = F.pad(x, (\u001b[32m0\u001b[39m, pad_w, \u001b[32m0\u001b[39m, pad_h))\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.flatten:\n\u001b[32m    133\u001b[39m     x = x.flatten(\u001b[32m2\u001b[39m).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# NCHW -> NLC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nas_vision/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: t() expects a tensor with <= 2 dimensions, but self is 4D"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from timm import create_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- SETTINGS ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_embed_dim = 384\n",
    "num_classes = 200\n",
    "data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "# --- DATA LOADERS ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# --- PATCH EMBEDDING EXTRACTOR ---\n",
    "# def extract_patch_embeddings(model, dataloader, max_samples=10000):\n",
    "#     model.eval()\n",
    "#     collected = 0\n",
    "#     embeddings = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "#             if collected >= max_samples:\n",
    "#                 break\n",
    "#             images = images.to(device)\n",
    "#             x = model.patch_embed(images)  # (B, C, H, W)\n",
    "#             B, C, H, W = x.shape\n",
    "#             patches = x.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "#             embeddings.append(patches.cpu())\n",
    "#             collected += images.size(0)\n",
    "\n",
    "#     return torch.cat(embeddings, dim=0)[:max_samples * 196]  # shape: (N, 768)\n",
    "\n",
    "def extract_patch_embeddings(model, dataloader, max_samples=10000):\n",
    "    model.eval()\n",
    "    collected = 0\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            if collected >= max_samples:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            x = model.patch_embed(images)  # [B, N, C] directly\n",
    "            if x.ndim == 3:\n",
    "                B, N, C = x.shape\n",
    "                patches = x.reshape(-1, C)  # Flatten all patches\n",
    "                embeddings.append(patches.cpu())\n",
    "            collected += images.size(0)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)[:max_samples * 196]  # (N * 196, 768)\n",
    "\n",
    "# --- PCA FITTER ---\n",
    "def fit_pca(embeddings, target_dim):\n",
    "    print(f\"✅ Embedding shape for PCA: {embeddings.shape}\")\n",
    "    pca = PCA(n_components=target_dim)\n",
    "    pca.fit(embeddings.numpy())\n",
    "    return pca\n",
    "\n",
    "# --- PATCH EMBED WEIGHT TRANSFORMER ---\n",
    "def transform_patch_embed_weights(orig_conv: nn.Conv2d, pca: PCA):\n",
    "    old_weight = orig_conv.weight.data.view(768, -1).cpu().numpy()  # (768, 3*16*16)\n",
    "    new_weight = torch.tensor(pca.transform(old_weight)).view(target_embed_dim, 3, 16, 16)\n",
    "\n",
    "    old_bias = orig_conv.bias.data.cpu().numpy().reshape(1, -1)  # (1, 768)\n",
    "    new_bias = torch.tensor(pca.transform(old_bias)[0])\n",
    "\n",
    "    return new_weight, new_bias\n",
    "\n",
    "# --- NEW VIT MODEL WITH 384 EMBEDDING ---\n",
    "# def create_vit_384(pretrained_vit, pca):\n",
    "#     model_384 = create_model(\n",
    "#         'vit_base_patch16_224',\n",
    "#         pretrained=False,\n",
    "#         num_classes=num_classes,\n",
    "#         embed_dim=target_embed_dim\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Transform patch embedding weights\n",
    "#     new_weight, new_bias = transform_patch_embed_weights(pretrained_vit.patch_embed.proj, pca)\n",
    "#     model_384.patch_embed.proj = nn.Conv2d(3, target_embed_dim, kernel_size=16, stride=16)\n",
    "#     model_384.patch_embed.proj.weight.data.copy_(new_weight)\n",
    "#     model_384.patch_embed.proj.bias.data.copy_(new_bias)\n",
    "\n",
    "#     # Positional embedding PCA projection\n",
    "#     pos_embed = pretrained_vit.pos_embed.data[:, 1:]  # Exclude class token\n",
    "#     pos_embed_reduced = torch.tensor(pca.transform(pos_embed[0].cpu().numpy())).unsqueeze(0)\n",
    "#     cls_token = torch.tensor(pca.transform(pretrained_vit.cls_token.data.cpu().numpy())[0]).unsqueeze(0).unsqueeze(0)\n",
    "#     model_384.pos_embed = nn.Parameter(torch.cat([cls_token, pos_embed_reduced], dim=1).to(device))\n",
    "#     model_384.cls_token = nn.Parameter(cls_token.to(device))\n",
    "\n",
    "#     # Transfer transformer weights block-wise\n",
    "#     for i in range(len(model_384.blocks)):\n",
    "#         # Skip weights that depend on embed dim\n",
    "#         for name, param in pretrained_vit.blocks[i].named_parameters():\n",
    "#             if param.shape == getattr(model_384.blocks[i], name.split('.')[0]).__getattr__(name.split('.')[1]).shape:\n",
    "#                 getattr(model_384.blocks[i], name.split('.')[0]).__getattr__(name.split('.')[1]).data.copy_(param.data)\n",
    "\n",
    "#     # Normalize layer\n",
    "#     if model_384.norm.weight.shape == pretrained_vit.norm.weight.shape:\n",
    "#         model_384.norm.load_state_dict(pretrained_vit.norm.state_dict(), strict=False)\n",
    "\n",
    "#     return model_384\n",
    "\n",
    "def create_vit_384(pretrained_vit, pca):\n",
    "    model_384 = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "    model_384.patch_embed.proj = nn.Linear(16*16*3, 384)\n",
    "    model_384.pos_embed = nn.Parameter(torch.zeros(1, 197, 384))\n",
    "    model_384.cls_token = nn.Parameter(torch.zeros(1, 1, 384))\n",
    "    model_384.head = nn.Linear(384, 1000)\n",
    "\n",
    "    # Transform patch embedding weights\n",
    "    W = pretrained_vit.patch_embed.proj.weight.data.reshape(768, -1)  # (768, 16*16*3)\n",
    "    W_reduced = torch.tensor(pca.components_ @ W.cpu().numpy()).float()\n",
    "    model_384.patch_embed.proj.weight.data = W_reduced.reshape(384, 3, 16, 16).to(device)\n",
    "    model_384.patch_embed.proj.bias.data = pretrained_vit.patch_embed.proj.bias.data[:384].to(device)\n",
    "\n",
    "    # Transform positional embeddings\n",
    "    pos_embed = pretrained_vit.pos_embed.data[:, 1:, :]  # Exclude CLS token: shape [1, 196, 768]\n",
    "    pos_embed_np = pos_embed.cpu().numpy().reshape(-1, pos_embed.shape[-1])  # (196, 768)\n",
    "    pos_embed_reduced_np = pca.transform(pos_embed_np)  # (196, 384)\n",
    "    pos_embed_reduced = torch.tensor(pos_embed_reduced_np).reshape(1, pos_embed.shape[1], 384).to(device)\n",
    "\n",
    "    # Transform CLS token embedding\n",
    "    cls_token_np = pretrained_vit.cls_token.data.cpu().numpy().reshape(-1, pretrained_vit.cls_token.shape[-1])  # (1, 768)\n",
    "    cls_token_reduced_np = pca.transform(cls_token_np)  # (1, 384)\n",
    "    cls_token_reduced = torch.tensor(cls_token_reduced_np).unsqueeze(0).to(device)  # (1, 1, 384)\n",
    "\n",
    "    # Set embeddings\n",
    "    model_384.pos_embed.data = torch.cat([cls_token_reduced, pos_embed_reduced], dim=1)\n",
    "    model_384.cls_token.data = cls_token_reduced.squeeze(0)\n",
    "\n",
    "    # Copy transformer weights except patch embedding and head\n",
    "    model_384.blocks.load_state_dict(pretrained_vit.blocks.state_dict())\n",
    "    model_384.norm.load_state_dict(pretrained_vit.norm.state_dict())\n",
    "\n",
    "    return model_384\n",
    "\n",
    "\n",
    "# --- EVALUATION FUNCTION ---\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"📊 Eval - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Load pretrained ViT-Base\n",
    "    pretrained_vit = create_model('vit_base_patch16_224', pretrained=True)\n",
    "    pretrained_vit.to(device)\n",
    "    pretrained_vit.eval()\n",
    "    print(\"✅ Pretrained ViT-Base loaded.\")\n",
    "\n",
    "    # Extract patch embeddings and fit PCA\n",
    "    embeddings = extract_patch_embeddings(pretrained_vit, train_loader, max_samples=10000)\n",
    "    pca = fit_pca(embeddings, target_embed_dim)\n",
    "\n",
    "    # Create new ViT model with 384-dim embedding\n",
    "    model_384 = create_vit_384(pretrained_vit, pca)\n",
    "\n",
    "    # Final evaluation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    evaluate_model(model_384, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new model with updated embedding dimension\n",
    "# model_384 = DynamicViT(\n",
    "#     img_size=224,\n",
    "#     patch_size=16,\n",
    "#     embed_dim=384,\n",
    "#     depth=12,\n",
    "#     num_heads=6,  # must divide 384 evenly\n",
    "#     mlp_ratio=4.0,\n",
    "#     num_classes=200\n",
    "# ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, dataloader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in dataloader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             total_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += targets.size(0)\n",
    "#             correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#     avg_loss = total_loss / total\n",
    "#     accuracy = 100. * correct / total\n",
    "#     print(f\"Evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "#     return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Loss: 5.4482, Accuracy: 0.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.448154718459745, 0.6428801028608164)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define criterion\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Call evaluation function\n",
    "# evaluate_model(model_384, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evolutioonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "SAVE_PATH = '/home/pratibha/nas_vision/weights-img-evol28-may'\n",
    "\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=200\n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(1,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top5]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(1,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top5_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-5 Acc: {top5_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top5_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([ 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-5 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=200).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=200).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-5 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
