{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms (resize all images to 224x224)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "# ])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "# Path to your ImageNet data\n",
    "# data_dir = '/home/pratibha/nas_vision/vit_nas_imgnet/imagenet200'\n",
    "\n",
    "## imagenet 1k dataset\n",
    "train_dir = '/SN02DATA/vit/train_val_dataset/train'\n",
    "test_dir = '/SN02DATA/vit/train_val_dataset/val'\n",
    "\n",
    "# Load ImageNet dataset and filter only the first 200 classes\n",
    "# filtered_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# Use only the first 200 classes\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1024892\n",
      "Test set size: 256275\n"
     ]
    }
   ],
   "source": [
    "# train_size = int(0.8 * len(filtered_dataset))\n",
    "# test_size = len(filtered_dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(filtered_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the number of samples in each set\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dynamic vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just defining model again here for easily avaliability\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "\n",
    "class DynamicMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = (embed_dim // num_heads) ** -0.5\n",
    "        self.num_heads = num_heads  # Store num_heads as a class attribute\n",
    "\n",
    "        # Ensure that the number of heads divides the embedding dimension\n",
    "        assert embed_dim % num_heads == 0, f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "class MLPBlock(nn.Module):  \n",
    "    def __init__(self, embed_dim, mlp_ratio):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # Matches `mlp.fc1`\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # Matches `mlp.fc2`\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DynamicTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = DynamicMultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        #  Fix: Wrap MLP inside a separate module to match ViT\n",
    "        self.mlp = MLPBlock(embed_dim, mlp_ratio)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# class DynamicViT(nn.Module):\n",
    "#     def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "#         #  Fix: Correct positional embedding key\n",
    "#         self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "#         self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "#         self.norm = nn.LayerNorm(embed_dim)\n",
    "#         self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         B = x.shape[0]\n",
    "\n",
    "#         # Add class token\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "#         x = x + self.pos_embed\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "\n",
    "#         x = self.norm(x[:, 0])\n",
    "#         return self.head(x)\n",
    "\n",
    "\n",
    "class DynamicViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.depth = depth  # Store depth as an instance variable\n",
    "        self.num_heads = num_heads  # Store num_heads as an instance variable\n",
    "        self.mlp_ratio = mlp_ratio  # Store mlp_ratio as an instance variable\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.patch_embed = DynamicPatchEmbed(img_size, patch_size, embed_dim)\n",
    "        \n",
    "        # Fix: Correct positional embedding key\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([DynamicTransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evol algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top 1 nad top 5 accuracy not used and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([4, 6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=1000 \n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Evaluate architecture: accuracy, latency, and memory usage\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start measuring inference latency\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Measure total time for inference (latency)\n",
    "    latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute memory usage (rough estimation)\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "    # Compute average loss\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "# Fine-tune model on dataset (train for a few epochs)\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "    # Save the model after fine-tuning\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=1000).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "            accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        # Saving top-ranked models\n",
    "        save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=16, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# Call the algorithm\n",
    "# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptflops in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (0.7.4)\n",
      "Requirement already satisfied: torch>=2.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from ptflops) (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from torch>=2.0->ptflops) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=4, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 20,421,352\n",
      "Sampled architecture: Depth=12, Num Heads=16, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 58,237,672\n",
      "Sampled architecture: Depth=6, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,875,432\n",
      "Sampled architecture: Depth=10, Num Heads=4, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 48,783,592\n",
      "Sampled architecture: Depth=4, Num Heads=4, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,864,680\n",
      "Sampled architecture: Depth=12, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 114,897,640\n",
      "Sampled architecture: Depth=4, Num Heads=12, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 20,421,352\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 72,391,912\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 96,000,232\n",
      "Sampled architecture: Depth=8, Num Heads=16, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 39,329,512\n",
      "Sampled architecture: Depth=4, Num Heads=16, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 39,308,008\n",
      "Sampled architecture: Depth=10, Num Heads=4, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 96,000,232\n",
      "Sampled architecture: Depth=8, Num Heads=4, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 77,102,824\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 48,783,592\n",
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,875,432\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 39,329,512\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=4, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 3.1228, Top-1 Acc: 33.78%, Top-5 Acc: 58.54%, Latency: 0.005734s/img, Mem: 77.90MB\n",
      "MACs: 3842.64 M\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 495\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m population\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# Run the evolutionary algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[43mevolutionary_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Call the algorithm\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\u001b[39;00m\n\u001b[32m    499\u001b[39m \n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# Run the evolutionary algorithm\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 430\u001b[39m, in \u001b[36mevolutionary_algorithm\u001b[39m\u001b[34m(population_size, generations, mutation_rate, crossover_rate, train_loader, test_loader)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    428\u001b[39m     load_pretrained_weights(model)\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43marchitecture_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n\u001b[32m    433\u001b[39m memory = count_parameters(model) * \u001b[32m4\u001b[39m / (\u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# MB\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mfine_tune_model\u001b[39m\u001b[34m(sampled_model, train_loader, test_loader, epochs, architecture_folder)\u001b[39m\n\u001b[32m    332\u001b[39m         running_loss += loss.item()\n\u001b[32m    333\u001b[39m     epoch_time = time.time() - start_epoch\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Top-1 Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Top-5 Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_top5\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Latency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_latency\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms/img, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Save model code unchanged\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 5)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#################################33 too many valuse to unpack error\n",
    "# import random\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import Adam\n",
    "# from timm import create_model\n",
    "# import time\n",
    "\n",
    "# # Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "# # SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# # Set the device (GPU if available, else CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # First-time loading pretrained weights for initialization\n",
    "# def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "#     pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     # Match keys between pretrained and current model\n",
    "#     model_state_dict = model.state_dict()\n",
    "#     filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "#     # Load pretrained weights\n",
    "#     model.load_state_dict(filtered_dict, strict=False)\n",
    "#     print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# # Check if pretrained weights are loaded correctly\n",
    "# def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "#     pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "#     pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "#     model_state_dict = model.state_dict()\n",
    "#     matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "#     if len(matching_keys) > 0:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "#     else:\n",
    "#         print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# # Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "# def sample_subnetwork(seen_architectures):\n",
    "#     while True:\n",
    "#         depth = random.choice([4, 6, 8, 10, 12])\n",
    "#         num_heads = random.choice([4, 8, 12, 16])\n",
    "#         mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#         embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "#         architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "#         # Skip if architecture has already been sampled\n",
    "#         if architecture not in seen_architectures:\n",
    "#             seen_architectures.add(architecture)\n",
    "#             print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "#             # Create the model to calculate its number of parameters\n",
    "#             # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "#             sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                         depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "#                                         num_classes=1000 \n",
    "#                                     )\n",
    "#             num_params = count_parameters(sampled_model)\n",
    "#             print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "#             return architecture\n",
    "#         else:\n",
    "#             print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# # Count number of trainable parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# def topk_accuracy(output, target, topk=(1,5)):\n",
    "#     \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "#     maxk = max(topk)\n",
    "#     batch_size = target.size(0)\n",
    "#     _, pred = output.topk(maxk, 1, True, True)\n",
    "#     pred = pred.t()\n",
    "#     correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "#     res = []\n",
    "#     for k in topk:\n",
    "#         correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "#         res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "#     return res  # [top1, top5]\n",
    "\n",
    "\n",
    "# # Evaluate architecture: accuracy, latency, and memory usage\n",
    "# # def evaluate_architecture(model, test_loader):\n",
    "# #     model.eval()\n",
    "# #     correct = 0\n",
    "# #     total = 0\n",
    "# #     running_loss = 0.0\n",
    "# #     y_true = []\n",
    "# #     y_pred = []\n",
    "    \n",
    "# #     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# #     # Start measuring inference latency\n",
    "# #     start_time = time.time()\n",
    "\n",
    "# #     with torch.no_grad():\n",
    "# #         for images, labels in test_loader:\n",
    "# #             images, labels = images.to(device), labels.to(device)  # Move to the same device\n",
    "# #             outputs = model(images)\n",
    "# #             loss = criterion(outputs, labels)\n",
    "# #             running_loss += loss.item()\n",
    "\n",
    "# #             _, predicted = torch.max(outputs, 1)\n",
    "# #             total += labels.size(0)\n",
    "# #             correct += (predicted == labels).sum().item()\n",
    "\n",
    "# #             y_true.extend(labels.cpu().numpy())\n",
    "# #             y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# #     # Measure total time for inference (latency)\n",
    "# #     latency = (time.time() - start_time) / len(test_loader.dataset)\n",
    "\n",
    "# #     # Compute accuracy\n",
    "# #     accuracy = 100 * correct / total\n",
    "\n",
    "# #     # Compute memory usage (rough estimation)\n",
    "# #     num_params = count_parameters(model)\n",
    "# #     memory_usage = (num_params * 4) / (1024 ** 2)  # Convert bytes to MB (FP32)\n",
    "\n",
    "# #     # Compute average loss\n",
    "# #     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "# #     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Latency: {latency:.6f} seconds/image, Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# #     return accuracy, test_loss, latency, memory_usage\n",
    "\n",
    "# from ptflops import get_model_complexity_info\n",
    "\n",
    "# def get_macs(model):\n",
    "#     with torch.cuda.device(0):\n",
    "#         macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "#     return macs\n",
    "\n",
    "# def evaluate_architecture(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     running_loss = 0.0\n",
    "#     top1_total = 0\n",
    "#     top5_total = 0\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "#             top1, top5 = topk_accuracy(outputs, labels, topk=(1,5))\n",
    "#             top1_total += top1 * labels.size(0) / 100.0\n",
    "#             top5_total += top5 * labels.size(0) / 100.0\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     latency = (time.time() - start_time) / total\n",
    "#     accuracy = 100 * top1_total / total\n",
    "#     top5_accuracy = 100 * top5_total / total\n",
    "#     num_params = count_parameters(model)\n",
    "#     memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "#     test_loss = running_loss / len(test_loader)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-5 Acc: {top5_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "#     macs = get_macs(model)\n",
    "#     print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "#     return accuracy, top5_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Estimate memory usage of a model during inference (rough estimation)\n",
    "# def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "#     # Create dummy input matching the expected shape of the input tensor\n",
    "#     dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "#     # Use torch.utils.benchmark to measure memory usage during inference\n",
    "#     start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "#     # Run the model once with the dummy input\n",
    "#     with torch.no_grad():\n",
    "#         model(dummy_input)\n",
    "    \n",
    "#     end_mem = torch.cuda.memory_allocated()\n",
    "#     memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "#     return memory_usage\n",
    "\n",
    "\n",
    "# def calculate_crowding_distance(population, test_loader):\n",
    "#     crowding_distances = [0] * len(population)\n",
    "#     num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "#     # Evaluate each architecture once, then reuse the results\n",
    "#     evaluated_results = []\n",
    "#     for arch in population:\n",
    "#         # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "#         #                    depth=arch[0], num_heads=arch[1],\n",
    "#         #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "#                             depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "#                             num_classes=1000).to(device)\n",
    "\n",
    "#         accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#         memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "#         evaluated_results.append((accuracy, latency, memory))\n",
    "#         del model\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     for objective_index in range(num_objectives):\n",
    "#         sorted_indices = sorted(range(len(population)),\n",
    "#                                 key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "#         crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "#         for i in range(1, len(sorted_indices) - 1):\n",
    "#             prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "#             next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "#             distance = next_value - prev_value\n",
    "#             crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "#     return crowding_distances\n",
    "\n",
    "\n",
    "# def dominates(model1, model2, test_loader):\n",
    "#     # Evaluate both models on the test set\n",
    "#     accuracy1, latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "#     accuracy2, latency2, _, _ = evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "#     # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "#     memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "#     memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "#     # Compare performance metrics\n",
    "#     dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "#     dominates_in_latency = latency1 <= latency2\n",
    "#     dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "#     # Return True if model1 dominates model2 in all aspects\n",
    "#     return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# # Mutation: Randomly mutate architecture's hyperparameters\n",
    "# def mutate(architecture):\n",
    "#     depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "#     if random.random() < 0.5: depth = random.choice([4, 6, 8, 10, 12])\n",
    "#     if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "#     if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "#     print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# # One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "# ############################# this is not weight based instead it is pareto selection\n",
    "# # Optimized Pareto selection based on stored performance metrics\n",
    "# def pareto_selection(arch_performance):\n",
    "#     def dominates(perf1, perf2):\n",
    "#         acc1, lat1, mem1 = perf1\n",
    "#         acc2, lat2, mem2 = perf2\n",
    "#         return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "#     ranks = {}\n",
    "#     for arch1, perf1 in arch_performance.items():\n",
    "#         dominated_count = 0\n",
    "#         for arch2, perf2 in arch_performance.items():\n",
    "#             if arch1 != arch2 and dominates(perf2, perf1):\n",
    "#                 dominated_count += 1\n",
    "#         ranks[arch1] = dominated_count\n",
    "\n",
    "#     # Sort architectures by rank (lower dominated_count = better)\n",
    "#     sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "#     return sorted_population\n",
    "\n",
    "# # Fine-tune model on dataset (train for a few epochs)\n",
    "# # def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "# #     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "# #     sampled_model.to(device)  # Ensure the model is on the correct device\n",
    "# #     criterion = nn.CrossEntropyLoss()\n",
    "# #     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "# #     for epoch in range(epochs):\n",
    "# #         sampled_model.train()\n",
    "# #         running_loss = 0.0\n",
    "# #         for images, labels in train_loader:\n",
    "# #             images, labels = images.to(device), labels.to(device)  # Ensure inputs are on the same device\n",
    "# #             optimizer.zero_grad()\n",
    "# #             outputs = sampled_model(images)\n",
    "# #             loss = criterion(outputs, labels)\n",
    "# #             loss.backward()\n",
    "# #             optimizer.step()\n",
    "# #             running_loss += loss.item()\n",
    "        \n",
    "# #         test_accuracy, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "# #         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%, Test Latency: {test_latency:.6f} seconds/image\")\n",
    "\n",
    "# #     # Save the model after fine-tuning\n",
    "# #     if architecture_folder:\n",
    "# #         os.makedirs(architecture_folder, exist_ok=True)\n",
    "# #         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "# #     return sampled_model\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_top_ranked_models(population, arch_performance, generation):\n",
    "#     top_n = min(5, len(population))\n",
    "#     for idx, arch in enumerate(population[:top_n]):\n",
    "#         depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#         # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "#         #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "#         model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "#                             num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "#                             num_classes=1000).to(device)\n",
    "\n",
    "\n",
    "#         architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "#         checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "#         model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "#         top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "#         torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "#         acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "#         # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "#         #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "#         #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "#         with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "#             f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "#             f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "#         print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# def plot_pareto_front(arch_performance):\n",
    "#     accuracies = [v[0] for v in arch_performance.values()]\n",
    "#     latencies = [v[1] for v in arch_performance.values()]\n",
    "#     memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "#     # Accuracy vs Latency\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.scatter(latencies, accuracies, c='blue')\n",
    "#     plt.xlabel('Latency (s/image)')\n",
    "#     plt.ylabel('Accuracy (%)')\n",
    "#     plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Accuracy vs Memory\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     plt.scatter(memories, accuracies, c='green')\n",
    "#     plt.xlabel('Memory (MB)')\n",
    "#     plt.ylabel('Accuracy (%)')\n",
    "#     plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# #\n",
    "\n",
    "# def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "#     seen_architectures = set()\n",
    "#     population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "#     arch_performance = {}\n",
    "\n",
    "#     prev_best_accuracy = 0\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     for generation in range(generations):\n",
    "#         print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "#         for arch in population:\n",
    "#             depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "#             architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "#             checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "#             model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "#                                depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "#                                num_classes=1000).to(device)\n",
    "\n",
    "#             # Clearly load weights once per architecture\n",
    "#             if os.path.exists(checkpoint_path):\n",
    "#                 model.load_state_dict(torch.load(checkpoint_path))\n",
    "#                 print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "#             else:\n",
    "#                 load_pretrained_weights(model)\n",
    "\n",
    "#             fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "#             accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "#             memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "#             arch_performance[arch] = (accuracy, latency, memory)\n",
    "\n",
    "#             del model\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         # Pareto selection\n",
    "#         population = pareto_selection(arch_performance)\n",
    "\n",
    "#         print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "#         for idx, arch in enumerate(population[:5]):\n",
    "#             acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "#             print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-5 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "#             # Saving top-ranked models\n",
    "#             save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "#         # Check for Pareto front convergence (early stopping criteria)\n",
    "#         current_best_accuracy = arch_performance[population[0]][0]\n",
    "#         if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "#             no_improvement_count += 1\n",
    "#             print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "#             if no_improvement_count >= 2:\n",
    "#                 print(\"Pareto front has converged. Stopping early.\")\n",
    "#                 break\n",
    "#         else:\n",
    "#             no_improvement_count = 0\n",
    "#         prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "#         # Generate offspring\n",
    "#         next_population = population[:len(population)//2]  # Only top half\n",
    "#         offspring = []\n",
    "\n",
    "#         for i in range(0, len(next_population)-1, 2):\n",
    "#             parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "#             if random.random() < crossover_rate:\n",
    "#                 child1, child2 = one_point_crossover(parent1, parent2)\n",
    "#                 print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "#                 offspring.extend([child1, child2])\n",
    "#             else:\n",
    "#                 offspring.extend([parent1, parent2])\n",
    "\n",
    "#         # Mutation with clear logging\n",
    "#         mutated_offspring = []\n",
    "#         for child in offspring:\n",
    "#             if random.random() < mutation_rate:\n",
    "#                 original_child = child\n",
    "#                 child = mutate(child)\n",
    "#                 print(f\"Mutated from {original_child} to {child}\")\n",
    "#             mutated_offspring.append(child)\n",
    "\n",
    "#         population = next_population + mutated_offspring\n",
    "\n",
    "#         print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "#         print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "#     # Plot Pareto Front at the end\n",
    "#     plot_pareto_front(arch_performance)\n",
    "\n",
    "#     return population\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "# evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# # Call the algorithm\n",
    "# # evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n",
    "# # Run the evolutionary algorithm\n",
    "# # evolutionary_algorithm(population_size=5, generations=2, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 1 and top 5 accuracy and embed dim only 768 and 1k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratibha/miniconda3/envs/nas_vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 39,329,512\n",
      "Sampled architecture: Depth=12, Num Heads=12, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 58,237,672\n",
      "Sampled architecture: Depth=12, Num Heads=16, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 86,567,656\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 114,897,640\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 86,567,656\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 58,205,416\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=6.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 96,000,232\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=6, Num Heads=4, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 44,040,424\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=2.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 29,875,432\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=4.0, Embed Dim=768\n",
      "Number of parameters in the sampled model: 58,216,168\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=8, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 2.6268, Top-1 Acc: 42.14%, Top-5 Acc: 67.21%, Latency: 0.004929s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 3.5435\n",
      "| Test Loss: 2.6268\n",
      "| Top-1 Accuracy: 42.14%\n",
      "| Top-5 Accuracy: 67.21%\n",
      "| Latency: 0.004929s/img\n",
      "| Memory Usage: 150.03MB\n",
      "| MACs: 7568.76M\n",
      "| Epoch Time: 19694.27s\n",
      "\n",
      "Test Loss: 2.1133, Top-1 Acc: 51.47%, Top-5 Acc: 75.83%, Latency: 0.004956s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 2.2344\n",
      "| Test Loss: 2.1133\n",
      "| Top-1 Accuracy: 51.47%\n",
      "| Top-5 Accuracy: 75.83%\n",
      "| Latency: 0.004956s/img\n",
      "| Memory Usage: 150.03MB\n",
      "| MACs: 7568.76M\n",
      "| Epoch Time: 12428.13s\n",
      "\n",
      "Test Loss: 1.9362, Top-1 Acc: 55.02%, Top-5 Acc: 78.79%, Latency: 0.004955s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 1.7318\n",
      "| Test Loss: 1.9362\n",
      "| Top-1 Accuracy: 55.02%\n",
      "| Top-5 Accuracy: 78.79%\n",
      "| Latency: 0.004955s/img\n",
      "| Memory Usage: 150.03MB\n",
      "| MACs: 7568.76M\n",
      "| Epoch Time: 12426.27s\n",
      "\n",
      "Test Loss: 1.8644, Top-1 Acc: 56.84%, Top-5 Acc: 80.15%, Latency: 0.004920s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 1.3911\n",
      "| Test Loss: 1.8644\n",
      "| Top-1 Accuracy: 56.84%\n",
      "| Top-5 Accuracy: 80.15%\n",
      "| Latency: 0.004920s/img\n",
      "| Memory Usage: 150.03MB\n",
      "| MACs: 7568.76M\n",
      "| Epoch Time: 12190.91s\n",
      "\n",
      "Test Loss: 1.8977, Top-1 Acc: 57.09%, Top-5 Acc: 80.20%, Latency: 0.004914s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 1.1100\n",
      "| Test Loss: 1.8977\n",
      "| Top-1 Accuracy: 57.09%\n",
      "| Top-5 Accuracy: 80.20%\n",
      "| Latency: 0.004914s/img\n",
      "| Memory Usage: 150.03MB\n",
      "| MACs: 7568.76M\n",
      "| Epoch Time: 11181.33s\n",
      "\n",
      "Test Loss: 1.8977, Top-1 Acc: 57.09%, Top-5 Acc: 80.20%, Latency: 0.004784s/img, Mem: 150.03MB\n",
      "MACs: 7568.76 M\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=12, MLP Ratio=2.0\n",
      "Test Loss: 2.0329, Top-1 Acc: 52.64%, Top-5 Acc: 77.25%, Latency: 0.005154s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "\n",
      "Epoch 1/5 Summary:\n",
      "| Training Loss: 2.9529\n",
      "| Test Loss: 2.0329\n",
      "| Top-1 Accuracy: 52.64%\n",
      "| Top-5 Accuracy: 77.25%\n",
      "| Latency: 0.005154s/img\n",
      "| Memory Usage: 222.16MB\n",
      "| MACs: 11294.88M\n",
      "| Epoch Time: 11906.83s\n",
      "\n",
      "Test Loss: 1.6878, Top-1 Acc: 59.47%, Top-5 Acc: 82.49%, Latency: 0.006877s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "\n",
      "Epoch 2/5 Summary:\n",
      "| Training Loss: 1.6767\n",
      "| Test Loss: 1.6878\n",
      "| Top-1 Accuracy: 59.47%\n",
      "| Top-5 Accuracy: 82.49%\n",
      "| Latency: 0.006877s/img\n",
      "| Memory Usage: 222.16MB\n",
      "| MACs: 11294.88M\n",
      "| Epoch Time: 13259.62s\n",
      "\n",
      "Test Loss: 1.5743, Top-1 Acc: 61.97%, Top-5 Acc: 84.26%, Latency: 0.007022s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "\n",
      "Epoch 3/5 Summary:\n",
      "| Training Loss: 1.2743\n",
      "| Test Loss: 1.5743\n",
      "| Top-1 Accuracy: 61.97%\n",
      "| Top-5 Accuracy: 84.26%\n",
      "| Latency: 0.007022s/img\n",
      "| Memory Usage: 222.16MB\n",
      "| MACs: 11294.88M\n",
      "| Epoch Time: 14193.39s\n",
      "\n",
      "Test Loss: 1.5462, Top-1 Acc: 63.40%, Top-5 Acc: 84.94%, Latency: 0.006859s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "\n",
      "Epoch 4/5 Summary:\n",
      "| Training Loss: 0.9903\n",
      "| Test Loss: 1.5462\n",
      "| Top-1 Accuracy: 63.40%\n",
      "| Top-5 Accuracy: 84.94%\n",
      "| Latency: 0.006859s/img\n",
      "| Memory Usage: 222.16MB\n",
      "| MACs: 11294.88M\n",
      "| Epoch Time: 15060.46s\n",
      "\n",
      "Test Loss: 1.5938, Top-1 Acc: 63.40%, Top-5 Acc: 85.06%, Latency: 0.007008s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "\n",
      "Epoch 5/5 Summary:\n",
      "| Training Loss: 0.7606\n",
      "| Test Loss: 1.5938\n",
      "| Top-1 Accuracy: 63.40%\n",
      "| Top-5 Accuracy: 85.06%\n",
      "| Latency: 0.007008s/img\n",
      "| Memory Usage: 222.16MB\n",
      "| MACs: 11294.88M\n",
      "| Epoch Time: 13888.08s\n",
      "\n",
      "Test Loss: 1.5938, Top-1 Acc: 63.40%, Top-5 Acc: 85.06%, Latency: 0.006827s/img, Mem: 222.16MB\n",
      "MACs: 11294.88 M\n",
      "Pretrained weights loaded into DynamicViT successfully.\n",
      "Fine-tuning model with architecture: Depth=12, Num Heads=16, MLP Ratio=4.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First-time loading pretrained weights for initialization\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Match keys between pretrained and current model\n",
    "    model_state_dict = model.state_dict()\n",
    "    filtered_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "\n",
    "    # Load pretrained weights\n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Pretrained weights loaded into {model.__class__.__name__} successfully.\")\n",
    "\n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([4, 8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        embed_dim = 768  # Fixed embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=1000 \n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(1,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top5]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(1,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top5_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-5 Acc: {top5_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top5_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "# Mutation: Randomly mutate architecture's hyperparameters\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    if random.random() < 0.5: depth = random.choice([ 6, 8, 10, 12])\n",
    "    if random.random() < 0.5: num_heads = random.choice([4, 8, 12, 16])\n",
    "    if random.random() < 0.5: mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "    print(f\"Mutated architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "    return depth, num_heads, mlp_ratio, embed_dim\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-5 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=1000).to(device)\n",
    "\n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-5 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=10, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 7,860,328\n",
      "Sampled architecture: Depth=6, Num Heads=8, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 23,107,720\n",
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 23,107,720\n",
      "Sampled architecture: Depth=10, Num Heads=16, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 28,656,520\n",
      "Sampled architecture: Depth=8, Num Heads=12, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 23,114,440\n",
      "Sampled architecture: Depth=10, Num Heads=8, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 37,882,120\n",
      "Sampled architecture: Depth=8, Num Heads=16, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 30,494,920\n",
      "Sampled architecture: Depth=8, Num Heads=12, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 15,733,960\n",
      "Sampled architecture: Depth=8, Num Heads=8, MLP Ratio=2.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 10,228,072\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 34,198,600\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=2.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 23,127,880\n",
      "Repeated architecture found, resampling...\n",
      "Sampled architecture: Depth=12, Num Heads=8, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 29,137,768\n",
      "Sampled architecture: Depth=6, Num Heads=16, MLP Ratio=6.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 23,107,720\n",
      "Sampled architecture: Depth=6, Num Heads=12, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 14,947,432\n",
      "Sampled architecture: Depth=10, Num Heads=12, MLP Ratio=6.0, Embed Dim=384\n",
      "Number of parameters in the sampled model: 24,407,656\n",
      "Sampled architecture: Depth=10, Num Heads=8, MLP Ratio=4.0, Embed Dim=480\n",
      "Number of parameters in the sampled model: 28,656,520\n",
      "\n",
      "--- Generation 1/5 ---\n",
      "Loaded pretrained weights with adaptation projection\n",
      "Fine-tuning model with architecture: Depth=6, Num Heads=8, MLP Ratio=2.0\n",
      "Test Loss: 4.3193, Top-1 Acc: 29.86%, Top-3 Acc: 36.83%, Latency: 0.005267s/img, Mem: 29.98MB\n",
      "MACs: 1458.51 M\n",
      "\n",
      "Epoch 1/16 Summary:\n",
      "| Training Loss: 5.1720\n",
      "| Test Loss: 4.3193\n",
      "| Top-1 Accuracy: 29.86%\n",
      "| Top-3 Accuracy: 36.83%\n",
      "| Latency: 0.005267s/img\n",
      "| Memory Usage: 29.98MB\n",
      "| MACs: 1458.51M\n",
      "| Epoch Time: 38567.18s\n",
      "\n",
      "Test Loss: 3.5358, Top-1 Acc: 43.49%, Top-3 Acc: 51.22%, Latency: 0.005538s/img, Mem: 29.98MB\n",
      "MACs: 1458.51 M\n",
      "\n",
      "Epoch 2/16 Summary:\n",
      "| Training Loss: 3.8711\n",
      "| Test Loss: 3.5358\n",
      "| Top-1 Accuracy: 43.49%\n",
      "| Top-3 Accuracy: 51.22%\n",
      "| Latency: 0.005538s/img\n",
      "| Memory Usage: 29.98MB\n",
      "| MACs: 1458.51M\n",
      "| Epoch Time: 14851.22s\n",
      "\n",
      "Test Loss: 3.1931, Top-1 Acc: 49.73%, Top-3 Acc: 57.36%, Latency: 0.005236s/img, Mem: 29.98MB\n",
      "MACs: 1458.51 M\n",
      "\n",
      "Epoch 3/16 Summary:\n",
      "| Training Loss: 3.2316\n",
      "| Test Loss: 3.1931\n",
      "| Top-1 Accuracy: 49.73%\n",
      "| Top-3 Accuracy: 57.36%\n",
      "| Latency: 0.005236s/img\n",
      "| Memory Usage: 29.98MB\n",
      "| MACs: 1458.51M\n",
      "| Epoch Time: 76304.42s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from timm import create_model\n",
    "import time\n",
    "\n",
    "# Path to save the models after fine-tuning\n",
    "# SAVE_PATH = '/SN02DATA/nas_vision/evol_img1k-wts'\n",
    "SAVE_PATH = '/SN02DATA/nas_vision/evol_img1knew-wts'\n",
    "\n",
    "# SAVE_PATH = '/kaggle/working/'\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_model_name=\"vit_base_patch16_224\"):\n",
    "    pretrained_vit = create_model(pretrained_model_name, pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    # Get embedding dimensions\n",
    "    pretrained_embed_dim = pretrained_state_dict['pos_embed'].shape[-1]\n",
    "    current_embed_dim = model.embed_dim\n",
    "    \n",
    "    # Create adaptation modules\n",
    "    adaptation_modules = nn.ModuleDict()\n",
    "    if pretrained_embed_dim != current_embed_dim:\n",
    "        adaptation_modules['pos_embed_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "        adaptation_modules['cls_token_proj'] = nn.Linear(pretrained_embed_dim, current_embed_dim)\n",
    "    \n",
    "    # Project pretrained weights\n",
    "    filtered_dict = {}\n",
    "    for k, v in pretrained_state_dict.items():\n",
    "        if k == 'pos_embed' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['pos_embed_proj'](v)\n",
    "        elif k == 'cls_token' and pretrained_embed_dim != current_embed_dim:\n",
    "            filtered_dict[k] = adaptation_modules['cls_token_proj'](v)\n",
    "        elif k in model.state_dict() and v.shape == model.state_dict()[k].shape:  # FIXED HERE\n",
    "            filtered_dict[k] = v\n",
    "    \n",
    "    model.load_state_dict(filtered_dict, strict=False)\n",
    "    print(f\"Loaded pretrained weights with {'adaptation' if pretrained_embed_dim != current_embed_dim else 'no'} projection\")\n",
    "    \n",
    "# Check if pretrained weights are loaded correctly\n",
    "def check_pretrained_weights(model, generation=0, model_type=\"subnetwork\"):\n",
    "    pretrained_vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "    pretrained_state_dict = pretrained_vit.state_dict()\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matching_keys = {k: v for k, v in pretrained_state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}\n",
    "    \n",
    "    if len(matching_keys) > 0:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has loaded {len(matching_keys)} layers from pretrained weights.\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1}: {model_type} model has NOT loaded any pretrained weights.\")\n",
    "\n",
    "# Sample Subnetwork - Randomly sample hyperparameters (depth, num_heads, etc.)\n",
    "def sample_subnetwork(seen_architectures):\n",
    "    while True:\n",
    "        depth = random.choice([6, 8, 10, 12])\n",
    "        num_heads = random.choice([8, 12, 16])\n",
    "        mlp_ratio = random.choice([2.0, 4.0, 6.0])\n",
    "        # embed_dim = 768  # Fixed embedding dimension\n",
    "        embed_dim = random.choice([384, 480])  # Variable embedding dimension\n",
    "        \n",
    "        architecture = (depth, num_heads, mlp_ratio, embed_dim)\n",
    "        \n",
    "        # Skip if architecture has already been sampled\n",
    "        if architecture not in seen_architectures:\n",
    "            seen_architectures.add(architecture)\n",
    "            print(f\"Sampled architecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\")\n",
    "            \n",
    "            # Create the model to calculate its number of parameters\n",
    "            # sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000)\n",
    "            sampled_model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                                        num_classes=1000\n",
    "                                    )\n",
    "            num_params = count_parameters(sampled_model)\n",
    "            print(f\"Number of parameters in the sampled model: {num_params:,}\")\n",
    "            \n",
    "            return architecture\n",
    "        else:\n",
    "            print(f\"Repeated architecture found, resampling...\")\n",
    "\n",
    "# Count number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def topk_accuracy(output, target, topk=(3,5)):\n",
    "    \"\"\"Computes the top-k accuracy for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "    return res  # [top1, top3]\n",
    "\n",
    "\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "def get_macs(model):\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model, (3, 224, 224), as_strings=False, print_per_layer_stat=False)\n",
    "    return macs\n",
    "\n",
    "def evaluate_architecture(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            top1, top5 = topk_accuracy(outputs, labels, topk=(3,5))\n",
    "            top1_total += top1 * labels.size(0) / 100.0\n",
    "            top5_total += top5 * labels.size(0) / 100.0\n",
    "            total += labels.size(0)\n",
    "\n",
    "    latency = (time.time() - start_time) / total\n",
    "    accuracy = 100 * top1_total / total\n",
    "    top3_accuracy = 100 * top5_total / total\n",
    "    num_params = count_parameters(model)\n",
    "    memory_usage = (num_params * 4) / (1024 ** 2)\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Top-1 Acc: {accuracy:.2f}%, Top-3 Acc: {top3_accuracy:.2f}%, Latency: {latency:.6f}s/img, Mem: {memory_usage:.2f}MB\")\n",
    "    macs = get_macs(model)\n",
    "    print(f\"MACs: {macs / 1e6:.2f} M\")\n",
    "    return accuracy, top3_accuracy, test_loss, latency, memory_usage, macs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estimate memory usage of a model during inference (rough estimation)\n",
    "def estimate_memory_usage(model):                                             ## this funtion is not needed\n",
    "    # Create dummy input matching the expected shape of the input tensor\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example for ViT (3-channel image of size 224x224)\n",
    "    \n",
    "    # Use torch.utils.benchmark to measure memory usage during inference\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Run the model once with the dummy input\n",
    "    with torch.no_grad():\n",
    "        model(dummy_input)\n",
    "    \n",
    "    end_mem = torch.cuda.memory_allocated()\n",
    "    memory_usage = (end_mem - start_mem) / (1024 ** 2)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "\n",
    "def calculate_crowding_distance(population, test_loader):\n",
    "    crowding_distances = [0] * len(population)\n",
    "    num_objectives = 3  # Accuracy, Latency, Memory\n",
    "\n",
    "    # Evaluate each architecture once, then reuse the results\n",
    "    evaluated_results = []\n",
    "    for arch in population:\n",
    "        # # model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "        #                    depth=arch[0], num_heads=arch[1],\n",
    "        #                    mlp_ratio=arch[2], num_classes=10).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=arch[3],\n",
    "                            depth=arch[0], num_heads=arch[1], mlp_ratio=arch[2], \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "        accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "        memory = count_parameters(model) * 4  # memory in bytes\n",
    "        \n",
    "        evaluated_results.append((accuracy, latency, memory))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for objective_index in range(num_objectives):\n",
    "        sorted_indices = sorted(range(len(population)),\n",
    "                                key=lambda idx: evaluated_results[idx][objective_index])\n",
    "        \n",
    "        crowding_distances[sorted_indices[0]] = crowding_distances[sorted_indices[-1]] = float('inf')\n",
    "\n",
    "        for i in range(1, len(sorted_indices) - 1):\n",
    "            prev_value = evaluated_results[sorted_indices[i - 1]][objective_index]\n",
    "            next_value = evaluated_results[sorted_indices[i + 1]][objective_index]\n",
    "            distance = next_value - prev_value\n",
    "            crowding_distances[sorted_indices[i]] += distance\n",
    "\n",
    "    return crowding_distances\n",
    "\n",
    "\n",
    "def dominates(model1, model2, test_loader):\n",
    "    # Evaluate both models on the test set\n",
    "    accuracy1, _, _,latency1, _, _ = evaluate_architecture(model1, test_loader)\n",
    "    accuracy2, _, _,latency2, _ , _= evaluate_architecture(model2, test_loader)\n",
    "    \n",
    "    # Calculate memory usage as the number of parameters * 4 bytes (FP32)\n",
    "    memory1 = count_parameters(model1) * 4  # Memory in bytes\n",
    "    memory2 = count_parameters(model2) * 4  # Memory in bytes\n",
    "    \n",
    "    # Compare performance metrics\n",
    "    dominates_in_accuracy = accuracy1 >= accuracy2\n",
    "    dominates_in_latency = latency1 <= latency2\n",
    "    dominates_in_memory = memory1 <= memory2\n",
    "\n",
    "    # Return True if model1 dominates model2 in all aspects\n",
    "    return dominates_in_accuracy and dominates_in_latency and dominates_in_memory\n",
    "\n",
    "\n",
    "\n",
    "#     return depth, num_heads, mlp_ratio, embed_dim\n",
    "def mutate(architecture):\n",
    "    depth, num_heads, mlp_ratio, embed_dim = architecture\n",
    "    mutation_choices = [\n",
    "        (random.choice([6, 8, 10, 12]), 'depth'),\n",
    "        (random.choice([8, 12, 16]), 'num_heads'),\n",
    "        (random.choice([2.0, 4.0, 6.0]), 'mlp_ratio'),\n",
    "        (random.choice([384, 480]), 'embed_dim')\n",
    "    ]\n",
    "    \n",
    "    # Mutate at least one parameter\n",
    "    while True:\n",
    "        for new_val, param in mutation_choices:\n",
    "            if random.random() < 0.5:\n",
    "                if param == 'depth': depth = new_val\n",
    "                elif param == 'num_heads': num_heads = new_val\n",
    "                elif param == 'mlp_ratio': mlp_ratio = new_val\n",
    "                elif param == 'embed_dim': embed_dim = new_val\n",
    "        if (depth, num_heads, mlp_ratio, embed_dim) != architecture:\n",
    "            break\n",
    "            \n",
    "    return (depth, num_heads, mlp_ratio, embed_dim)                            ## check whether tuple is returned or not\n",
    "\n",
    "# One-Point Crossover: Combine two parent architectures to create new architectures\n",
    "# def one_point_crossover(parent1, parent2):\n",
    "#     crossover_point = random.choice([0, 1, 2, 3])  # Crossover at depth, num_heads, etc.\n",
    "#     child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "#     child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "#     print(f\"Crossover result: Child1={child1}, Child2={child2}\")\n",
    "#     return child1, child2\n",
    "def one_point_crossover(parent1, parent2):\n",
    "    crossover_point = random.randint(0, 3)\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "############################# this is not weight based instead it is pareto selection\n",
    "# Optimized Pareto selection based on stored performance metrics\n",
    "def pareto_selection(arch_performance):\n",
    "    def dominates(perf1, perf2):\n",
    "        acc1, lat1, mem1 = perf1\n",
    "        acc2, lat2, mem2 = perf2\n",
    "        return (acc1 >= acc2 and lat1 <= lat2 and mem1 <= mem2) and (acc1 > acc2 or lat1 < lat2 or mem1 < mem2)\n",
    "\n",
    "    ranks = {}\n",
    "    for arch1, perf1 in arch_performance.items():\n",
    "        dominated_count = 0\n",
    "        for arch2, perf2 in arch_performance.items():\n",
    "            if arch1 != arch2 and dominates(perf2, perf1):\n",
    "                dominated_count += 1\n",
    "        ranks[arch1] = dominated_count\n",
    "\n",
    "    # Sort architectures by rank (lower dominated_count = better)\n",
    "    sorted_population = sorted(ranks.keys(), key=lambda arch: ranks[arch])\n",
    "    return sorted_population\n",
    "\n",
    "\n",
    "\n",
    "# def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "#     print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "#     sampled_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         start_epoch = time.time()\n",
    "#         sampled_model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for images, labels in train_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = sampled_model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "#         epoch_time = time.time() - start_epoch\n",
    "#         test_accuracy, test_top5, test_loss, test_latency, memory_usage = evaluate_architecture(sampled_model, test_loader)\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}, Top-1 Acc: {test_accuracy:.2f}%, Top-5 Acc: {test_top5:.2f}%, Latency: {test_latency:.6f}s/img, Time: {epoch_time:.2f}s\")\n",
    "#     # Save model code unchanged\n",
    "#     if architecture_folder:\n",
    "#         os.makedirs(architecture_folder, exist_ok=True)\n",
    "#         torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "#     return sampled_model\n",
    "\n",
    "def fine_tune_model(sampled_model, train_loader, test_loader, epochs=3, architecture_folder=None):\n",
    "    print(f\"Fine-tuning model with architecture: Depth={sampled_model.depth}, Num Heads={sampled_model.num_heads}, MLP Ratio={sampled_model.mlp_ratio}\")\n",
    "    sampled_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(sampled_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        sampled_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = sampled_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        epoch_time = time.time() - start_epoch\n",
    "        test_accuracy, test_top5, test_loss, test_latency, memory_usage, macs = evaluate_architecture(sampled_model, test_loader)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n",
    "        print(f\"| Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"| Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"| Top-1 Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"| Top-3 Accuracy: {test_top5:.2f}%\")\n",
    "        print(f\"| Latency: {test_latency:.6f}s/img\")\n",
    "        print(f\"| Memory Usage: {memory_usage:.2f}MB\")\n",
    "        print(f\"| MACs: {macs/1e6:.2f}M\")\n",
    "        print(f\"| Epoch Time: {epoch_time:.2f}s\\n\")\n",
    "\n",
    "    # Save model weights\n",
    "    if architecture_folder:\n",
    "        os.makedirs(architecture_folder, exist_ok=True)\n",
    "        torch.save(sampled_model.state_dict(), os.path.join(architecture_folder, 'checkpoint.pth'))\n",
    "    return sampled_model\n",
    "\n",
    "\n",
    "\n",
    "def save_top_ranked_models(population, arch_performance, generation):\n",
    "    top_n = min(5, len(population))                                             ## how top n is taken ??????????\n",
    "    for idx, arch in enumerate(population[:top_n]):\n",
    "        depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        # model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "        #                    num_heads=num_heads, mlp_ratio=mlp_ratio, num_classes=1000).to(device)\n",
    "        model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim, depth=depth,\n",
    "                            num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                            num_classes=1000).to(device)\n",
    "\n",
    "\n",
    "        architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "        top_model_path = os.path.join(SAVE_PATH, f'top_ranked_model_gen{generation+1}_rank_{idx+1}.pth')\n",
    "        torch.save(model.state_dict(), top_model_path)\n",
    "        \n",
    "        acc, lat, mem = arch_performance[arch]\n",
    "\n",
    "        # with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "        #     f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "        #     f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "        with open(top_model_path.replace('.pth', '.txt'), 'w') as f:\n",
    "            f.write(f\"Rank: {idx+1}\\nArchitecture: Depth={depth}, Num Heads={num_heads}, MLP Ratio={mlp_ratio}, Embed Dim={embed_dim}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.2f}%, Latency: {lat:.6f}s/image, Memory: {mem / (1024 ** 2):.2f}MB\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Saved top-ranked model: Generation {generation+1}, Rank {idx+1} (Acc={acc:.2f}%, Lat={lat:.6f}, Mem={mem/(1024**2):.2f}MB)\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def plot_pareto_front(arch_performance):\n",
    "    accuracies = [v[0] for v in arch_performance.values()]\n",
    "    latencies = [v[1] for v in arch_performance.values()]\n",
    "    memories = [v[2] / (1024**2) for v in arch_performance.values()]  # convert to MB\n",
    "\n",
    "    # Accuracy vs Latency\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latencies, accuracies, c='blue')\n",
    "    plt.xlabel('Latency (s/image)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Latency)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs Memory\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(memories, accuracies, c='green')\n",
    "    plt.xlabel('Memory (MB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Pareto Front (Accuracy vs Memory)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "def evolutionary_algorithm(population_size=16, generations=5, mutation_rate=0.1, crossover_rate=0.7, train_loader=None, test_loader=None):\n",
    "    seen_architectures = set()\n",
    "    population = [sample_subnetwork(seen_architectures) for _ in range(population_size)]\n",
    "    arch_performance = {}\n",
    "\n",
    "    prev_best_accuracy = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for generation in range(generations):\n",
    "        print(f\"\\n--- Generation {generation + 1}/{generations} ---\")\n",
    "\n",
    "        # for arch in population:\n",
    "        #     depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "        #     architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "        #     checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "        #     model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "        #                        depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "        #                        num_classes=200).to(device)\n",
    "\n",
    "        for arch in population:\n",
    "            depth, num_heads, mlp_ratio, embed_dim = arch\n",
    "            architecture_folder = os.path.join(SAVE_PATH, f\"arch_{depth}_{num_heads}_{mlp_ratio}_{embed_dim}\")\n",
    "            checkpoint_path = os.path.join(architecture_folder, 'checkpoint.pth')\n",
    "\n",
    "            model = DynamicViT(img_size=224, patch_size=16, embed_dim=embed_dim,\n",
    "                               depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                               num_classes=1000).to(device)\n",
    "\n",
    "            # Determine fine-tuning epochs based on embedding dimension\n",
    "            fine_tune_epochs = 16 if embed_dim != 768 else 3  # 768 is pretrained model's embed_dim\n",
    "            \n",
    "            # Clearly load weights once per architecture\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                model.load_state_dict(torch.load(checkpoint_path))\n",
    "                print(f\"Loaded weights from previous generation for architecture {arch}\")\n",
    "            else:\n",
    "                load_pretrained_weights(model)\n",
    "\n",
    "            # fine_tune_model(model, train_loader, test_loader, epochs=5, architecture_folder=architecture_folder)\n",
    "            fine_tune_model(model, train_loader, test_loader, epochs=fine_tune_epochs, \n",
    "                           architecture_folder=architecture_folder)\n",
    "\n",
    "            # accuracy, _, latency, _ = evaluate_architecture(model, test_loader)\n",
    "            accuracy, top5_accuracy, test_loss, latency, memory_usage, macs = evaluate_architecture(model, test_loader)\n",
    "            memory = count_parameters(model) * 4 / (1024 ** 2)  # MB\n",
    "            # arch_performance[arch] = (accuracy, latency, memory)\n",
    "            arch_performance[arch] = (accuracy, top5_accuracy, latency, memory_usage, macs)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Pareto selection\n",
    "        population = pareto_selection(arch_performance)\n",
    "\n",
    "        print(\"\\nTop 5 Ranked Models of Generation\", generation+1)\n",
    "        for idx, arch in enumerate(population[:5]):\n",
    "            acc, top5_acc, lat, mem, macs = arch_performance[arch]\n",
    "            print(f\"Rank {idx+1}: Model {arch} | Top-1 Acc: {acc:.2f}%, Top-3 Acc: {top5_acc:.2f}%, Latency: {lat:.6f}s/img, Mem: {mem:.2f}MB, MACs: {macs/1e6:.2f}M\")\n",
    "            # Saving top-ranked models\n",
    "            save_top_ranked_models(population, arch_performance, generation)\n",
    "\n",
    "        # Check for Pareto front convergence (early stopping criteria)\n",
    "        current_best_accuracy = arch_performance[population[0]][0]\n",
    "        if current_best_accuracy - prev_best_accuracy < 1.0:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"Minimal improvement detected: {current_best_accuracy - prev_best_accuracy:.2f}%\")\n",
    "            if no_improvement_count >= 2:\n",
    "                print(\"Pareto front has converged. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        prev_best_accuracy = current_best_accuracy\n",
    "\n",
    "        # Generate offspring\n",
    "        next_population = population[:len(population)//2]  # Only top half\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, len(next_population)-1, 2):\n",
    "            parent1, parent2 = next_population[i], next_population[i+1]\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                child1, child2 = one_point_crossover(parent1, parent2)\n",
    "                print(f\"Crossover parents: {parent1} & {parent2}\")\n",
    "                offspring.extend([child1, child2])\n",
    "            else:\n",
    "                offspring.extend([parent1, parent2])\n",
    "\n",
    "        # Mutation with clear logging\n",
    "        mutated_offspring = []\n",
    "        for child in offspring:\n",
    "            if random.random() < mutation_rate:\n",
    "                original_child = child\n",
    "                child = mutate(child)\n",
    "                print(f\"Mutated from {original_child} to {child}\")\n",
    "            mutated_offspring.append(child)\n",
    "\n",
    "        population = next_population + mutated_offspring\n",
    "\n",
    "        print(f\"\\nAfter mutation and crossover, {len(mutated_offspring)} offspring models generated.\")\n",
    "        print(\"Only top 5 models will be used for the next generation.\")\n",
    "\n",
    "    # Plot Pareto Front at the end\n",
    "    plot_pareto_front(arch_performance)\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run the evolutionary algorithm\n",
    "evolutionary_algorithm(population_size=16, generations=5, train_loader=train_loader, test_loader=test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
